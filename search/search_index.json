{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prism-H - Mosquito Breeding Spot Detection","text":"<p>Welcome to the documentation for Prism-H, an AI-driven project designed to support ASHA (Accredited Social Health Activist) workers in identifying and documenting mosquito breeding spots more effectively.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>Mosquito-borne diseases remain a major public health challenge, especially in areas where stagnant water creates perfect breeding grounds for mosquitoes. ASHA workers, who are part of local NGOs, play a crucial role in identifying and documenting these breeding sites to enable targeted intervention efforts.</p>"},{"location":"#the-challenge","title":"The Challenge","text":"<p>Despite their essential role, ASHA workers face several challenges:</p> <ul> <li>Mislabeled Data: Many images are labeled with wrong container types or missing labels altogether</li> <li>Duplicate Submissions: Workers often capture multiple images of the same breeding spot, creating data redundancy</li> <li>Poor Image Quality: Many images are blurry, poorly framed, or taken in bad lighting conditions</li> <li>Inconsistent Protocols: Lack of clear guidelines leads to huge variations in data quality and content</li> </ul>"},{"location":"#our-solution","title":"Our Solution","text":"<p>This project streamlines and improves mosquito breeding spot identification through:</p> <ul> <li>AI-Driven Image Analysis: Automated categorization and quality assessment</li> <li>Self-Supervised Learning: Feature extraction without requiring extensive labeled data</li> <li>Metadata Integration: Analysis of GPS coordinates, timestamps, and worker IDs for behavioral insights</li> <li>Real-time Feedback: Guidance for ASHA workers to capture more useful images</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#intelligent-preprocessing","title":"\ud83d\udd0d Intelligent Preprocessing","text":"<ul> <li>Automated detection of invalid, duplicate, blurry, and dark images</li> <li>Quality-based image filtering using fastdup library</li> <li>Efficient data cleaning pipeline</li> </ul>"},{"location":"#self-supervised-learning","title":"\ud83e\udde0 Self-Supervised Learning","text":"<ul> <li>SimCLR-based feature extraction for unlabeled data</li> <li>Domain-specific fine-tuning for mosquito breeding spots</li> <li>Robust representation learning</li> </ul>"},{"location":"#smart-classification","title":"\ud83c\udfaf Smart Classification","text":"<ul> <li>Container type classification (drums, sumps, plant pots, etc.)</li> <li>Object detection using Grounding DINO</li> <li>Multi-modal analysis combining visual and metadata features</li> </ul>"},{"location":"#advanced-analytics","title":"\ud83d\udcca Advanced Analytics","text":"<ul> <li>Clustering analysis for pattern discovery</li> <li>Spatial-temporal analysis using GPS and timestamps</li> <li>Worker performance and behavior insights</li> </ul>"},{"location":"#geospatial-integration","title":"\ud83d\uddfa\ufe0f Geospatial Integration","text":"<ul> <li>Location-based duplicate detection</li> <li>Spatial distribution analysis</li> <li>Territory mapping and optimization</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    A[Raw Images + Metadata] --&gt; B[Preprocessing Pipeline]\n    B --&gt; C[Quality Filtering]\n    B --&gt; D[Duplicate Detection]\n    C --&gt; E[Clean Dataset]\n    D --&gt; E\n    E --&gt; F[SimCLR Feature Extraction]\n    F --&gt; G[Clustering Analysis]\n    F --&gt; H[Classification]\n    G --&gt; I[Visualization &amp; Reports]\n    H --&gt; I\n    A --&gt; J[Metadata Integration]\n    J --&gt; I\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Set up the environment and dependencies</li> <li>Quick Start - Run your first analysis</li> <li>Configuration - Customize the pipeline for your needs</li> </ol>"},{"location":"#main-components","title":"Main Components","text":"Module Purpose Key Features Preprocessing Data cleaning and quality control Invalid/duplicate/blur/dark image detection Feature Extraction Generate image embeddings SimCLR-based feature extraction Clustering Pattern discovery Fastdup clustering and visualization Classification Container type prediction Multi-class image classification Metadata Integration Spatial-temporal analysis GPS/timestamp/worker analysis"},{"location":"#project-impact","title":"Project Impact","text":"<p>This initiative aims to:</p> <ul> <li>Improve Data Quality: Reduce mislabeled and duplicate submissions by up to 70%</li> <li>Enhance Efficiency: Streamline intervention targeting through automated analysis  </li> <li>Support Workers: Provide real-time feedback to improve data collection practices</li> <li>Scale Impact: Enable more effective mosquito control efforts across larger areas</li> </ul>"},{"location":"#research-development","title":"Research &amp; Development","text":"<p>The project incorporates cutting-edge techniques in:</p> <ul> <li>Self-Supervised Learning: SimCLR for feature learning without extensive labels</li> <li>Computer Vision: Object detection and image quality assessment</li> <li>Geospatial Analysis: Location-based insights and optimization</li> <li>Behavioral Analytics: Worker performance and pattern analysis</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udcd6 User Guide - Comprehensive usage instructions</li> <li>\ud83d\udd27 API Reference - Detailed function documentation  </li> <li>\ud83d\udca1 Examples - Practical usage examples</li> <li>\ud83d\ude80 Getting Started - Installation and setup</li> </ul> <p>Need Help?</p> <p>Check out our examples for practical usage scenarios or browse the API documentation for detailed function references.</p>"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#prismh","title":"<code>prismh</code>","text":""},{"location":"api/classifier/","title":"Classification","text":""},{"location":"api/classifier/#prismh.models.classify","title":"<code>prismh.models.classify</code>","text":""},{"location":"api/classifier/#prismh.models.classify.ClassificationDataset","title":"<code>ClassificationDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/prismh/models/classify.py</code> <pre><code>class ClassificationDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        \"\"\"\n        Args:\n            image_paths (list): List of paths to images.\n            labels (list or np.array): List of corresponding labels (integer indices).\n            transform (callable, optional): Transform to apply to images.\n        \"\"\"\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n        if len(self.image_paths) != len(self.labels):\n             raise ValueError(\"Number of image paths must match number of labels\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            # Ensure label is a tensor if needed by the loss function\n            return image, torch.tensor(label, dtype=torch.long)\n        except FileNotFoundError:\n            print(f\"Warning: Image file not found {img_path}. Skipping.\")\n            return None, None # Need handling in DataLoader collate_fn or training loop\n        except Exception as e:\n            print(f\"Warning: Error loading image {img_path}: {e}. Skipping.\")\n            # Handle error appropriately, maybe return None or skip\n            return None, None # Need handling in DataLoader collate_fn or training loop\n</code></pre>"},{"location":"api/classifier/#prismh.models.classify.ClassificationDataset.__init__","title":"<code>__init__(image_paths, labels, transform=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>list</code> <p>List of paths to images.</p> required <code>labels</code> <code>list or array</code> <p>List of corresponding labels (integer indices).</p> required <code>transform</code> <code>callable</code> <p>Transform to apply to images.</p> <code>None</code> Source code in <code>src/prismh/models/classify.py</code> <pre><code>def __init__(self, image_paths, labels, transform=None):\n    \"\"\"\n    Args:\n        image_paths (list): List of paths to images.\n        labels (list or np.array): List of corresponding labels (integer indices).\n        transform (callable, optional): Transform to apply to images.\n    \"\"\"\n    self.image_paths = image_paths\n    self.labels = labels\n    self.transform = transform\n\n    if len(self.image_paths) != len(self.labels):\n         raise ValueError(\"Number of image paths must match number of labels\")\n</code></pre>"},{"location":"api/clustering/","title":"Clustering","text":""},{"location":"api/clustering/#prismh.core.cluster_embeddings","title":"<code>prismh.core.cluster_embeddings</code>","text":""},{"location":"api/embeddings/","title":"Embeddings","text":""},{"location":"api/embeddings/#prismh.core.extract_embeddings","title":"<code>prismh.core.extract_embeddings</code>","text":""},{"location":"api/embeddings/#prismh.core.extract_embeddings.SimCLRModel","title":"<code>SimCLRModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>SimCLR model with encoder and projection head</p> Source code in <code>src/prismh/core/extract_embeddings.py</code> <pre><code>class SimCLRModel(nn.Module):\n    \"\"\"SimCLR model with encoder and projection head\"\"\"\n    def __init__(self, base_model='resnet50', pretrained=True, output_dim=128):\n        super(SimCLRModel, self).__init__()\n        if base_model == 'resnet50':\n            # Use the updated weights parameter for torchvision &gt;= 0.13\n            weights = models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n            self.encoder = models.resnet50(weights=weights)\n            self.encoder_dim = 2048\n        elif base_model == 'resnet18':\n            weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n            self.encoder = models.resnet18(weights=weights)\n            self.encoder_dim = 512\n        else:\n            raise ValueError(f\"Unsupported base model: {base_model}\")\n        self.encoder.fc = nn.Identity()\n        self.projection_head = SimCLRProjectionHead(\n            input_dim=self.encoder_dim,\n            output_dim=output_dim\n        )\n    def forward(self, x):\n        features = self.encoder(x)\n        projections = self.projection_head(features)\n        # Note: For embedding extraction, we only need 'features'\n        return features, projections # Original return for compatibility if needed elsewhere\n</code></pre>"},{"location":"api/embeddings/#prismh.core.extract_embeddings.SimCLRProjectionHead","title":"<code>SimCLRProjectionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Projection head for SimCLR</p> Source code in <code>src/prismh/core/extract_embeddings.py</code> <pre><code>class SimCLRProjectionHead(nn.Module):\n    \"\"\"Projection head for SimCLR\"\"\"\n    def __init__(self, input_dim, hidden_dim=2048, output_dim=128):\n        super(SimCLRProjectionHead, self).__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    def forward(self, x):\n        return self.projection(x)\n</code></pre>"},{"location":"api/json-processor/","title":"JSON Processing","text":""},{"location":"api/json-processor/#prismh.data.json_processor","title":"<code>prismh.data.json_processor</code>","text":""},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor","title":"<code>ImageProcessor</code>","text":"<p>A class to handle various tasks on a JSON dataset of image URLs: 1) Counting image extensions (.jpg, .png, etc.) 2) Analyzing invalid image URLs 3) Downloading PNG images 4) Displaying random images from a folder</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>class ImageProcessor:\n    \"\"\"\n    A class to handle various tasks on a JSON dataset of image URLs:\n    1) Counting image extensions (.jpg, .png, etc.)\n    2) Analyzing invalid image URLs\n    3) Downloading PNG images\n    4) Displaying random images from a folder\n    \"\"\"\n\n    def __init__(self, json_files, save_folder, data_key=\"data\"):\n        \"\"\"\n        :param json_files: List of paths to JSON files containing image records.\n        :param save_folder: Folder where images will be downloaded.\n        :param data_key: The key under which the list of records is stored in JSON.\n        \"\"\"\n        self.json_files = [Path(f) for f in json_files]  # Convert paths to Path objects\n        self.save_folder = Path(save_folder)  # Convert save folder to Path object\n        self.data_key = data_key\n        self.data = {}  # Store data from each JSON file\n        self.extension_counts = {}\n\n    def load_json(self):\n        \"\"\"Load JSON data from all provided files.\"\"\"\n        for json_file in self.json_files:\n            with json_file.open(\"r\", encoding=\"utf-8\") as f:  # Use Path.open\n                self.data[json_file.name] = json.load(f).get(self.data_key, []) # Use file name as key\n            print(f\"Loaded {len(self.data[json_file.name])} records from {json_file.name}\")\n\n    def count_image_extensions(self):\n        \"\"\"Count how many URLs end with .jpg, .png, or other extensions.\"\"\"\n        for json_file_name, records in self.data.items(): # Iterate over names now\n            jpg_count, png_count, other_count = 0, 0, 0\n\n            for entry in tqdm(records, desc=f\"Counting extensions in {json_file_name}\"):\n                url = entry.get(\"image_url\", \"\").lower()\n                if url.endswith(\".jpg\"):\n                    jpg_count += 1\n                elif url.endswith(\".png\"):\n                    png_count += 1\n                else:\n                    other_count += 1\n\n            self.extension_counts[json_file_name] = {\n                \"jpg\": jpg_count,\n                \"png\": png_count,\n                \"other\": other_count\n            }\n\n    def analyze_invalid_entries(self, sample_size=10):\n        \"\"\"Print details of invalid image URLs that don't end in .jpg or .png.\"\"\"\n        for json_file_name, records in self.data.items(): # Iterate over names now\n            invalid_entries = [entry for entry in records if not entry.get(\"image_url\", \"\").lower().endswith((\".jpg\", \".png\"))]\n\n            print(f\"Total invalid entries in {json_file_name} = {len(invalid_entries)}\")\n            for i, entry in enumerate(invalid_entries[:sample_size], 1):\n                print(f\"\\nInvalid Entry #{i} from {json_file_name}:\")\n                for key, value in entry.items():\n                    print(f\"  {key}: {value}\")\n\n    def download_image(self, image_url, image_id):\n        \"\"\"Download a single image to the specified folder, using the provided image URL and ID.\"\"\"\n        try:\n            response = requests.get(image_url, stream=True, timeout=10)\n            if response.status_code == 200:\n                ext = image_url.split(\".\")[-1].lower()  # Get file extension\n                file_path = self.save_folder / f\"{image_id}.{ext}\" # Use Path concatenation\n                with file_path.open('wb') as out_file: # Use Path.open\n                    for chunk in response.iter_content(1024):\n                        out_file.write(chunk)\n                return True\n        except requests.RequestException:\n            return False\n        return False\n\n    def download_images(self):\n        \"\"\"Download PNG and JPG images from the specified JSON files.\"\"\"\n        self.save_folder.mkdir(parents=True, exist_ok=True) # Use Path.mkdir\n\n        for json_file_name, records in self.data.items(): # Iterate over names now\n            print(f\"Processing {json_file_name} for image downloads...\")\n            downloaded_count = 0\n            failed_count = 0\n\n            for entry in tqdm(records, desc=f\"Downloading images from {json_file_name}\"):\n                image_url = entry.get(\"image_url\", \"\").lower()\n                image_id = entry.get(\"Id\")\n\n                # Only attempt download if URL is valid (.png or .jpg) and ID exists\n                if image_id and image_url.endswith((\".png\", \".jpg\")):\n                    success = self.download_image(image_url, image_id)\n                    if success:\n                        downloaded_count += 1\n                    else:\n                        failed_count += 1\n\n            print(f\"Finished {json_file_name}: Downloaded: {downloaded_count}, Failed: {failed_count}\")\n\n    def show_random_images(self, num_images=30, rows=3, cols=10):\n        \"\"\"Display a grid of randomly selected images from the saved folder.\"\"\"\n        valid_extensions = ('.jpg', '.jpeg', '.png')\n        # Use Path.iterdir and Path.suffix\n        all_images = [f for f in self.save_folder.iterdir() if f.suffix.lower() in valid_extensions]\n\n        if len(all_images) &lt; num_images:\n            # Use f-string for Path object\n            raise ValueError(f\"Not enough images in {self.save_folder} to select {num_images}.\")\n\n        selected_files = random.sample(all_images, num_images)\n        plt.figure(figsize=(cols * 2, rows * 2))\n\n        for idx, filepath in enumerate(selected_files, start=1): # filepath is now a Path object\n            # img_path = os.path.join(self.save_folder, filename) # No longer needed\n            with Image.open(filepath) as img: # Open directly with Path object\n                plt.subplot(rows, cols, idx)\n                plt.imshow(img)\n                plt.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.__init__","title":"<code>__init__(json_files, save_folder, data_key='data')</code>","text":"<p>:param json_files: List of paths to JSON files containing image records. :param save_folder: Folder where images will be downloaded. :param data_key: The key under which the list of records is stored in JSON.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def __init__(self, json_files, save_folder, data_key=\"data\"):\n    \"\"\"\n    :param json_files: List of paths to JSON files containing image records.\n    :param save_folder: Folder where images will be downloaded.\n    :param data_key: The key under which the list of records is stored in JSON.\n    \"\"\"\n    self.json_files = [Path(f) for f in json_files]  # Convert paths to Path objects\n    self.save_folder = Path(save_folder)  # Convert save folder to Path object\n    self.data_key = data_key\n    self.data = {}  # Store data from each JSON file\n    self.extension_counts = {}\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.analyze_invalid_entries","title":"<code>analyze_invalid_entries(sample_size=10)</code>","text":"<p>Print details of invalid image URLs that don't end in .jpg or .png.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def analyze_invalid_entries(self, sample_size=10):\n    \"\"\"Print details of invalid image URLs that don't end in .jpg or .png.\"\"\"\n    for json_file_name, records in self.data.items(): # Iterate over names now\n        invalid_entries = [entry for entry in records if not entry.get(\"image_url\", \"\").lower().endswith((\".jpg\", \".png\"))]\n\n        print(f\"Total invalid entries in {json_file_name} = {len(invalid_entries)}\")\n        for i, entry in enumerate(invalid_entries[:sample_size], 1):\n            print(f\"\\nInvalid Entry #{i} from {json_file_name}:\")\n            for key, value in entry.items():\n                print(f\"  {key}: {value}\")\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.count_image_extensions","title":"<code>count_image_extensions()</code>","text":"<p>Count how many URLs end with .jpg, .png, or other extensions.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def count_image_extensions(self):\n    \"\"\"Count how many URLs end with .jpg, .png, or other extensions.\"\"\"\n    for json_file_name, records in self.data.items(): # Iterate over names now\n        jpg_count, png_count, other_count = 0, 0, 0\n\n        for entry in tqdm(records, desc=f\"Counting extensions in {json_file_name}\"):\n            url = entry.get(\"image_url\", \"\").lower()\n            if url.endswith(\".jpg\"):\n                jpg_count += 1\n            elif url.endswith(\".png\"):\n                png_count += 1\n            else:\n                other_count += 1\n\n        self.extension_counts[json_file_name] = {\n            \"jpg\": jpg_count,\n            \"png\": png_count,\n            \"other\": other_count\n        }\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.download_image","title":"<code>download_image(image_url, image_id)</code>","text":"<p>Download a single image to the specified folder, using the provided image URL and ID.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def download_image(self, image_url, image_id):\n    \"\"\"Download a single image to the specified folder, using the provided image URL and ID.\"\"\"\n    try:\n        response = requests.get(image_url, stream=True, timeout=10)\n        if response.status_code == 200:\n            ext = image_url.split(\".\")[-1].lower()  # Get file extension\n            file_path = self.save_folder / f\"{image_id}.{ext}\" # Use Path concatenation\n            with file_path.open('wb') as out_file: # Use Path.open\n                for chunk in response.iter_content(1024):\n                    out_file.write(chunk)\n            return True\n    except requests.RequestException:\n        return False\n    return False\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.download_images","title":"<code>download_images()</code>","text":"<p>Download PNG and JPG images from the specified JSON files.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def download_images(self):\n    \"\"\"Download PNG and JPG images from the specified JSON files.\"\"\"\n    self.save_folder.mkdir(parents=True, exist_ok=True) # Use Path.mkdir\n\n    for json_file_name, records in self.data.items(): # Iterate over names now\n        print(f\"Processing {json_file_name} for image downloads...\")\n        downloaded_count = 0\n        failed_count = 0\n\n        for entry in tqdm(records, desc=f\"Downloading images from {json_file_name}\"):\n            image_url = entry.get(\"image_url\", \"\").lower()\n            image_id = entry.get(\"Id\")\n\n            # Only attempt download if URL is valid (.png or .jpg) and ID exists\n            if image_id and image_url.endswith((\".png\", \".jpg\")):\n                success = self.download_image(image_url, image_id)\n                if success:\n                    downloaded_count += 1\n                else:\n                    failed_count += 1\n\n        print(f\"Finished {json_file_name}: Downloaded: {downloaded_count}, Failed: {failed_count}\")\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.load_json","title":"<code>load_json()</code>","text":"<p>Load JSON data from all provided files.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def load_json(self):\n    \"\"\"Load JSON data from all provided files.\"\"\"\n    for json_file in self.json_files:\n        with json_file.open(\"r\", encoding=\"utf-8\") as f:  # Use Path.open\n            self.data[json_file.name] = json.load(f).get(self.data_key, []) # Use file name as key\n        print(f\"Loaded {len(self.data[json_file.name])} records from {json_file.name}\")\n</code></pre>"},{"location":"api/json-processor/#prismh.data.json_processor.ImageProcessor.show_random_images","title":"<code>show_random_images(num_images=30, rows=3, cols=10)</code>","text":"<p>Display a grid of randomly selected images from the saved folder.</p> Source code in <code>src/prismh/data/json_processor.py</code> <pre><code>def show_random_images(self, num_images=30, rows=3, cols=10):\n    \"\"\"Display a grid of randomly selected images from the saved folder.\"\"\"\n    valid_extensions = ('.jpg', '.jpeg', '.png')\n    # Use Path.iterdir and Path.suffix\n    all_images = [f for f in self.save_folder.iterdir() if f.suffix.lower() in valid_extensions]\n\n    if len(all_images) &lt; num_images:\n        # Use f-string for Path object\n        raise ValueError(f\"Not enough images in {self.save_folder} to select {num_images}.\")\n\n    selected_files = random.sample(all_images, num_images)\n    plt.figure(figsize=(cols * 2, rows * 2))\n\n    for idx, filepath in enumerate(selected_files, start=1): # filepath is now a Path object\n        # img_path = os.path.join(self.save_folder, filename) # No longer needed\n        with Image.open(filepath) as img: # Open directly with Path object\n            plt.subplot(rows, cols, idx)\n            plt.imshow(img)\n            plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/metadata-integrator/","title":"Metadata Integration","text":""},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator","title":"<code>prismh.data.metadata_integrator</code>","text":"<p>Metadata Integrator</p> <p>A module for integrating JSON metadata with processed images and providing analysis.</p>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator","title":"<code>MetadataIntegrator</code>","text":"<p>A class for integrating JSON metadata with processed images. Provides functions for analysis, statistics, and generating reports.</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>class MetadataIntegrator:\n    \"\"\"\n    A class for integrating JSON metadata with processed images.\n    Provides functions for analysis, statistics, and generating reports.\n    \"\"\"\n\n    def __init__(self,\n                 json_file_path: str,\n                 processed_dir: str,\n                 output_dir: str = \"metadata_analysis\",\n                 id_field: str = \"Id\",\n                 image_url_field: str = \"image_url\",\n                 env: str = \"dev\"):\n        \"\"\"\n        Initialize the metadata integrator.\n\n        Args:\n            json_file_path (str): Path to the JSON metadata file\n            processed_dir (str): Path to the directory with processed images\n            output_dir (str): Path to save analysis output\n            id_field (str): Field name in JSON that contains the image ID\n            image_url_field (str): Field name in JSON that contains the image URL\n            env (str): Environment (dev/main)\n        \"\"\"\n        self.json_file_path = Path(json_file_path).resolve()\n        self.processed_dir = Path(processed_dir).resolve()\n        self.output_dir = Path(output_dir).resolve()\n        self.id_field = id_field\n        self.image_url_field = image_url_field\n        self.env = env\n\n        # Create output directory if it doesn't exist\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Load JSON data\n        self.json_data = self._load_json_data()\n\n        # Load image data\n        self.image_data = self._load_image_data()\n\n        # Initialize integrated data\n        self.integrated_data = None\n\n        # Initialize visualization utilities\n        viz_output_dir = self.output_dir / \"visualizations\"\n        self.viz = VisualizationUtils(output_dir=str(viz_output_dir))\n\n    def _load_json_data(self) -&gt; List[Dict]:\n        \"\"\"\n        Load JSON metadata from file.\n\n        Returns:\n            List[Dict]: List of metadata records\n        \"\"\"\n        try:\n            with self.json_file_path.open('r') as f:\n                data = json.load(f)\n\n            # Handle both list and dict formats\n            if isinstance(data, dict):\n                if \"data\" in data:\n                    return data[\"data\"]\n                else:\n                    return [data]\n            return data\n        except Exception as e:\n            print(f\"Error loading JSON data: {e}\")\n            return []\n\n    def _load_image_data(self) -&gt; Dict[str, Dict]:\n        \"\"\"\n        Load information about processed images in the directories.\n\n        Returns:\n            Dict[str, Dict]: Dictionary mapping image IDs to their metadata\n        \"\"\"\n        image_data = {}\n\n        # Get all image files from the processed directory\n        clean_dir = self.processed_dir / \"clean\"\n        problematic_dir = self.processed_dir / \"problematic\"\n\n        # Process clean images\n        if clean_dir.exists():\n            for f_path in clean_dir.iterdir():\n                if f_path.is_file() and f_path.suffix.lower() in ('.png', '.jpg', '.jpeg'):\n                    image_id = f_path.stem # Use stem to get filename without extension\n                    image_data[image_id] = {\n                        'status': 'clean',\n                        'path': str(f_path), # Store as string for compatibility if needed\n                        'filename': f_path.name\n                    }\n\n        # Process problematic images\n        if problematic_dir.exists():\n            for category_path in problematic_dir.iterdir():\n                if category_path.is_dir():\n                    category_name = category_path.name\n                    for f_path in category_path.iterdir():\n                        if f_path.is_file() and f_path.suffix.lower() in ('.png', '.jpg', '.jpeg'):\n                            image_id = f_path.stem\n                            image_data[image_id] = {\n                                'status': 'problematic',\n                                'problem_category': category_name,\n                                'path': str(f_path),\n                                'filename': f_path.name\n                            }\n\n        return image_data\n\n    def integrate_metadata(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Integrate JSON metadata with processed image information.\n\n        Returns:\n            pd.DataFrame: DataFrame with integrated metadata\n        \"\"\"\n        # Create list to store integrated records\n        integrated_records = []\n\n        # Track statistics for reporting\n        self.match_stats = {\n            'total_metadata_records': len(self.json_data),\n            'total_processed_images': len(self.image_data),\n            'matched_records': 0,\n            'unmatched_metadata': [],\n            'unmatched_images': []\n        }\n\n        # Integrate metadata with processed images\n        for record in self.json_data:\n            # Extract ID\n            if self.id_field in record:\n                record_id = str(record[self.id_field])\n\n                # Check if we have a processed image for this ID\n                if record_id in self.image_data:\n                    # Combine metadata with image info\n                    integrated_record = {**record, **self.image_data[record_id]}\n                    integrated_records.append(integrated_record)\n                    self.match_stats['matched_records'] += 1\n                else:\n                    # Keep track of metadata without matching images\n                    self.match_stats['unmatched_metadata'].append(record_id)\n            else:\n                print(f\"Warning: Record missing ID field: {record}\")\n\n        # Find processed images without metadata\n        metadata_ids = set(str(record[self.id_field]) for record in self.json_data if self.id_field in record)\n        self.match_stats['unmatched_images'] = [\n            image_id for image_id in self.image_data.keys() \n            if image_id not in metadata_ids\n        ]\n\n        # Convert to DataFrame\n        if integrated_records:\n            self.integrated_data = pd.DataFrame(integrated_records)\n            return self.integrated_data\n        else:\n            # Return empty DataFrame with expected columns\n            self.integrated_data = pd.DataFrame()\n            return self.integrated_data\n\n    def analyze_data(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Analyze the integrated data and return statistics.\n\n        Returns:\n            Dict[str, Any]: Dictionary with analysis results\n        \"\"\"\n        # Make sure we have integrated data\n        if self.integrated_data is None or self.integrated_data.empty:\n            self.integrate_metadata()\n\n        if self.integrated_data.empty:\n            return {'error': 'No integrated data available for analysis'}\n\n        # Initialize analysis results\n        analysis = {\n            'total_records': len(self.integrated_data),\n            'match_rate': (self.match_stats['matched_records'] / self.match_stats['total_metadata_records'] * 100\n                           if self.match_stats['total_metadata_records'] &gt; 0 else 0), # Avoid division by zero\n            'quality_distribution': {},\n            'category_distribution': {},\n            'date_distribution': {},\n            'location_distribution': {}\n        }\n\n        # Analyze image quality distribution\n        if 'status' in self.integrated_data.columns:\n            status_counts = self.integrated_data['status'].value_counts().to_dict()\n            analysis['quality_distribution'] = status_counts\n\n            # Get problematic category distribution if available\n            if 'problem_category' in self.integrated_data.columns:\n                problem_categories = self.integrated_data[\n                    self.integrated_data['status'] == 'problematic'\n                ]['problem_category'].value_counts().to_dict()\n                analysis['problem_category_distribution'] = problem_categories\n\n        # Analyze category distribution if available\n        category_fields = [col for col in self.integrated_data.columns \n                          if col.lower() in ('category', 'type', 'container type', 'breeding spot')]\n\n        if category_fields:\n            primary_category_field = category_fields[0]\n            category_counts = self.integrated_data[primary_category_field].value_counts().to_dict()\n            analysis['category_distribution'] = category_counts\n\n        # Analyze date distribution if available\n        date_fields = [col for col in self.integrated_data.columns \n                      if any(date_term in col.lower() for date_term in ('date', 'time', 'day', 'month', 'year'))]\n\n        if date_fields:\n            # Try to convert to datetime\n            try:\n                date_field = date_fields[0]\n                self.integrated_data[f'{date_field}_dt'] = pd.to_datetime(\n                    self.integrated_data[date_field], errors='coerce'\n                )\n\n                # Group by month\n                date_counts = self.integrated_data[f'{date_field}_dt'].dt.to_period('M').value_counts().sort_index()\n                analysis['date_distribution'] = {str(period): count for period, count in date_counts.items()}\n            except Exception as e:\n                print(f\"Error processing dates: {e}\")\n\n        # Analyze location distribution if available\n        location_fields = [col for col in self.integrated_data.columns \n                         if any(loc_term in col.lower() for loc_term in ('location', 'ward', 'area', 'region', 'district'))]\n\n        if location_fields:\n            location_field = location_fields[0]\n            location_counts = self.integrated_data[location_field].value_counts().to_dict()\n            # Limit to top 10 for readability\n            analysis['location_distribution'] = {\n                k: v for k, v in sorted(location_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n            }\n\n        return analysis\n\n    def generate_visualizations(self, analysis: Optional[Dict[str, Any]] = None) -&gt; List[str]:\n        \"\"\"\n        Generate visualizations from the integrated data.\n\n        Args:\n            analysis (Dict[str, Any], optional): Analysis results to visualize\n\n        Returns:\n            List[str]: List of paths to generated visualizations\n        \"\"\"\n        # Ensure data is analyzed\n        if analysis is None:\n            analysis = self.analyze_data()\n            if 'error' in analysis:\n                print(f\"Cannot generate visualizations: {analysis['error']}\")\n                return []\n\n        # Use the visualization utility's output dir\n        viz_output_dir = Path(self.viz.output_dir) # Ensure it's a Path object\n        viz_output_dir.mkdir(parents=True, exist_ok=True) # Ensure it exists\n\n        generated_files = []\n\n        # Quality distribution pie chart\n        if 'quality_distribution' in analysis and analysis['quality_distribution']:\n            quality_data = analysis['quality_distribution']\n\n            self.viz.plot_pie(\n                data=quality_data,\n                title='Image Quality Distribution',\n                filename=str(viz_output_dir / 'quality_distribution_pie.png')\n            )\n            generated_files.append(str(viz_output_dir / 'quality_distribution_pie.png'))\n\n        # Problematic category distribution bar chart (if available)\n        if 'problem_category_distribution' in analysis and analysis['problem_category_distribution']:\n            self.viz.plot_bar(\n                data=analysis['problem_category_distribution'],\n                title='Distribution of Problematic Image Categories',\n                xlabel='Category',\n                ylabel='Count',\n                filename=str(viz_output_dir / 'problematic_categories_bar.png')\n            )\n            generated_files.append(str(viz_output_dir / 'problematic_categories_bar.png'))\n\n        # Category distribution bar chart (if available)\n        if 'category_distribution' in analysis and analysis['category_distribution']:\n            category_data = analysis['category_distribution']\n\n            self.viz.plot_bar(\n                data=category_data,\n                title='Data Category Distribution',\n                xlabel='Category',\n                ylabel='Count',\n                filename=str(viz_output_dir / 'category_distribution_bar.png')\n            )\n            generated_files.append(str(viz_output_dir / 'category_distribution_bar.png'))\n\n        # Date distribution line chart (if available)\n        if 'date_distribution' in analysis and analysis['date_distribution'] and date_fields:\n            self.viz.plot_line(\n                data=analysis['date_distribution'],\n                title='Image Distribution Over Time (Monthly)',\n                xlabel='Month',\n                ylabel='Count',\n                filename=str(viz_output_dir / 'date_distribution_line.png')\n            )\n            generated_files.append(str(viz_output_dir / 'date_distribution_line.png'))\n\n        # Location distribution bar chart (if available)\n        if 'location_distribution' in analysis and analysis['location_distribution']:\n            self.viz.plot_bar(\n                data=analysis['location_distribution'],\n                title='Location Distribution',\n                xlabel='Location',\n                ylabel='Count',\n                filename=str(viz_output_dir / 'location_distribution_bar.png'),\n                top_n=20 # Show top 20 locations\n            )\n            generated_files.append(str(viz_output_dir / 'location_distribution_bar.png'))\n\n        print(f\"Visualizations saved to: {viz_output_dir}\")\n        return generated_files\n\n    def _find_column(self, possible_names: List[str]) -&gt; Optional[str]:\n        \"\"\"\n        Find a column in the data that matches any of the possible names.\n\n        Args:\n            possible_names (List[str]): List of possible column name substrings\n\n        Returns:\n            Optional[str]: Found column name or None\n        \"\"\"\n        if self.integrated_data is None or self.integrated_data.empty:\n            return None\n\n        for col in self.integrated_data.columns:\n            if any(name.lower() in col.lower() for name in possible_names):\n                return col\n\n        return None\n\n    def generate_report(self) -&gt; str:\n        \"\"\"\n        Generate a comprehensive report with all visualizations and analysis.\n\n        Returns:\n            str: Path to the generated report HTML file\n        \"\"\"\n        # Make sure we have integrated data\n        if self.integrated_data is None or self.integrated_data.empty:\n            self.integrate_metadata()\n\n        if self.integrated_data.empty:\n            print(\"No integrated data available for report generation\")\n            return \"\"\n\n        # Run analysis\n        analysis = self.analyze_data()\n\n        # Generate visualizations\n        visualizations = self.generate_visualizations(analysis)\n\n        # Prepare summary data for the report\n        summary_data = {\n            'Total Metadata Records': self.match_stats['total_metadata_records'],\n            'Total Processed Images': self.match_stats['total_processed_images'],\n            'Matched Records': self.match_stats['matched_records'],\n            'Match Rate': self.match_stats['matched_records'] / self.match_stats['total_metadata_records'] * 100,\n            'Unmatched Metadata Records': len(self.match_stats['unmatched_metadata']),\n            'Unmatched Images': len(self.match_stats['unmatched_images'])\n        }\n\n        # Add quality stats if available\n        if 'quality_distribution' in analysis:\n            for quality, count in analysis['quality_distribution'].items():\n                summary_data[f'{quality} Images'] = count\n\n        # Add problem category stats if available\n        if 'problem_category_distribution' in analysis:\n            for category, count in analysis['problem_category_distribution'].items():\n                summary_data[f'{category} Issues'] = count\n\n        # Add category stats if available\n        if 'category_distribution' in analysis:\n            category_col = self._find_column(['category', 'type', 'container type', 'breeding spot'])\n            if category_col:\n                summary_data[f'{category_col.capitalize()} Distribution'] = ', '.join(\n                    f\"{k}: {v}\" for k, v in sorted(\n                        analysis['category_distribution'].items(), \n                        key=lambda x: x[1], \n                        reverse=True\n                    )\n                )\n\n        # Check for breeding spot distribution\n        breeding_col = self._find_column(['breed', 'spot', 'larv', 'pupa'])\n        if breeding_col and breeding_col in self.integrated_data.columns:\n            # Calculate breeding spot percentage\n            breeding_counts = self.integrated_data[breeding_col].value_counts()\n            if 'Yes' in breeding_counts:\n                yes_count = breeding_counts['Yes']\n                total_count = len(self.integrated_data)\n                yes_percent = yes_count / total_count * 100\n                summary_data['Breeding Spots Detected'] = yes_count\n                summary_data['Breeding Spot Percentage'] = yes_percent\n\n        # Check quality issues\n        quality_cols = [col for col in self.integrated_data.columns \n                      if col.startswith('is_') and col in ['is_dark', 'is_blurry', \n                                                         'is_bright', 'is_duplicate', \n                                                         'is_outlier', 'is_redundant']]\n        if quality_cols:\n            for col in quality_cols:\n                if col in self.integrated_data.columns:\n                    issue_count = self.integrated_data[col].sum()\n                    issue_pct = issue_count / len(self.integrated_data) * 100\n                    issue_name = col.replace('is_', '').capitalize()\n                    summary_data[f'{issue_name} Images'] = issue_count\n                    summary_data[f'{issue_name} Image Percentage'] = issue_pct\n\n            # Count images with any quality issue\n            has_issue = self.integrated_data[quality_cols].any(axis=1)\n            issue_count = has_issue.sum()\n            issue_pct = issue_count / len(self.integrated_data) * 100\n            summary_data['Images with Any Quality Issue'] = issue_count\n            summary_data['Quality Issue Percentage'] = issue_pct\n\n        # Add some metadata analysis\n        worker_col = self._find_column(['worker', 'user', 'asha', 'uid', 'Uid'])\n        if worker_col and worker_col in self.integrated_data.columns:\n            worker_counts = self.integrated_data[worker_col].value_counts()\n            summary_data['Total Workers/Users'] = len(worker_counts)\n            summary_data['Most Active Worker'] = f\"{worker_counts.index[0]} ({worker_counts.iloc[0]} images)\"\n            summary_data['Average Images per Worker'] = len(self.integrated_data) / len(worker_counts)\n\n        ward_col = self._find_column(['ward', 'area', 'zone', 'district'])\n        if ward_col and ward_col in self.integrated_data.columns:\n            ward_counts = self.integrated_data[ward_col].value_counts()\n            summary_data['Total Wards/Areas'] = len(ward_counts)\n            summary_data['Most Common Ward'] = f\"{ward_counts.index[0]} ({ward_counts.iloc[0]} images)\"\n\n        date_col = self._find_column(['date', 'time', 'timestamp'])\n        if date_col and date_col in self.integrated_data.columns:\n            # Convert to datetime if not already\n            try:\n                if self.integrated_data[date_col].dtype != 'datetime64[ns]':\n                    self.integrated_data[date_col] = pd.to_datetime(self.integrated_data[date_col], errors='coerce')\n\n                # Filter out invalid dates\n                valid_dates = self.integrated_data.dropna(subset=[date_col])\n\n                if not valid_dates.empty:\n                    min_date = valid_dates[date_col].min()\n                    max_date = valid_dates[date_col].max()\n                    date_range = max_date - min_date\n\n                    summary_data['Date Range'] = f\"{min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\"\n                    summary_data['Collection Period'] = f\"{date_range.days} days\"\n                    summary_data['Average Images per Day'] = len(valid_dates) / max(date_range.days, 1)\n            except Exception as e:\n                print(f\"Error analyzing dates: {e}\")\n\n        # Create additional text for the report\n        additional_text = self._generate_report_insights()\n\n        # Generate HTML report\n        report_file = self.viz.generate_html_report(\n            title=\"Mosquito Breeding Site Data Analysis Report\",\n            filename=\"report.html\",\n            visualizations=visualizations,\n            summary_data=summary_data,\n            additional_text=additional_text\n        )\n\n        print(f\"Report generated at: {report_file}\")\n        return report_file\n\n    def _generate_report_insights(self) -&gt; str:\n        \"\"\"\n        Generate insights text for the report based on the analysis.\n\n        Returns:\n            str: HTML-formatted insights text\n        \"\"\"\n        insights = []\n\n        # Check match rate\n        match_rate = self.match_stats['matched_records'] / self.match_stats['total_metadata_records'] * 100\n        if match_rate &lt; 80:\n            insights.append(f\"&lt;p&gt;&lt;strong&gt;Low Match Rate:&lt;/strong&gt; Only {match_rate:.1f}% of metadata records were matched with processed images. Consider reviewing the data collection process to ensure images are properly linked to metadata.&lt;/p&gt;\")\n\n        # Check quality issues\n        quality_cols = [col for col in self.integrated_data.columns \n                      if col.startswith('is_') and col in ['is_dark', 'is_blurry', \n                                                          'is_bright', 'is_duplicate', \n                                                          'is_outlier', 'is_redundant']]\n\n        if quality_cols:\n            # Check for high quality issue rates\n            for col in quality_cols:\n                if col in self.integrated_data.columns:\n                    issue_pct = self.integrated_data[col].mean() * 100\n                    issue_name = col.replace('is_', '').capitalize()\n\n                    if issue_pct &gt; 15:\n                        insights.append(f\"&lt;p&gt;&lt;strong&gt;High {issue_name} Rate:&lt;/strong&gt; {issue_pct:.1f}% of images are detected as {issue_name.lower()}. This may indicate issues with camera settings or collection conditions.&lt;/p&gt;\")\n\n            # Check overall quality\n            has_issue = self.integrated_data[quality_cols].any(axis=1)\n            issue_pct = has_issue.sum() / len(self.integrated_data) * 100\n\n            if issue_pct &gt; 40:\n                insights.append(f\"&lt;p&gt;&lt;strong&gt;Poor Overall Quality:&lt;/strong&gt; {issue_pct:.1f}% of images have at least one quality issue. Training on proper image capture techniques may be beneficial.&lt;/p&gt;\")\n\n        # Check breeding spot distribution\n        breeding_col = self._find_column(['breed', 'spot', 'larv', 'pupa'])\n        if breeding_col and breeding_col in self.integrated_data.columns:\n            # Calculate breeding spot percentage\n            breeding_counts = self.integrated_data[breeding_col].value_counts()\n            if 'Yes' in breeding_counts:\n                yes_count = breeding_counts['Yes']\n                total_count = len(self.integrated_data)\n                yes_percent = yes_count / total_count * 100\n\n                if yes_percent &gt; 80:\n                    insights.append(f\"&lt;p&gt;&lt;strong&gt;Very High Breeding Spot Rate:&lt;/strong&gt; {yes_percent:.1f}% of images are labeled as breeding spots. This seems unusually high and may indicate labeling bias or selection bias in collection.&lt;/p&gt;\")\n                elif yes_percent &lt; 5:\n                    insights.append(f\"&lt;p&gt;&lt;strong&gt;Very Low Breeding Spot Rate:&lt;/strong&gt; Only {yes_percent:.1f}% of images are labeled as breeding spots. This may indicate missed detections or focused collection in low-risk areas.&lt;/p&gt;\")\n\n        # Check worker/ward distribution\n        worker_col = self._find_column(['worker', 'user', 'asha', 'uid', 'Uid'])\n        if worker_col and worker_col in self.integrated_data.columns:\n            worker_counts = self.integrated_data[worker_col].value_counts()\n\n            # Check for worker concentration\n            top_worker_pct = worker_counts.iloc[0] / len(self.integrated_data) * 100\n\n            if top_worker_pct &gt; 40 and len(worker_counts) &gt; 3:\n                insights.append(f\"&lt;p&gt;&lt;strong&gt;Worker Concentration:&lt;/strong&gt; The most active worker ({worker_counts.index[0]}) contributed {top_worker_pct:.1f}% of all images. Consider ensuring more balanced data collection across workers.&lt;/p&gt;\")\n\n            # Check for duplicate concentration\n            duplicate_col = self._find_column(['duplicate', 'redundant', 'is_duplicate', 'is_redundant'])\n            if duplicate_col and duplicate_col in self.integrated_data.columns:\n                # Group by worker and calculate duplicate rate\n                try:\n                    worker_dupes = self.integrated_data.groupby(worker_col)[duplicate_col].mean() * 100\n                    high_dupe_workers = worker_dupes[worker_dupes &gt; 25]\n\n                    if not high_dupe_workers.empty:\n                        top_dupe_worker = high_dupe_workers.index[0]\n                        top_dupe_pct = high_dupe_workers.iloc[0]\n\n                        insights.append(f\"&lt;p&gt;&lt;strong&gt;High Duplicate Rate from Workers:&lt;/strong&gt; Worker {top_dupe_worker} has a {top_dupe_pct:.1f}% duplicate rate. Consider additional training on proper image collection protocols.&lt;/p&gt;\")\n                except Exception as e:\n                    print(f\"Error analyzing worker duplicates: {e}\")\n\n        # Check time patterns\n        date_col = self._find_column(['date', 'time', 'timestamp'])\n        if date_col and date_col in self.integrated_data.columns:\n            try:\n                if self.integrated_data[date_col].dtype != 'datetime64[ns]':\n                    self.integrated_data[date_col] = pd.to_datetime(self.integrated_data[date_col], errors='coerce')\n\n                # Get hour of day distribution\n                hour_counts = self.integrated_data[date_col].dt.hour.value_counts().sort_index()\n\n                # Check for off-hours collection\n                night_hours = hour_counts.loc[hour_counts.index.isin(range(19, 24))].sum() + hour_counts.loc[hour_counts.index.isin(range(0, 6))].sum()\n                night_pct = night_hours / hour_counts.sum() * 100\n\n                if night_pct &gt; 15:\n                    insights.append(f\"&lt;p&gt;&lt;strong&gt;Unusual Collection Hours:&lt;/strong&gt; {night_pct:.1f}% of images were collected during night hours (7PM-6AM). This may affect image quality and breeding spot visibility.&lt;/p&gt;\")\n            except Exception as e:\n                print(f\"Error analyzing time patterns: {e}\")\n\n        # Container type analysis\n        container_col = self._find_column(['container', 'type'])\n        if container_col and container_col in self.integrated_data.columns and breeding_col:\n            try:\n                # Get container type counts\n                container_counts = self.integrated_data[container_col].value_counts()\n                top_container = container_counts.index[0]\n                top_container_pct = container_counts.iloc[0] / len(self.integrated_data) * 100\n\n                if top_container_pct &gt; 50:\n                    insights.append(f\"&lt;p&gt;&lt;strong&gt;Container Type Focus:&lt;/strong&gt; {top_container_pct:.1f}% of images are of '{top_container}' containers. This may indicate over-focus on one container type or common breeding environments in the area.&lt;/p&gt;\")\n\n                # Check breeding rate by container\n                container_breeding = self.integrated_data.groupby(container_col)[breeding_col].apply(\n                    lambda x: (x == 'Yes').mean() * 100\n                ).sort_values(ascending=False)\n\n                if not container_breeding.empty:\n                    highest_breeding = container_breeding.index[0]\n                    highest_breeding_pct = container_breeding.iloc[0]\n\n                    if highest_breeding_pct &gt; 70:\n                        insights.append(f\"&lt;p&gt;&lt;strong&gt;High-Risk Container Type:&lt;/strong&gt; '{highest_breeding}' containers show a {highest_breeding_pct:.1f}% breeding spot rate, significantly higher than other types. These containers may be priority targets for intervention.&lt;/p&gt;\")\n            except Exception as e:\n                print(f\"Error analyzing container types: {e}\")\n\n        # If no insights were generated\n        if not insights:\n            insights.append(\"&lt;p&gt;No significant insights detected in the current dataset. The data appears to follow expected patterns.&lt;/p&gt;\")\n\n        # Combine insights\n        return \"&lt;h3&gt;Key Insights&lt;/h3&gt;\" + \"\".join(insights)\n\n    def create_annotated_images(self, \n                               output_dir: Optional[str] = None,\n                               fields_to_show: Optional[List[str]] = None,\n                               sample_size: int = 0) -&gt; List[str]:\n        \"\"\"\n        Create annotated versions of images with metadata overlay.\n\n        Args:\n            output_dir (str, optional): Directory to save annotated images\n            fields_to_show (List[str], optional): Fields to show in annotation\n            sample_size (int): If &gt; 0, only process a sample of this size\n\n        Returns:\n            List[str]: List of paths to annotated images\n        \"\"\"\n        # Set output directory\n        if output_dir is None:\n            output_dir = os.path.join(self.output_dir, \"annotated_images\")\n\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Make sure we have integrated data\n        if self.integrated_data is None or self.integrated_data.empty:\n            self.integrate_metadata()\n\n        if self.integrated_data.empty:\n            print(\"No integrated data available for annotation\")\n            return []\n\n        # If no fields specified, use common fields\n        if fields_to_show is None:\n            # Look for common fields in the data\n            potential_fields = [\n                self.id_field, 'Ward Name', 'Ward Number', 'Date and Time', \n                'Breeding spot', 'Container Type', 'Latitude and Longitude', \n                'Remarks', 'Asha worker'\n            ]\n            fields_to_show = [field for field in potential_fields if field in self.integrated_data.columns]\n\n        # Get data to process\n        if sample_size &gt; 0 and sample_size &lt; len(self.integrated_data):\n            data_to_process = self.integrated_data.sample(sample_size)\n        else:\n            data_to_process = self.integrated_data\n\n        annotated_paths = []\n\n        # Process each image\n        for _, row in data_to_process.iterrows():\n            if 'path' in row and os.path.exists(row['path']):\n                try:\n                    # Open image\n                    img = Image.open(row['path'])\n                    draw = ImageDraw.Draw(img)\n\n                    # Try to get a font\n                    try:\n                        font = ImageFont.truetype(\"arial.ttf\", 20)\n                    except:\n                        # Fallback to default font\n                        font = ImageFont.load_default()\n\n                    # Create annotation text\n                    annotation = []\n                    for field in fields_to_show:\n                        if field in row and not pd.isna(row[field]):\n                            annotation.append(f\"{field}: {row[field]}\")\n\n                    # Add status information\n                    if 'status' in row:\n                        status_text = f\"Status: {row['status']}\"\n                        if row['status'] == 'problematic' and 'problem_category' in row:\n                            status_text += f\" ({row['problem_category']})\"\n                        annotation.append(status_text)\n\n                    annotation_text = \"\\n\".join(annotation)\n\n                    # Create semi-transparent background for text\n                    text_width, text_height = draw.textsize(annotation_text, font=font)\n                    padding = 10\n\n                    # Ensure text bg stays within image bounds\n                    bg_width = min(text_width + 2*padding, img.width)\n                    bg_height = min(text_height + 2*padding, img.height)\n\n                    # Add bg rectangle\n                    draw.rectangle(\n                        [(0, 0), (bg_width, bg_height)],\n                        fill=(0, 0, 0, 128)  # Semi-transparent black\n                    )\n\n                    # Add text\n                    draw.text((padding, padding), annotation_text, fill=(255, 255, 255), font=font)\n\n                    # Save annotated image\n                    output_path = os.path.join(output_dir, f\"annotated_{os.path.basename(row['path'])}\")\n                    img.save(output_path)\n                    annotated_paths.append(output_path)\n\n                except Exception as e:\n                    print(f\"Error annotating image {row['path']}: {e}\")\n\n        print(f\"Created {len(annotated_paths)} annotated images in {output_dir}\")\n        return annotated_paths\n\n    def export_integrated_data(self, output_format: str = 'csv') -&gt; str:\n        \"\"\"\n        Export the integrated data to a file.\n\n        Args:\n            output_format (str): Format to export (csv, excel, json)\n\n        Returns:\n            str: Path to the exported file\n        \"\"\"\n        # Make sure we have integrated data\n        if self.integrated_data is None or self.integrated_data.empty:\n            self.integrate_metadata()\n\n        if self.integrated_data.empty:\n            print(\"No integrated data available for export\")\n            return \"\"\n\n        # Define output path\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"integrated_metadata_{timestamp}.{output_format}\"\n        output_path = self.output_dir / filename\n\n        try:\n            if output_format == 'csv':\n                self.integrated_data.to_csv(output_path, index=False)\n            elif output_format == 'json':\n                self.integrated_data.to_json(output_path, orient='records', indent=4)\n            elif output_format == 'xlsx':\n                self.integrated_data.to_excel(output_path, index=False)\n            else:\n                raise ValueError(f\"Unsupported format: {output_format}\")\n\n            print(f\"Integrated data exported successfully to: {output_path}\")\n            return str(output_path)\n        except Exception as e:\n            print(f\"Error exporting data: {e}\")\n            return \"\"\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.__init__","title":"<code>__init__(json_file_path: str, processed_dir: str, output_dir: str = 'metadata_analysis', id_field: str = 'Id', image_url_field: str = 'image_url', env: str = 'dev')</code>","text":"<p>Initialize the metadata integrator.</p> <p>Parameters:</p> Name Type Description Default <code>json_file_path</code> <code>str</code> <p>Path to the JSON metadata file</p> required <code>processed_dir</code> <code>str</code> <p>Path to the directory with processed images</p> required <code>output_dir</code> <code>str</code> <p>Path to save analysis output</p> <code>'metadata_analysis'</code> <code>id_field</code> <code>str</code> <p>Field name in JSON that contains the image ID</p> <code>'Id'</code> <code>image_url_field</code> <code>str</code> <p>Field name in JSON that contains the image URL</p> <code>'image_url'</code> <code>env</code> <code>str</code> <p>Environment (dev/main)</p> <code>'dev'</code> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def __init__(self,\n             json_file_path: str,\n             processed_dir: str,\n             output_dir: str = \"metadata_analysis\",\n             id_field: str = \"Id\",\n             image_url_field: str = \"image_url\",\n             env: str = \"dev\"):\n    \"\"\"\n    Initialize the metadata integrator.\n\n    Args:\n        json_file_path (str): Path to the JSON metadata file\n        processed_dir (str): Path to the directory with processed images\n        output_dir (str): Path to save analysis output\n        id_field (str): Field name in JSON that contains the image ID\n        image_url_field (str): Field name in JSON that contains the image URL\n        env (str): Environment (dev/main)\n    \"\"\"\n    self.json_file_path = Path(json_file_path).resolve()\n    self.processed_dir = Path(processed_dir).resolve()\n    self.output_dir = Path(output_dir).resolve()\n    self.id_field = id_field\n    self.image_url_field = image_url_field\n    self.env = env\n\n    # Create output directory if it doesn't exist\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load JSON data\n    self.json_data = self._load_json_data()\n\n    # Load image data\n    self.image_data = self._load_image_data()\n\n    # Initialize integrated data\n    self.integrated_data = None\n\n    # Initialize visualization utilities\n    viz_output_dir = self.output_dir / \"visualizations\"\n    self.viz = VisualizationUtils(output_dir=str(viz_output_dir))\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.analyze_data","title":"<code>analyze_data() -&gt; Dict[str, Any]</code>","text":"<p>Analyze the integrated data and return statistics.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary with analysis results</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def analyze_data(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyze the integrated data and return statistics.\n\n    Returns:\n        Dict[str, Any]: Dictionary with analysis results\n    \"\"\"\n    # Make sure we have integrated data\n    if self.integrated_data is None or self.integrated_data.empty:\n        self.integrate_metadata()\n\n    if self.integrated_data.empty:\n        return {'error': 'No integrated data available for analysis'}\n\n    # Initialize analysis results\n    analysis = {\n        'total_records': len(self.integrated_data),\n        'match_rate': (self.match_stats['matched_records'] / self.match_stats['total_metadata_records'] * 100\n                       if self.match_stats['total_metadata_records'] &gt; 0 else 0), # Avoid division by zero\n        'quality_distribution': {},\n        'category_distribution': {},\n        'date_distribution': {},\n        'location_distribution': {}\n    }\n\n    # Analyze image quality distribution\n    if 'status' in self.integrated_data.columns:\n        status_counts = self.integrated_data['status'].value_counts().to_dict()\n        analysis['quality_distribution'] = status_counts\n\n        # Get problematic category distribution if available\n        if 'problem_category' in self.integrated_data.columns:\n            problem_categories = self.integrated_data[\n                self.integrated_data['status'] == 'problematic'\n            ]['problem_category'].value_counts().to_dict()\n            analysis['problem_category_distribution'] = problem_categories\n\n    # Analyze category distribution if available\n    category_fields = [col for col in self.integrated_data.columns \n                      if col.lower() in ('category', 'type', 'container type', 'breeding spot')]\n\n    if category_fields:\n        primary_category_field = category_fields[0]\n        category_counts = self.integrated_data[primary_category_field].value_counts().to_dict()\n        analysis['category_distribution'] = category_counts\n\n    # Analyze date distribution if available\n    date_fields = [col for col in self.integrated_data.columns \n                  if any(date_term in col.lower() for date_term in ('date', 'time', 'day', 'month', 'year'))]\n\n    if date_fields:\n        # Try to convert to datetime\n        try:\n            date_field = date_fields[0]\n            self.integrated_data[f'{date_field}_dt'] = pd.to_datetime(\n                self.integrated_data[date_field], errors='coerce'\n            )\n\n            # Group by month\n            date_counts = self.integrated_data[f'{date_field}_dt'].dt.to_period('M').value_counts().sort_index()\n            analysis['date_distribution'] = {str(period): count for period, count in date_counts.items()}\n        except Exception as e:\n            print(f\"Error processing dates: {e}\")\n\n    # Analyze location distribution if available\n    location_fields = [col for col in self.integrated_data.columns \n                     if any(loc_term in col.lower() for loc_term in ('location', 'ward', 'area', 'region', 'district'))]\n\n    if location_fields:\n        location_field = location_fields[0]\n        location_counts = self.integrated_data[location_field].value_counts().to_dict()\n        # Limit to top 10 for readability\n        analysis['location_distribution'] = {\n            k: v for k, v in sorted(location_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n        }\n\n    return analysis\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.create_annotated_images","title":"<code>create_annotated_images(output_dir: Optional[str] = None, fields_to_show: Optional[List[str]] = None, sample_size: int = 0) -&gt; List[str]</code>","text":"<p>Create annotated versions of images with metadata overlay.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save annotated images</p> <code>None</code> <code>fields_to_show</code> <code>List[str]</code> <p>Fields to show in annotation</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>If &gt; 0, only process a sample of this size</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to annotated images</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def create_annotated_images(self, \n                           output_dir: Optional[str] = None,\n                           fields_to_show: Optional[List[str]] = None,\n                           sample_size: int = 0) -&gt; List[str]:\n    \"\"\"\n    Create annotated versions of images with metadata overlay.\n\n    Args:\n        output_dir (str, optional): Directory to save annotated images\n        fields_to_show (List[str], optional): Fields to show in annotation\n        sample_size (int): If &gt; 0, only process a sample of this size\n\n    Returns:\n        List[str]: List of paths to annotated images\n    \"\"\"\n    # Set output directory\n    if output_dir is None:\n        output_dir = os.path.join(self.output_dir, \"annotated_images\")\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Make sure we have integrated data\n    if self.integrated_data is None or self.integrated_data.empty:\n        self.integrate_metadata()\n\n    if self.integrated_data.empty:\n        print(\"No integrated data available for annotation\")\n        return []\n\n    # If no fields specified, use common fields\n    if fields_to_show is None:\n        # Look for common fields in the data\n        potential_fields = [\n            self.id_field, 'Ward Name', 'Ward Number', 'Date and Time', \n            'Breeding spot', 'Container Type', 'Latitude and Longitude', \n            'Remarks', 'Asha worker'\n        ]\n        fields_to_show = [field for field in potential_fields if field in self.integrated_data.columns]\n\n    # Get data to process\n    if sample_size &gt; 0 and sample_size &lt; len(self.integrated_data):\n        data_to_process = self.integrated_data.sample(sample_size)\n    else:\n        data_to_process = self.integrated_data\n\n    annotated_paths = []\n\n    # Process each image\n    for _, row in data_to_process.iterrows():\n        if 'path' in row and os.path.exists(row['path']):\n            try:\n                # Open image\n                img = Image.open(row['path'])\n                draw = ImageDraw.Draw(img)\n\n                # Try to get a font\n                try:\n                    font = ImageFont.truetype(\"arial.ttf\", 20)\n                except:\n                    # Fallback to default font\n                    font = ImageFont.load_default()\n\n                # Create annotation text\n                annotation = []\n                for field in fields_to_show:\n                    if field in row and not pd.isna(row[field]):\n                        annotation.append(f\"{field}: {row[field]}\")\n\n                # Add status information\n                if 'status' in row:\n                    status_text = f\"Status: {row['status']}\"\n                    if row['status'] == 'problematic' and 'problem_category' in row:\n                        status_text += f\" ({row['problem_category']})\"\n                    annotation.append(status_text)\n\n                annotation_text = \"\\n\".join(annotation)\n\n                # Create semi-transparent background for text\n                text_width, text_height = draw.textsize(annotation_text, font=font)\n                padding = 10\n\n                # Ensure text bg stays within image bounds\n                bg_width = min(text_width + 2*padding, img.width)\n                bg_height = min(text_height + 2*padding, img.height)\n\n                # Add bg rectangle\n                draw.rectangle(\n                    [(0, 0), (bg_width, bg_height)],\n                    fill=(0, 0, 0, 128)  # Semi-transparent black\n                )\n\n                # Add text\n                draw.text((padding, padding), annotation_text, fill=(255, 255, 255), font=font)\n\n                # Save annotated image\n                output_path = os.path.join(output_dir, f\"annotated_{os.path.basename(row['path'])}\")\n                img.save(output_path)\n                annotated_paths.append(output_path)\n\n            except Exception as e:\n                print(f\"Error annotating image {row['path']}: {e}\")\n\n    print(f\"Created {len(annotated_paths)} annotated images in {output_dir}\")\n    return annotated_paths\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.export_integrated_data","title":"<code>export_integrated_data(output_format: str = 'csv') -&gt; str</code>","text":"<p>Export the integrated data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_format</code> <code>str</code> <p>Format to export (csv, excel, json)</p> <code>'csv'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the exported file</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def export_integrated_data(self, output_format: str = 'csv') -&gt; str:\n    \"\"\"\n    Export the integrated data to a file.\n\n    Args:\n        output_format (str): Format to export (csv, excel, json)\n\n    Returns:\n        str: Path to the exported file\n    \"\"\"\n    # Make sure we have integrated data\n    if self.integrated_data is None or self.integrated_data.empty:\n        self.integrate_metadata()\n\n    if self.integrated_data.empty:\n        print(\"No integrated data available for export\")\n        return \"\"\n\n    # Define output path\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"integrated_metadata_{timestamp}.{output_format}\"\n    output_path = self.output_dir / filename\n\n    try:\n        if output_format == 'csv':\n            self.integrated_data.to_csv(output_path, index=False)\n        elif output_format == 'json':\n            self.integrated_data.to_json(output_path, orient='records', indent=4)\n        elif output_format == 'xlsx':\n            self.integrated_data.to_excel(output_path, index=False)\n        else:\n            raise ValueError(f\"Unsupported format: {output_format}\")\n\n        print(f\"Integrated data exported successfully to: {output_path}\")\n        return str(output_path)\n    except Exception as e:\n        print(f\"Error exporting data: {e}\")\n        return \"\"\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.generate_report","title":"<code>generate_report() -&gt; str</code>","text":"<p>Generate a comprehensive report with all visualizations and analysis.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the generated report HTML file</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def generate_report(self) -&gt; str:\n    \"\"\"\n    Generate a comprehensive report with all visualizations and analysis.\n\n    Returns:\n        str: Path to the generated report HTML file\n    \"\"\"\n    # Make sure we have integrated data\n    if self.integrated_data is None or self.integrated_data.empty:\n        self.integrate_metadata()\n\n    if self.integrated_data.empty:\n        print(\"No integrated data available for report generation\")\n        return \"\"\n\n    # Run analysis\n    analysis = self.analyze_data()\n\n    # Generate visualizations\n    visualizations = self.generate_visualizations(analysis)\n\n    # Prepare summary data for the report\n    summary_data = {\n        'Total Metadata Records': self.match_stats['total_metadata_records'],\n        'Total Processed Images': self.match_stats['total_processed_images'],\n        'Matched Records': self.match_stats['matched_records'],\n        'Match Rate': self.match_stats['matched_records'] / self.match_stats['total_metadata_records'] * 100,\n        'Unmatched Metadata Records': len(self.match_stats['unmatched_metadata']),\n        'Unmatched Images': len(self.match_stats['unmatched_images'])\n    }\n\n    # Add quality stats if available\n    if 'quality_distribution' in analysis:\n        for quality, count in analysis['quality_distribution'].items():\n            summary_data[f'{quality} Images'] = count\n\n    # Add problem category stats if available\n    if 'problem_category_distribution' in analysis:\n        for category, count in analysis['problem_category_distribution'].items():\n            summary_data[f'{category} Issues'] = count\n\n    # Add category stats if available\n    if 'category_distribution' in analysis:\n        category_col = self._find_column(['category', 'type', 'container type', 'breeding spot'])\n        if category_col:\n            summary_data[f'{category_col.capitalize()} Distribution'] = ', '.join(\n                f\"{k}: {v}\" for k, v in sorted(\n                    analysis['category_distribution'].items(), \n                    key=lambda x: x[1], \n                    reverse=True\n                )\n            )\n\n    # Check for breeding spot distribution\n    breeding_col = self._find_column(['breed', 'spot', 'larv', 'pupa'])\n    if breeding_col and breeding_col in self.integrated_data.columns:\n        # Calculate breeding spot percentage\n        breeding_counts = self.integrated_data[breeding_col].value_counts()\n        if 'Yes' in breeding_counts:\n            yes_count = breeding_counts['Yes']\n            total_count = len(self.integrated_data)\n            yes_percent = yes_count / total_count * 100\n            summary_data['Breeding Spots Detected'] = yes_count\n            summary_data['Breeding Spot Percentage'] = yes_percent\n\n    # Check quality issues\n    quality_cols = [col for col in self.integrated_data.columns \n                  if col.startswith('is_') and col in ['is_dark', 'is_blurry', \n                                                     'is_bright', 'is_duplicate', \n                                                     'is_outlier', 'is_redundant']]\n    if quality_cols:\n        for col in quality_cols:\n            if col in self.integrated_data.columns:\n                issue_count = self.integrated_data[col].sum()\n                issue_pct = issue_count / len(self.integrated_data) * 100\n                issue_name = col.replace('is_', '').capitalize()\n                summary_data[f'{issue_name} Images'] = issue_count\n                summary_data[f'{issue_name} Image Percentage'] = issue_pct\n\n        # Count images with any quality issue\n        has_issue = self.integrated_data[quality_cols].any(axis=1)\n        issue_count = has_issue.sum()\n        issue_pct = issue_count / len(self.integrated_data) * 100\n        summary_data['Images with Any Quality Issue'] = issue_count\n        summary_data['Quality Issue Percentage'] = issue_pct\n\n    # Add some metadata analysis\n    worker_col = self._find_column(['worker', 'user', 'asha', 'uid', 'Uid'])\n    if worker_col and worker_col in self.integrated_data.columns:\n        worker_counts = self.integrated_data[worker_col].value_counts()\n        summary_data['Total Workers/Users'] = len(worker_counts)\n        summary_data['Most Active Worker'] = f\"{worker_counts.index[0]} ({worker_counts.iloc[0]} images)\"\n        summary_data['Average Images per Worker'] = len(self.integrated_data) / len(worker_counts)\n\n    ward_col = self._find_column(['ward', 'area', 'zone', 'district'])\n    if ward_col and ward_col in self.integrated_data.columns:\n        ward_counts = self.integrated_data[ward_col].value_counts()\n        summary_data['Total Wards/Areas'] = len(ward_counts)\n        summary_data['Most Common Ward'] = f\"{ward_counts.index[0]} ({ward_counts.iloc[0]} images)\"\n\n    date_col = self._find_column(['date', 'time', 'timestamp'])\n    if date_col and date_col in self.integrated_data.columns:\n        # Convert to datetime if not already\n        try:\n            if self.integrated_data[date_col].dtype != 'datetime64[ns]':\n                self.integrated_data[date_col] = pd.to_datetime(self.integrated_data[date_col], errors='coerce')\n\n            # Filter out invalid dates\n            valid_dates = self.integrated_data.dropna(subset=[date_col])\n\n            if not valid_dates.empty:\n                min_date = valid_dates[date_col].min()\n                max_date = valid_dates[date_col].max()\n                date_range = max_date - min_date\n\n                summary_data['Date Range'] = f\"{min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\"\n                summary_data['Collection Period'] = f\"{date_range.days} days\"\n                summary_data['Average Images per Day'] = len(valid_dates) / max(date_range.days, 1)\n        except Exception as e:\n            print(f\"Error analyzing dates: {e}\")\n\n    # Create additional text for the report\n    additional_text = self._generate_report_insights()\n\n    # Generate HTML report\n    report_file = self.viz.generate_html_report(\n        title=\"Mosquito Breeding Site Data Analysis Report\",\n        filename=\"report.html\",\n        visualizations=visualizations,\n        summary_data=summary_data,\n        additional_text=additional_text\n    )\n\n    print(f\"Report generated at: {report_file}\")\n    return report_file\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.generate_visualizations","title":"<code>generate_visualizations(analysis: Optional[Dict[str, Any]] = None) -&gt; List[str]</code>","text":"<p>Generate visualizations from the integrated data.</p> <p>Parameters:</p> Name Type Description Default <code>analysis</code> <code>Dict[str, Any]</code> <p>Analysis results to visualize</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to generated visualizations</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def generate_visualizations(self, analysis: Optional[Dict[str, Any]] = None) -&gt; List[str]:\n    \"\"\"\n    Generate visualizations from the integrated data.\n\n    Args:\n        analysis (Dict[str, Any], optional): Analysis results to visualize\n\n    Returns:\n        List[str]: List of paths to generated visualizations\n    \"\"\"\n    # Ensure data is analyzed\n    if analysis is None:\n        analysis = self.analyze_data()\n        if 'error' in analysis:\n            print(f\"Cannot generate visualizations: {analysis['error']}\")\n            return []\n\n    # Use the visualization utility's output dir\n    viz_output_dir = Path(self.viz.output_dir) # Ensure it's a Path object\n    viz_output_dir.mkdir(parents=True, exist_ok=True) # Ensure it exists\n\n    generated_files = []\n\n    # Quality distribution pie chart\n    if 'quality_distribution' in analysis and analysis['quality_distribution']:\n        quality_data = analysis['quality_distribution']\n\n        self.viz.plot_pie(\n            data=quality_data,\n            title='Image Quality Distribution',\n            filename=str(viz_output_dir / 'quality_distribution_pie.png')\n        )\n        generated_files.append(str(viz_output_dir / 'quality_distribution_pie.png'))\n\n    # Problematic category distribution bar chart (if available)\n    if 'problem_category_distribution' in analysis and analysis['problem_category_distribution']:\n        self.viz.plot_bar(\n            data=analysis['problem_category_distribution'],\n            title='Distribution of Problematic Image Categories',\n            xlabel='Category',\n            ylabel='Count',\n            filename=str(viz_output_dir / 'problematic_categories_bar.png')\n        )\n        generated_files.append(str(viz_output_dir / 'problematic_categories_bar.png'))\n\n    # Category distribution bar chart (if available)\n    if 'category_distribution' in analysis and analysis['category_distribution']:\n        category_data = analysis['category_distribution']\n\n        self.viz.plot_bar(\n            data=category_data,\n            title='Data Category Distribution',\n            xlabel='Category',\n            ylabel='Count',\n            filename=str(viz_output_dir / 'category_distribution_bar.png')\n        )\n        generated_files.append(str(viz_output_dir / 'category_distribution_bar.png'))\n\n    # Date distribution line chart (if available)\n    if 'date_distribution' in analysis and analysis['date_distribution'] and date_fields:\n        self.viz.plot_line(\n            data=analysis['date_distribution'],\n            title='Image Distribution Over Time (Monthly)',\n            xlabel='Month',\n            ylabel='Count',\n            filename=str(viz_output_dir / 'date_distribution_line.png')\n        )\n        generated_files.append(str(viz_output_dir / 'date_distribution_line.png'))\n\n    # Location distribution bar chart (if available)\n    if 'location_distribution' in analysis and analysis['location_distribution']:\n        self.viz.plot_bar(\n            data=analysis['location_distribution'],\n            title='Location Distribution',\n            xlabel='Location',\n            ylabel='Count',\n            filename=str(viz_output_dir / 'location_distribution_bar.png'),\n            top_n=20 # Show top 20 locations\n        )\n        generated_files.append(str(viz_output_dir / 'location_distribution_bar.png'))\n\n    print(f\"Visualizations saved to: {viz_output_dir}\")\n    return generated_files\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.MetadataIntegrator.integrate_metadata","title":"<code>integrate_metadata() -&gt; pd.DataFrame</code>","text":"<p>Integrate JSON metadata with processed image information.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with integrated metadata</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def integrate_metadata(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Integrate JSON metadata with processed image information.\n\n    Returns:\n        pd.DataFrame: DataFrame with integrated metadata\n    \"\"\"\n    # Create list to store integrated records\n    integrated_records = []\n\n    # Track statistics for reporting\n    self.match_stats = {\n        'total_metadata_records': len(self.json_data),\n        'total_processed_images': len(self.image_data),\n        'matched_records': 0,\n        'unmatched_metadata': [],\n        'unmatched_images': []\n    }\n\n    # Integrate metadata with processed images\n    for record in self.json_data:\n        # Extract ID\n        if self.id_field in record:\n            record_id = str(record[self.id_field])\n\n            # Check if we have a processed image for this ID\n            if record_id in self.image_data:\n                # Combine metadata with image info\n                integrated_record = {**record, **self.image_data[record_id]}\n                integrated_records.append(integrated_record)\n                self.match_stats['matched_records'] += 1\n            else:\n                # Keep track of metadata without matching images\n                self.match_stats['unmatched_metadata'].append(record_id)\n        else:\n            print(f\"Warning: Record missing ID field: {record}\")\n\n    # Find processed images without metadata\n    metadata_ids = set(str(record[self.id_field]) for record in self.json_data if self.id_field in record)\n    self.match_stats['unmatched_images'] = [\n        image_id for image_id in self.image_data.keys() \n        if image_id not in metadata_ids\n    ]\n\n    # Convert to DataFrame\n    if integrated_records:\n        self.integrated_data = pd.DataFrame(integrated_records)\n        return self.integrated_data\n    else:\n        # Return empty DataFrame with expected columns\n        self.integrated_data = pd.DataFrame()\n        return self.integrated_data\n</code></pre>"},{"location":"api/metadata-integrator/#prismh.data.metadata_integrator.main","title":"<code>main()</code>","text":"<p>Main function to run the metadata integrator from command line.</p> Source code in <code>src/prismh/data/metadata_integrator.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to run the metadata integrator from command line.\n    \"\"\"\n    import argparse\n    import sys\n\n    parser = argparse.ArgumentParser(description=\"Metadata Integration Tool for Mosquito Breeding Site Analysis\")\n\n    # Required arguments\n    parser.add_argument(\"--json\", required=True, help=\"Path to JSON metadata file\")\n    parser.add_argument(\"--images\", required=True, help=\"Path to directory with processed images\")\n\n    # Optional arguments\n    parser.add_argument(\"--output\", default=\"metadata_analysis\", help=\"Output directory for analysis\")\n    parser.add_argument(\"--id-field\", default=\"Id\", help=\"Field name in JSON that contains image ID\")\n    parser.add_argument(\"--image-url-field\", default=\"image_url\", help=\"Field name in JSON that contains image URL\")\n    parser.add_argument(\"--env\", default=\"dev\", choices=[\"dev\", \"main\"], help=\"Environment (dev/main)\")\n    parser.add_argument(\"--annotate\", action=\"store_true\", help=\"Create annotated images with metadata overlay\")\n    parser.add_argument(\"--export\", choices=[\"csv\", \"excel\", \"json\"], help=\"Export integrated data in specified format\")\n    parser.add_argument(\"--visualize-only\", action=\"store_true\", help=\"Only generate visualizations without full report\")\n\n    # Specific visualization options\n    parser.add_argument(\"--worker-viz\", action=\"store_true\", help=\"Generate worker-specific visualizations\")\n    parser.add_argument(\"--worker-quality\", action=\"store_true\", help=\"Generate worker quality analysis visualizations\")\n    parser.add_argument(\"--timeline-viz\", action=\"store_true\", help=\"Generate timeline visualizations\")\n    parser.add_argument(\"--quality-viz\", action=\"store_true\", help=\"Generate quality distribution visualizations\")\n    parser.add_argument(\"--container-viz\", action=\"store_true\", help=\"Generate container type visualizations\")\n    parser.add_argument(\"--ward-viz\", action=\"store_true\", help=\"Generate ward-level visualizations\")\n\n    args = parser.parse_args()\n\n    print(\"=\" * 70)\n    print(\"Mosquito Breeding Site Metadata Integrator\".center(70))\n    print(\"=\" * 70)\n\n    # Print configuration\n    print(\"\\nConfiguration:\")\n    print(f\"  JSON Metadata: {args.json}\")\n    print(f\"  Processed Images: {args.images}\")\n    print(f\"  Output Directory: {args.output}\")\n    print(f\"  ID Field: {args.id_field}\")\n    print(f\"  Environment: {args.env}\")\n\n    if args.annotate:\n        print(f\"  Creating annotated images\")\n\n    if args.export:\n        print(f\"  Exporting data as: {args.export}\")\n\n    print(\"\")\n\n    try:\n        # Create integrator\n        integrator = MetadataIntegrator(\n            json_file_path=args.json,\n            processed_dir=args.images,\n            output_dir=args.output,\n            id_field=args.id_field,\n            image_url_field=args.image_url_field,\n            env=args.env\n        )\n\n        # Integrate metadata with processed images\n        print(\"\\nIntegrating metadata with processed images...\")\n        integrated_data = integrator.integrate_metadata()\n\n        if integrated_data is None or len(integrated_data) == 0:\n            print(\"No data was integrated. Check input files and paths.\")\n            return 1\n\n        # Get metadata and processed counts from match_stats\n        metadata_count = integrator.match_stats['total_metadata_records']\n        processed_count = integrator.match_stats['total_processed_images']\n\n        # Calculate match rate\n        match_rate = (len(integrated_data) / metadata_count) * 100 if metadata_count &gt; 0 else 0\n\n        print(f\"Integration summary:\")\n        print(f\"  Metadata records: {metadata_count}\")\n        print(f\"  Processed images: {processed_count}\")\n        print(f\"  Integrated records: {len(integrated_data)}\")\n        print(f\"  Match rate: {match_rate:.1f}%\")\n\n        # Analyze data\n        print(\"\\nAnalyzing integrated data...\")\n        analysis = integrator.analyze_data()\n\n        # Generate specific visualizations if requested\n        if any([args.worker_viz, args.worker_quality, args.timeline_viz, args.quality_viz, args.container_viz, args.ward_viz]):\n            print(\"\\nGenerating requested visualizations...\")\n            viz_list = []\n\n            # Find common columns\n            breeding_col = integrator._find_column(['breed', 'spot', 'larv', 'pupa'])\n            worker_col = integrator._find_column(['worker', 'user', 'asha', 'uid', 'Uid'])\n            ward_col = integrator._find_column(['ward', 'area', 'zone', 'district'])\n            date_col = integrator._find_column(['date', 'time', 'timestamp'])\n            container_col = integrator._find_column(['container', 'type'])\n            quality_cols = [col for col in integrated_data.columns \n                          if col.startswith('is_') and col in ['is_dark', 'is_blurry', \n                                                              'is_bright', 'is_duplicate', \n                                                              'is_outlier', 'is_redundant']]\n\n            # Worker visualizations\n            if args.worker_viz and worker_col:\n                print(f\"  Generating worker visualizations...\")\n                if breeding_col:\n                    integrator.viz.plot_worker_breeding_spots(\n                        data=integrated_data,\n                        worker_column=worker_col,\n                        spot_column=breeding_col,\n                        title=f\"Breeding Spot Counts per {worker_col.capitalize()}\",\n                        filename=\"worker_breeding_spots.png\"\n                    )\n                    viz_list.append(\"worker_breeding_spots.png\")\n\n                duplicate_col = integrator._find_column(['duplicate', 'redundant', 'is_duplicate', 'is_redundant'])\n                if duplicate_col:\n                    integrator.viz.plot_worker_duplicates(\n                        data=integrated_data,\n                        worker_column=worker_col,\n                        duplicate_column=duplicate_col,\n                        title=f\"Duplicate Submissions per {worker_col.capitalize()}\",\n                        filename=\"worker_duplicates.png\"\n                    )\n                    viz_list.append(\"worker_duplicates.png\")\n\n            # Worker quality analysis\n            if args.worker_quality and worker_col and quality_cols:\n                print(f\"  Generating worker quality analysis...\")\n                integrator.viz.plot_worker_quality_analysis(\n                    data=integrated_data,\n                    worker_column=worker_col,\n                    quality_columns=quality_cols,\n                    title=f\"Quality Issues by {worker_col.capitalize()}\",\n                    filename=\"worker_quality_analysis.png\"\n                )\n                viz_list.append(\"worker_quality_analysis.png\")\n\n            # Timeline visualizations\n            if args.timeline_viz and date_col:\n                print(f\"  Generating timeline visualizations...\")\n                if breeding_col:\n                    integrator.viz.plot_timeline_captures(\n                        data=integrated_data,\n                        date_column=date_col,\n                        category_column=breeding_col,\n                        category_value=\"Yes\",\n                        title=\"Timeline of Image Captures\",\n                        filename=\"image_timeline.png\",\n                        time_unit='W',\n                        show_cumulative=True\n                    )\n                else:\n                    integrator.viz.plot_timeline_captures(\n                        data=integrated_data,\n                        date_column=date_col,\n                        title=\"Timeline of Image Captures\",\n                        filename=\"image_timeline.png\",\n                        time_unit='W',\n                        show_cumulative=True\n                    )\n                viz_list.append(\"image_timeline.png\")\n\n            # Quality visualizations\n            if args.quality_viz and quality_cols:\n                print(f\"  Generating quality visualizations...\")\n                integrator.viz.plot_quality_distribution(\n                    data=integrated_data,\n                    quality_columns=quality_cols,\n                    title=\"Image Quality Issues\",\n                    filename=\"quality_issues.png\",\n                    by_worker=worker_col\n                )\n                viz_list.append(\"quality_issues.png\")\n\n            # Container visualizations\n            if args.container_viz and container_col and breeding_col:\n                print(f\"  Generating container visualizations...\")\n                integrator.viz.plot_container_breakdown(\n                    data=integrated_data,\n                    container_column=container_col,\n                    breeding_column=breeding_col,\n                    title=\"Container Types vs. Breeding Spots\",\n                    filename=\"container_breeding.png\"\n                )\n                viz_list.append(\"container_breeding.png\")\n\n            # Ward-level visualizations\n            if args.ward_viz and ward_col:\n                print(f\"  Generating ward visualizations...\")\n                if breeding_col:\n                    integrator.viz.plot_worker_breeding_spots(\n                        data=integrated_data,\n                        worker_column=ward_col,\n                        spot_column=breeding_col,\n                        title=f\"Breeding Spot Counts per {ward_col.capitalize()}\",\n                        filename=\"ward_breeding_spots.png\"\n                    )\n                    viz_list.append(\"ward_breeding_spots.png\")\n\n                if quality_cols:\n                    integrator.viz.plot_ward_summary(\n                        data=integrated_data,\n                        ward_column=ward_col,\n                        quality_columns=quality_cols,\n                        breeding_column=breeding_col,\n                        title=f\"{ward_col.capitalize()}-Level Quality Summary\",\n                        filename=\"ward_summary.png\"\n                    )\n                    viz_list.append(\"ward_summary.png\")\n\n            # If no specific visualization was requested or visualization-only mode,\n            # generate all visualizations\n            if not viz_list or args.visualize_only:\n                print(\"\\nGenerating all visualizations...\")\n                viz_list = integrator.generate_visualizations(analysis)\n\n                # Additionally generate worker quality analysis if worker column exists\n                if worker_col and quality_cols and \"worker_quality_analysis.png\" not in viz_list:\n                    integrator.viz.plot_worker_quality_analysis(\n                        data=integrated_data,\n                        worker_column=worker_col,\n                        quality_columns=quality_cols,\n                        title=f\"Quality Issues by {worker_col.capitalize()}\",\n                        filename=\"worker_quality_analysis.png\"\n                    )\n                    viz_list.append(\"worker_quality_analysis.png\")\n        else:\n            # Generate all visualizations as part of normal flow\n            print(\"\\nGenerating visualizations...\")\n            viz_list = integrator.generate_visualizations(analysis)\n\n            # Additionally generate worker quality analysis if worker column exists\n            worker_col = integrator._find_column(['worker', 'user', 'asha', 'uid', 'Uid'])\n            quality_cols = [col for col in integrated_data.columns \n                          if col.startswith('is_') and col in ['is_dark', 'is_blurry', \n                                                              'is_bright', 'is_duplicate', \n                                                              'is_outlier', 'is_redundant']]\n            if worker_col and quality_cols:\n                integrator.viz.plot_worker_quality_analysis(\n                    data=integrated_data,\n                    worker_column=worker_col,\n                    quality_columns=quality_cols,\n                    title=f\"Quality Issues by {worker_col.capitalize()}\",\n                    filename=\"worker_quality_analysis.png\"\n                )\n                viz_list.append(\"worker_quality_analysis.png\")\n\n        # Create annotated images if requested\n        if args.annotate:\n            print(\"\\nCreating annotated images...\")\n            annotated_paths = integrator.create_annotated_images()\n            print(f\"Created {len(annotated_paths)} annotated images\")\n\n        # Export data if requested\n        if args.export:\n            print(f\"\\nExporting integrated data as {args.export}...\")\n            export_path = integrator.export_integrated_data(args.export)\n            print(f\"Data exported to: {export_path}\")\n\n        # Generate report (unless visualize-only mode)\n        if not args.visualize_only:\n            print(\"\\nGenerating comprehensive report...\")\n            report_path = integrator.generate_report()\n\n            # Try to open the report\n            try:\n                import webbrowser\n                print(f\"Opening report in browser: {report_path}\")\n                webbrowser.open(f\"file://{os.path.abspath(report_path)}\")\n            except Exception as e:\n                print(f\"  Note: Could not open report automatically ({e})\")\n                print(f\"  Please open manually: {report_path}\")\n\n        print(\"\\n\" + \"=\" * 70)\n        print(\"Processing Complete\".center(70))\n        print(\"=\" * 70)\n\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 1\n\n    return 0\n</code></pre>"},{"location":"api/preprocess/","title":"Preprocessing","text":""},{"location":"api/preprocess/#prismh.core.preprocess","title":"<code>prismh.core.preprocess</code>","text":""},{"location":"api/preprocess/#prismh.core.preprocess.ImagePreprocessor","title":"<code>ImagePreprocessor</code>","text":"<p>A class that uses the fastdup library to preprocess images by identifying invalid files, duplicates, outliers, dark, and blurry images, and segregating them into 'clean' and 'problematic' sets.</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>class ImagePreprocessor:\n    \"\"\"\n    A class that uses the fastdup library to preprocess images by identifying\n    invalid files, duplicates, outliers, dark, and blurry images, and segregating\n    them into 'clean' and 'problematic' sets.\n    \"\"\"\n    def __init__(self, \n                 input_dir: str,\n                 output_dir: str,\n                 ccthreshold: float = 0.9,\n                 outlier_distance: float = 0.68):\n        \"\"\"\n        :param input_dir: Path to the folder that contains all your images.\n        :param output_dir: Path where the cleaned and problematic folders will be created.\n        :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9).\n        :param outlier_distance: Distance threshold for outlier detection (default 0.68).\n        \"\"\"\n        # Convert to absolute paths using pathlib\n        self.input_dir = Path(input_dir).resolve()\n        self.output_dir = Path(output_dir).resolve()\n        self.ccthreshold = ccthreshold\n        self.outlier_distance = outlier_distance\n\n        # Folders for final categorized images using pathlib\n        self.clean_folder = self.output_dir / \"clean\"\n        self.problematic_folder = self.output_dir / \"problematic\"\n        self.invalid_folder = self.problematic_folder / \"invalid\"\n        self.duplicates_folder = self.problematic_folder / \"duplicates\"\n        self.outliers_folder = self.problematic_folder / \"outliers\"\n        self.dark_folder = self.problematic_folder / \"dark\"\n        self.blurry_folder = self.problematic_folder / \"blurry\"\n\n        # Create output directories\n        self._create_directories()\n\n    def _create_directories(self):\n        \"\"\"Create the output directory structure using pathlib.\"\"\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.clean_folder.mkdir(parents=True, exist_ok=True)\n        self.problematic_folder.mkdir(parents=True, exist_ok=True)\n        self.invalid_folder.mkdir(parents=True, exist_ok=True)\n        self.duplicates_folder.mkdir(parents=True, exist_ok=True)\n        self.outliers_folder.mkdir(parents=True, exist_ok=True)\n        self.dark_folder.mkdir(parents=True, exist_ok=True)\n        self.blurry_folder.mkdir(parents=True, exist_ok=True)\n\n    def _extract_filename(self, path):\n        \"\"\"Extract just the filename from a path (relative or absolute) using pathlib\"\"\"\n        # Assuming path is a string from fastdup, convert to Path first\n        return Path(path).name\n\n    def run_preprocessing(self):\n        \"\"\"\n        Run the entire fastdup-based preprocessing pipeline:\n          1) Detect invalid images\n          2) Compute similarity and connected components (duplicates)\n          3) Identify outliers, dark images, and blurry images\n          4) Copy images to respective categories\n        \"\"\"\n        # 1) Create a FastDup object and run the analysis\n        fd = fastdup.create(input_dir=self.input_dir)\n        fd.run(ccthreshold=self.ccthreshold)\n\n        # 2) Identify invalid images\n        broken_images_df = fd.invalid_instances()\n        broken_filenames = [self._extract_filename(path) for path in broken_images_df['filename'].tolist()]\n        print(f\"Found {len(broken_filenames)} invalid images.\")\n\n        # 3) Find duplicates via connected components\n        connected_components_df, _ = fd.connected_components()\n        clusters_df = self._get_clusters(\n            connected_components_df, \n            sort_by='count', \n            min_count=2, \n            ascending=False\n        )\n\n        keep_filenames = []\n        duplicate_filenames = []\n\n        for cluster_file_list in clusters_df.filename:\n            if not cluster_file_list:  # Skip empty lists\n                continue\n\n            # We'll keep the first one and mark the rest as duplicates\n            keep = self._extract_filename(cluster_file_list[0])\n            discard = [self._extract_filename(path) for path in cluster_file_list[1:]]\n\n            keep_filenames.append(keep)\n            duplicate_filenames.extend(discard)\n\n        print(f\"Found {len(set(duplicate_filenames))} duplicates.\")\n\n        # 4) Find outliers (distance &lt; outlier_distance)\n        outlier_df = fd.outliers()\n        outlier_filenames = [\n            self._extract_filename(path) \n            for path in outlier_df[outlier_df.distance &lt; self.outlier_distance].filename_outlier.tolist()\n        ]\n        print(f\"Found {len(outlier_filenames)} outliers with distance &lt; {self.outlier_distance}.\")\n\n        # 5) Dark and blurry images from stats\n        stats_df = fd.img_stats()\n\n        dark_images = stats_df[stats_df['mean'] &lt; 13]    # threshold for darkness\n        dark_filenames = [self._extract_filename(path) for path in dark_images['filename'].tolist()]\n        print(f\"Found {len(dark_filenames)} dark images (mean &lt; 13).\")\n\n        blurry_images = stats_df[stats_df['blur'] &lt; 50]  # threshold for blur\n        blurry_filenames = [self._extract_filename(path) for path in blurry_images['filename'].tolist()]\n        print(f\"Found {len(blurry_filenames)} blurry images (blur &lt; 50).\")\n\n        # 6) Collect all problematic filenames\n        broken_set = set(broken_filenames)\n        duplicates_set = set(duplicate_filenames)\n        outlier_set = set(outlier_filenames)\n        dark_set = set(dark_filenames)\n        blurry_set = set(blurry_filenames)\n        keep_set = set(keep_filenames)\n\n        # 7) Build sets for processing\n        all_problematic = broken_set.union(duplicates_set, outlier_set, dark_set, blurry_set)\n        print(f\"Total problematic images: {len(all_problematic)}\")\n        print(f\"Images to keep from clusters: {len(keep_set)}\")\n\n        # 8) Process all files in the input directory\n        problematic_count = {\n            \"invalid\": 0,\n            \"duplicates\": 0,\n            \"outliers\": 0,\n            \"dark\": 0,\n            \"blurry\": 0\n        }\n\n        clean_count = 0\n        kept_duplicates = 0\n\n        # Get a list of all files using pathlib (assuming flat structure or recursion handled by fastdup already, adjust if needed)\n        # Using rglob to find all files recursively. Filter for actual files.\n        all_paths = [p for p in self.input_dir.rglob('*') if p.is_file()]\n        all_files = [(p, p.name) for p in all_paths]\n\n        print(f\"Found {len(all_files)} total files in input directory: {self.input_dir}\")\n\n        # Process each file\n        for full_path, filename in all_files:\n            # Copy to the problematic folders if needed\n            if filename in broken_set:\n                self._copy_to_folder(full_path, self.invalid_folder)\n                problematic_count[\"invalid\"] += 1\n\n            if filename in duplicates_set:\n                self._copy_to_folder(full_path, self.duplicates_folder)\n                problematic_count[\"duplicates\"] += 1\n\n            if filename in outlier_set:\n                self._copy_to_folder(full_path, self.outliers_folder)\n                problematic_count[\"outliers\"] += 1\n\n            if filename in dark_set:\n                self._copy_to_folder(full_path, self.dark_folder)\n                problematic_count[\"dark\"] += 1\n\n            if filename in blurry_set:\n                self._copy_to_folder(full_path, self.blurry_folder)\n                problematic_count[\"blurry\"] += 1\n\n            # Copy to clean folder if not problematic or if it's a keeper\n            if filename not in all_problematic or filename in keep_set:\n                self._copy_to_folder(full_path, self.clean_folder)\n                clean_count += 1\n                if filename in keep_set:\n                    kept_duplicates += 1\n\n        # Print summary\n        print(\"Copying results:\")\n        print(f\"- Invalid: {problematic_count['invalid']}/{len(broken_set)}\")\n        print(f\"- Duplicates: {problematic_count['duplicates']}/{len(duplicates_set)}\")\n        print(f\"- Outliers: {problematic_count['outliers']}/{len(outlier_set)}\")\n        print(f\"- Dark: {problematic_count['dark']}/{len(dark_set)}\")\n        print(f\"- Blurry: {problematic_count['blurry']}/{len(blurry_set)}\")\n        print(f\"- Clean: {clean_count} (including {kept_duplicates} kept duplicates)\")\n\n    def _copy_to_folder(self, src_path, dest_folder):\n        \"\"\"Copy a file to the destination folder using pathlib\"\"\"\n        # Ensure src_path is a Path object if it comes from all_files list\n        src_path_obj = Path(src_path) \n        filename = src_path_obj.name \n        # Ensure dest_folder is a Path object\n        dest_folder_obj = Path(dest_folder)\n        dest_path = os.path.join(dest_folder, filename)\n        dest_path_obj = dest_folder_obj / filename # Use pathlib join\n        try: \n            shutil.copy2(src_path, dest_path)\n            return True\n        except Exception as e:\n            print(f\"Error copying {src_path} to {dest_folder}: {e}\")\n            return False\n\n    def _get_clusters(self, df, sort_by='count', min_count=2, ascending=False):\n        \"\"\"\n        Given a connected_components DataFrame from fastdup, group into clusters\n        with the specified sorting options.\n        \"\"\"\n        agg_dict = {'filename': list, 'mean_distance': 'max', 'count': 'count'}\n        if 'label' in df.columns:\n            agg_dict['label'] = list\n\n        # only consider rows where 'count' &gt;= min_count\n        df = df[df['count'] &gt;= min_count]\n\n        grouped_df = df.groupby('component_id').agg(agg_dict)\n        grouped_df = grouped_df.sort_values(by=[sort_by], ascending=ascending)\n        return grouped_df\n</code></pre>"},{"location":"api/preprocess/#prismh.core.preprocess.ImagePreprocessor.__init__","title":"<code>__init__(input_dir: str, output_dir: str, ccthreshold: float = 0.9, outlier_distance: float = 0.68)</code>","text":"<p>:param input_dir: Path to the folder that contains all your images. :param output_dir: Path where the cleaned and problematic folders will be created. :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9). :param outlier_distance: Distance threshold for outlier detection (default 0.68).</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>def __init__(self, \n             input_dir: str,\n             output_dir: str,\n             ccthreshold: float = 0.9,\n             outlier_distance: float = 0.68):\n    \"\"\"\n    :param input_dir: Path to the folder that contains all your images.\n    :param output_dir: Path where the cleaned and problematic folders will be created.\n    :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9).\n    :param outlier_distance: Distance threshold for outlier detection (default 0.68).\n    \"\"\"\n    # Convert to absolute paths using pathlib\n    self.input_dir = Path(input_dir).resolve()\n    self.output_dir = Path(output_dir).resolve()\n    self.ccthreshold = ccthreshold\n    self.outlier_distance = outlier_distance\n\n    # Folders for final categorized images using pathlib\n    self.clean_folder = self.output_dir / \"clean\"\n    self.problematic_folder = self.output_dir / \"problematic\"\n    self.invalid_folder = self.problematic_folder / \"invalid\"\n    self.duplicates_folder = self.problematic_folder / \"duplicates\"\n    self.outliers_folder = self.problematic_folder / \"outliers\"\n    self.dark_folder = self.problematic_folder / \"dark\"\n    self.blurry_folder = self.problematic_folder / \"blurry\"\n\n    # Create output directories\n    self._create_directories()\n</code></pre>"},{"location":"api/preprocess/#prismh.core.preprocess.ImagePreprocessor.run_preprocessing","title":"<code>run_preprocessing()</code>","text":"Run the entire fastdup-based preprocessing pipeline <p>1) Detect invalid images 2) Compute similarity and connected components (duplicates) 3) Identify outliers, dark images, and blurry images 4) Copy images to respective categories</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>def run_preprocessing(self):\n    \"\"\"\n    Run the entire fastdup-based preprocessing pipeline:\n      1) Detect invalid images\n      2) Compute similarity and connected components (duplicates)\n      3) Identify outliers, dark images, and blurry images\n      4) Copy images to respective categories\n    \"\"\"\n    # 1) Create a FastDup object and run the analysis\n    fd = fastdup.create(input_dir=self.input_dir)\n    fd.run(ccthreshold=self.ccthreshold)\n\n    # 2) Identify invalid images\n    broken_images_df = fd.invalid_instances()\n    broken_filenames = [self._extract_filename(path) for path in broken_images_df['filename'].tolist()]\n    print(f\"Found {len(broken_filenames)} invalid images.\")\n\n    # 3) Find duplicates via connected components\n    connected_components_df, _ = fd.connected_components()\n    clusters_df = self._get_clusters(\n        connected_components_df, \n        sort_by='count', \n        min_count=2, \n        ascending=False\n    )\n\n    keep_filenames = []\n    duplicate_filenames = []\n\n    for cluster_file_list in clusters_df.filename:\n        if not cluster_file_list:  # Skip empty lists\n            continue\n\n        # We'll keep the first one and mark the rest as duplicates\n        keep = self._extract_filename(cluster_file_list[0])\n        discard = [self._extract_filename(path) for path in cluster_file_list[1:]]\n\n        keep_filenames.append(keep)\n        duplicate_filenames.extend(discard)\n\n    print(f\"Found {len(set(duplicate_filenames))} duplicates.\")\n\n    # 4) Find outliers (distance &lt; outlier_distance)\n    outlier_df = fd.outliers()\n    outlier_filenames = [\n        self._extract_filename(path) \n        for path in outlier_df[outlier_df.distance &lt; self.outlier_distance].filename_outlier.tolist()\n    ]\n    print(f\"Found {len(outlier_filenames)} outliers with distance &lt; {self.outlier_distance}.\")\n\n    # 5) Dark and blurry images from stats\n    stats_df = fd.img_stats()\n\n    dark_images = stats_df[stats_df['mean'] &lt; 13]    # threshold for darkness\n    dark_filenames = [self._extract_filename(path) for path in dark_images['filename'].tolist()]\n    print(f\"Found {len(dark_filenames)} dark images (mean &lt; 13).\")\n\n    blurry_images = stats_df[stats_df['blur'] &lt; 50]  # threshold for blur\n    blurry_filenames = [self._extract_filename(path) for path in blurry_images['filename'].tolist()]\n    print(f\"Found {len(blurry_filenames)} blurry images (blur &lt; 50).\")\n\n    # 6) Collect all problematic filenames\n    broken_set = set(broken_filenames)\n    duplicates_set = set(duplicate_filenames)\n    outlier_set = set(outlier_filenames)\n    dark_set = set(dark_filenames)\n    blurry_set = set(blurry_filenames)\n    keep_set = set(keep_filenames)\n\n    # 7) Build sets for processing\n    all_problematic = broken_set.union(duplicates_set, outlier_set, dark_set, blurry_set)\n    print(f\"Total problematic images: {len(all_problematic)}\")\n    print(f\"Images to keep from clusters: {len(keep_set)}\")\n\n    # 8) Process all files in the input directory\n    problematic_count = {\n        \"invalid\": 0,\n        \"duplicates\": 0,\n        \"outliers\": 0,\n        \"dark\": 0,\n        \"blurry\": 0\n    }\n\n    clean_count = 0\n    kept_duplicates = 0\n\n    # Get a list of all files using pathlib (assuming flat structure or recursion handled by fastdup already, adjust if needed)\n    # Using rglob to find all files recursively. Filter for actual files.\n    all_paths = [p for p in self.input_dir.rglob('*') if p.is_file()]\n    all_files = [(p, p.name) for p in all_paths]\n\n    print(f\"Found {len(all_files)} total files in input directory: {self.input_dir}\")\n\n    # Process each file\n    for full_path, filename in all_files:\n        # Copy to the problematic folders if needed\n        if filename in broken_set:\n            self._copy_to_folder(full_path, self.invalid_folder)\n            problematic_count[\"invalid\"] += 1\n\n        if filename in duplicates_set:\n            self._copy_to_folder(full_path, self.duplicates_folder)\n            problematic_count[\"duplicates\"] += 1\n\n        if filename in outlier_set:\n            self._copy_to_folder(full_path, self.outliers_folder)\n            problematic_count[\"outliers\"] += 1\n\n        if filename in dark_set:\n            self._copy_to_folder(full_path, self.dark_folder)\n            problematic_count[\"dark\"] += 1\n\n        if filename in blurry_set:\n            self._copy_to_folder(full_path, self.blurry_folder)\n            problematic_count[\"blurry\"] += 1\n\n        # Copy to clean folder if not problematic or if it's a keeper\n        if filename not in all_problematic or filename in keep_set:\n            self._copy_to_folder(full_path, self.clean_folder)\n            clean_count += 1\n            if filename in keep_set:\n                kept_duplicates += 1\n\n    # Print summary\n    print(\"Copying results:\")\n    print(f\"- Invalid: {problematic_count['invalid']}/{len(broken_set)}\")\n    print(f\"- Duplicates: {problematic_count['duplicates']}/{len(duplicates_set)}\")\n    print(f\"- Outliers: {problematic_count['outliers']}/{len(outlier_set)}\")\n    print(f\"- Dark: {problematic_count['dark']}/{len(dark_set)}\")\n    print(f\"- Blurry: {problematic_count['blurry']}/{len(blurry_set)}\")\n    print(f\"- Clean: {clean_count} (including {kept_duplicates} kept duplicates)\")\n</code></pre>"},{"location":"api/simclr/","title":"SimCLR","text":""},{"location":"api/simclr/#prismh.models.simclr","title":"<code>prismh.models.simclr</code>","text":""},{"location":"api/simclr/#prismh.models.simclr.NTXentLoss","title":"<code>NTXentLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Normalized Temperature-scaled Cross Entropy Loss from SimCLR paper</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class NTXentLoss(nn.Module):\n    \"\"\"\n    Normalized Temperature-scaled Cross Entropy Loss from SimCLR paper\n    \"\"\"\n    def __init__(self, temperature=0.5, batch_size=32):\n        super(NTXentLoss, self).__init__()\n        self.temperature = temperature\n        self.batch_size = batch_size\n        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n        self.similarity_f = nn.CosineSimilarity(dim=2)\n        # Mask to remove positive examples from the denominator of the loss function\n        mask = torch.ones((2 * batch_size, 2 * batch_size), dtype=bool)\n        mask.fill_diagonal_(0)\n\n        for i in range(batch_size):\n            mask[i, batch_size + i] = 0\n            mask[batch_size + i, i] = 0\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        Calculate NT-Xent loss\n        Args:\n            z_i, z_j: Normalized projection vectors from the two augmented views\n        \"\"\"\n        # Calculate cosine similarity\n        representations = torch.cat([z_i, z_j], dim=0)\n        similarity_matrix = self.similarity_f(representations.unsqueeze(1), representations.unsqueeze(0)) / self.temperature\n\n        # Mask out the positives\n        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n        positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n        # Mask out the diagnonal (self-similarity)\n        negatives = similarity_matrix[self.mask].reshape(2 * self.batch_size, -1)\n\n        # Create labels - positives are the \"correct\" predictions\n        labels = torch.zeros(2 * self.batch_size).long().to(positives.device)\n\n        # Calculate loss\n        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n        loss = self.criterion(logits, labels)\n        loss = loss / (2 * self.batch_size)\n\n        return loss\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.NTXentLoss.forward","title":"<code>forward(z_i, z_j)</code>","text":"<p>Calculate NT-Xent loss Args:     z_i, z_j: Normalized projection vectors from the two augmented views</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def forward(self, z_i, z_j):\n    \"\"\"\n    Calculate NT-Xent loss\n    Args:\n        z_i, z_j: Normalized projection vectors from the two augmented views\n    \"\"\"\n    # Calculate cosine similarity\n    representations = torch.cat([z_i, z_j], dim=0)\n    similarity_matrix = self.similarity_f(representations.unsqueeze(1), representations.unsqueeze(0)) / self.temperature\n\n    # Mask out the positives\n    sim_ij = torch.diag(similarity_matrix, self.batch_size)\n    sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n    positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n    # Mask out the diagnonal (self-similarity)\n    negatives = similarity_matrix[self.mask].reshape(2 * self.batch_size, -1)\n\n    # Create labels - positives are the \"correct\" predictions\n    labels = torch.zeros(2 * self.batch_size).long().to(positives.device)\n\n    # Calculate loss\n    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n    loss = self.criterion(logits, labels)\n    loss = loss / (2 * self.batch_size)\n\n    return loss\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.SimCLRDataset","title":"<code>SimCLRDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimCLR that returns two augmented views of each image</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRDataset(Dataset):\n    \"\"\"Dataset for SimCLR that returns two augmented views of each image\"\"\"\n\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (string or Path): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.image_paths = []\n\n        # Collect all image paths\n        for entry in self.root_dir.iterdir():\n            if entry.is_file() and entry.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n                self.image_paths.append(entry)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            # Apply the same transform twice to get two different augmented views\n            view1 = self.transform(image)\n            view2 = self.transform(image)\n            return view1, view2\n\n        return image, image\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.SimCLRDataset.__init__","title":"<code>__init__(root_dir, transform=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>string or Path</code> <p>Directory with all the images.</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied on a sample.</p> <code>None</code> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def __init__(self, root_dir, transform=None):\n    \"\"\"\n    Args:\n        root_dir (string or Path): Directory with all the images.\n        transform (callable, optional): Optional transform to be applied on a sample.\n    \"\"\"\n    self.root_dir = Path(root_dir)\n    self.transform = transform\n    self.image_paths = []\n\n    # Collect all image paths\n    for entry in self.root_dir.iterdir():\n        if entry.is_file() and entry.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n            self.image_paths.append(entry)\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.SimCLRModel","title":"<code>SimCLRModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>SimCLR model with encoder and projection head</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRModel(nn.Module):\n    \"\"\"SimCLR model with encoder and projection head\"\"\"\n\n    def __init__(self, base_model='resnet50', pretrained=True, output_dim=128):\n        super(SimCLRModel, self).__init__()\n\n        # Load the base encoder model (e.g., ResNet-50)\n        if base_model == 'resnet50':\n            self.encoder = models.resnet50(pretrained=pretrained)\n            self.encoder_dim = 2048\n        elif base_model == 'resnet18':\n            self.encoder = models.resnet18(pretrained=pretrained)\n            self.encoder_dim = 512\n        else:\n            raise ValueError(f\"Unsupported base model: {base_model}\")\n\n        # Replace the final fully connected layer\n        self.encoder.fc = nn.Identity()\n\n        # Add projection head\n        self.projection_head = SimCLRProjectionHead(\n            input_dim=self.encoder_dim,\n            output_dim=output_dim\n        )\n\n    def forward(self, x):\n        features = self.encoder(x)\n        projections = self.projection_head(features)\n        return features, F.normalize(projections, dim=1)\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.SimCLRProjectionHead","title":"<code>SimCLRProjectionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Projection head for SimCLR</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRProjectionHead(nn.Module):\n    \"\"\"Projection head for SimCLR\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=2048, output_dim=128):\n        super(SimCLRProjectionHead, self).__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.projection(x)\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.get_simclr_transforms","title":"<code>get_simclr_transforms(size=224)</code>","text":"<p>Get the augmentation transforms for SimCLR</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def get_simclr_transforms(size=224):\n    \"\"\"Get the augmentation transforms for SimCLR\"\"\"\n    color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n\n    # Calculate kernel size and make sure it's odd\n    kernel_size = int(0.1 * size)\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n    if kernel_size &lt; 3:\n        kernel_size = 3  # Minimum kernel size\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=size),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=kernel_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    return train_transform\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.save_embeddings","title":"<code>save_embeddings(model, data_loader, output_file, device)</code>","text":"<p>Extract and save embeddings from trained model</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def save_embeddings(model, data_loader, output_file, device):\n    \"\"\"Extract and save embeddings from trained model\"\"\"\n    output_file = Path(output_file)\n    model.eval()\n    embeddings = []\n    file_paths = [] # Keep paths as Path objects internally\n\n    with torch.no_grad():\n        # Assuming data_loader yields ((img1, img2), path) or similar\n        for item in tqdm(data_loader, desc=\"Extracting embeddings\"):\n            # Adapt based on actual data_loader structure\n            if isinstance(item, (tuple, list)) and len(item) == 2:\n                 # Handle cases like ((img1, img2), path) or (img_tuple, path)\n                if isinstance(item[0], (tuple, list)) and len(item[0]) &gt;= 1:\n                    img_data = item[0][0] # Get the first image view for features\n                else:\n                    img_data = item[0] # Assume item[0] is the image tensor\n\n                path_data = item[1]\n            else:\n                 # Fallback or error handling if structure is unexpected\n                 print(f\"Unexpected item structure in data_loader: {type(item)}\")\n                 continue # Or raise an error\n\n            img_data = img_data.to(device)\n            features, _ = model(img_data)\n            embeddings.append(features.cpu().numpy())\n\n            # Ensure paths are handled correctly (list or single Path)\n            if isinstance(path_data, list):\n                 file_paths.extend([Path(p) for p in path_data])\n            else:\n                 file_paths.append(Path(path_data))\n\n    if not embeddings:\n         print(f\"No embeddings extracted for {output_file}\")\n         return None, None\n\n    embeddings = np.vstack(embeddings)\n\n    # Save embeddings and convert file paths to strings for npz compatibility\n    np.savez(str(output_file), embeddings=embeddings, file_paths=[str(p) for p in file_paths])\n    print(f\"Saved {len(embeddings)} embeddings to {output_file}\")\n\n    return embeddings, file_paths # Return original Path objects if needed downstream\n</code></pre>"},{"location":"api/simclr/#prismh.models.simclr.train_simclr","title":"<code>train_simclr(model, train_loader, optimizer, criterion, device, epochs=100, checkpoint_dir='checkpoints', resume_from=None, early_stopping_patience=10, validation_loader=None, writer=None)</code>","text":"<p>Train the SimCLR model with proper analytics and early stopping</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def train_simclr(model, train_loader, optimizer, criterion, device, epochs=100, checkpoint_dir=\"checkpoints\", \n                 resume_from=None, early_stopping_patience=10, validation_loader=None, writer=None):\n    \"\"\"Train the SimCLR model with proper analytics and early stopping\"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize trackers\n    best_loss = float('inf')\n    best_model_path = checkpoint_dir / 'best_model.pt'\n    patience_counter = 0\n    start_epoch = 0\n    train_losses = []\n    val_losses = []\n    lr_history = []\n    global_step = 0\n\n    # Resume from checkpoint if available\n    if resume_from:\n        resume_path = Path(resume_from)\n        if resume_path.exists():\n            print(f\"Resuming training from checkpoint: {resume_path}\")\n            checkpoint = torch.load(resume_path, map_location=device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            start_epoch = checkpoint['epoch'] + 1\n            if 'best_loss' in checkpoint:\n                best_loss = checkpoint['best_loss']\n            if 'train_losses' in checkpoint:\n                train_losses = checkpoint['train_losses']\n            if 'val_losses' in checkpoint:\n                val_losses = checkpoint['val_losses']\n            if 'lr_history' in checkpoint:\n                lr_history = checkpoint['lr_history']\n            if 'global_step' in checkpoint:\n                global_step = checkpoint['global_step']\n            print(f\"Resuming from epoch {start_epoch}, best loss: {best_loss:.4f}, global step: {global_step}\")\n\n    # Create a learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n\n    # Log model graph to TensorBoard if available\n    if writer is not None:\n        # Get a batch of data for graph visualization\n        sample_images, _ = next(iter(train_loader))\n        sample_images = sample_images.to(device)\n        writer.add_graph(model, sample_images)\n\n        # Log some example augmented pairs\n        fig = plt.figure(figsize=(12, 6))\n        for i in range(min(4, len(sample_images))):\n            ax1 = fig.add_subplot(2, 4, i+1)\n            img = sample_images[i].cpu().permute(1, 2, 0).numpy()\n            # Denormalize image\n            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img = np.clip(img, 0, 1)\n            ax1.imshow(img)\n            ax1.set_title(f\"View 1 - img {i}\")\n            ax1.axis('off')\n\n            sample2, _ = next(iter(train_loader))\n            ax2 = fig.add_subplot(2, 4, i+5)\n            img2 = sample2[i].cpu().permute(1, 2, 0).numpy()\n            img2 = img2 * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img2 = np.clip(img2, 0, 1)\n            ax2.imshow(img2)\n            ax2.set_title(f\"View 2 - img {i}\")\n            ax2.axis('off')\n\n        plt.tight_layout()\n        writer.add_figure('Example Augmented Pairs', fig, global_step=0)\n\n    # Function to evaluate on validation set\n    def evaluate():\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for images1, images2 in validation_loader:\n                images1, images2 = images1.to(device), images2.to(device)\n                _, z1 = model(images1)\n                _, z2 = model(images2)\n                loss = criterion(z1, z2)\n                total_val_loss += loss.item()\n        return total_val_loss / len(validation_loader)\n\n    # Function to log embeddings\n    def log_embeddings(step):\n        if writer is None:\n            return\n\n        # Extract embeddings for visualization\n        model.eval()\n        embeddings = []\n        imgs = []\n        with torch.no_grad():\n            for i, (images1, _) in enumerate(validation_loader):\n                if i &gt;= 2:  # Limit to a few batches for visualization\n                    break\n                images1 = images1.to(device)\n                features, _ = model(images1)\n                embeddings.append(features.cpu().numpy())\n                imgs.append(images1.cpu())\n\n        if not embeddings:\n            return\n\n        embeddings = np.vstack(embeddings)\n        imgs = torch.cat(imgs, dim=0)\n\n        # Use t-SNE for dimensionality reduction\n        if len(embeddings) &gt; 10:  # Need enough samples for meaningful t-SNE\n            tsne = TSNE(n_components=2, random_state=42)\n            embeddings_2d = tsne.fit_transform(embeddings)\n\n            # Plot t-SNE visualization\n            fig = plt.figure(figsize=(10, 10))\n            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n            plt.title('t-SNE of Feature Embeddings')\n            writer.add_figure('Embeddings/t-SNE', fig, global_step=step)\n\n        # Log embeddings with images\n        writer.add_embedding(\n            mat=torch.from_numpy(embeddings),\n            label_img=imgs,\n            global_step=step,\n            tag='features' # Explicit tag for embeddings\n        )\n\n    print(f\"Starting training from epoch {start_epoch+1}/{epochs}\")\n    for epoch in range(start_epoch, epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for i, (images1, images2) in enumerate(progress_bar):\n            # Move images to device\n            images1 = images1.to(device)\n            images2 = images2.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass for both augmented views\n            _, z1 = model(images1)\n            _, z2 = model(images2)\n\n            # Calculate loss\n            loss = criterion(z1, z2)\n\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Update statistics\n            running_loss += loss.item()\n            progress_bar.set_postfix({'loss': loss.item()})\n\n            # Log to TensorBoard (every 10 batches)\n            if writer is not None and i % 10 == 0:\n                writer.add_scalar('Batch/train_loss', loss.item(), global_step)\n                global_step += 1\n\n        # Record average loss for the epoch\n        epoch_loss = running_loss / len(train_loader)\n        train_losses.append(epoch_loss)\n\n        # Track current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        lr_history.append(current_lr)\n\n        # Validation phase (if validation loader provided)\n        val_loss = None\n        if validation_loader:\n            val_loss = evaluate()\n            val_losses.append(val_loss)\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n\n            # Log metrics to TensorBoard\n            if writer is not None:\n                writer.add_scalar('Epoch/train_loss', epoch_loss, epoch)\n                writer.add_scalar('Epoch/val_loss', val_loss, epoch)\n                writer.add_scalar('Epoch/learning_rate', current_lr, epoch)\n\n                # Log embeddings periodically\n                if epoch % 5 == 0 or epoch == epochs - 1:\n                    log_embeddings(epoch)\n\n            # Update scheduler based on validation loss\n            scheduler.step(val_loss)\n\n            # Early stopping check\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                # Save best model\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': val_loss,\n                    'best_loss': best_loss,\n                    'train_losses': train_losses,\n                    'val_losses': val_losses,\n                    'lr_history': lr_history,\n                    'global_step': global_step,\n                }, best_model_path)\n                patience_counter = 0\n                print(f\"New best model saved with validation loss: {best_loss:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"Validation loss did not improve. Patience: {patience_counter}/{early_stopping_patience}\")\n\n                if patience_counter &gt;= early_stopping_patience:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    # Load the best model before returning\n                    checkpoint = torch.load(best_model_path, map_location=device)\n                    model.load_state_dict(checkpoint['model_state_dict'])\n                    break\n        else:\n            # If no validation set, use training loss\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}\")\n\n            # Log to TensorBoard\n            if writer is not None:\n                writer.add_scalar('Epoch/train_loss', epoch_loss, epoch)\n                writer.add_scalar('Epoch/learning_rate', current_lr, epoch)\n\n                # Log embeddings periodically\n                if epoch % 5 == 0 or epoch == epochs - 1:\n                    log_embeddings(epoch)\n\n            # Save if better than best so far\n            if epoch_loss &lt; best_loss:\n                best_loss = epoch_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': epoch_loss,\n                    'best_loss': best_loss,\n                    'train_losses': train_losses,\n                    'val_losses': val_losses,\n                    'lr_history': lr_history,\n                    'global_step': global_step,\n                }, best_model_path)\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter &gt;= early_stopping_patience:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    # Load the best model before returning\n                    checkpoint = torch.load(best_model_path, map_location=device)\n                    model.load_state_dict(checkpoint['model_state_dict'])\n                    break\n\n            # Update scheduler based on training loss\n            scheduler.step(epoch_loss)\n\n        # Regular checkpoint (every 5 epochs)\n        if (epoch + 1) % 5 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': val_loss if validation_loader else epoch_loss,\n                'best_loss': best_loss,\n                'train_losses': train_losses,\n                'val_losses': val_losses,\n                'lr_history': lr_history,\n                'global_step': global_step,\n            }, checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pt')\n\n    # Plot training curves\n    plt.figure(figsize=(15, 5))\n\n    # Plot loss curves\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    if val_losses:\n        plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('SimCLR Training Loss')\n    plt.legend()\n\n    # Plot learning rate\n    plt.subplot(1, 2, 2)\n    plt.plot(lr_history)\n    plt.xlabel('Epoch')\n    plt.ylabel('Learning Rate')\n    plt.title('Learning Rate Schedule')\n    plt.yscale('log')\n\n    plt.tight_layout()\n    plt.savefig(checkpoint_dir / 'training_curves.png')\n\n    # Save training history as CSV for further analysis\n    history = pd.DataFrame({\n        'epoch': list(range(1, len(train_losses) + 1)),\n        'train_loss': train_losses,\n        'val_loss': val_losses if val_losses else [None] * len(train_losses),\n        'learning_rate': lr_history\n    })\n    history.to_csv(checkpoint_dir / 'training_history.csv', index=False)\n\n    print(f\"Training completed. Best loss: {best_loss:.4f}\")\n    print(f\"Training analytics saved to {checkpoint_dir}/\")\n\n    return model, {'train_losses': train_losses, 'val_losses': val_losses, 'lr_history': lr_history}\n</code></pre>"},{"location":"api/core/extract_embeddings/","title":"Feature Extraction API","text":"<p>The feature extraction module extracts meaningful visual representations from images using pre-trained SimCLR models. These embeddings capture semantic information about mosquito breeding spots and can be used for downstream tasks like clustering and classification.</p>"},{"location":"api/core/extract_embeddings/#overview","title":"Overview","text":"<p>The feature extraction process:</p> <ol> <li>Model Loading: Loads a pre-trained or fine-tuned SimCLR model</li> <li>Data Processing: Applies standardized transforms to images</li> <li>Feature Extraction: Generates dense feature vectors (embeddings)</li> <li>Storage: Saves embeddings and metadata for further analysis</li> </ol>"},{"location":"api/core/extract_embeddings/#quick-start","title":"Quick Start","text":"<pre><code>from prismh.core.extract_embeddings import extract_embeddings_main\n\n# Extract embeddings using default configuration\nextract_embeddings_main()\n</code></pre>"},{"location":"api/core/extract_embeddings/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Basic feature extraction\npython -m prismh.core.extract_embeddings \\\n    --input_dir /path/to/clean/images \\\n    --output_dir /path/to/embeddings\n\n# With specific model and device\npython -m prismh.core.extract_embeddings \\\n    --input_dir results/clean \\\n    --output_dir results/embeddings \\\n    --model_path models/simclr_finetuned.pt \\\n    --device cuda \\\n    --batch_size 64\n</code></pre>"},{"location":"api/core/extract_embeddings/#api-reference","title":"API Reference","text":""},{"location":"api/core/extract_embeddings/#prismh.core.extract_embeddings.SimCLRModel","title":"<code>SimCLRModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>SimCLR model with encoder and projection head</p> Source code in <code>src/prismh/core/extract_embeddings.py</code> <pre><code>class SimCLRModel(nn.Module):\n    \"\"\"SimCLR model with encoder and projection head\"\"\"\n    def __init__(self, base_model='resnet50', pretrained=True, output_dim=128):\n        super(SimCLRModel, self).__init__()\n        if base_model == 'resnet50':\n            # Use the updated weights parameter for torchvision &gt;= 0.13\n            weights = models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n            self.encoder = models.resnet50(weights=weights)\n            self.encoder_dim = 2048\n        elif base_model == 'resnet18':\n            weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n            self.encoder = models.resnet18(weights=weights)\n            self.encoder_dim = 512\n        else:\n            raise ValueError(f\"Unsupported base model: {base_model}\")\n        self.encoder.fc = nn.Identity()\n        self.projection_head = SimCLRProjectionHead(\n            input_dim=self.encoder_dim,\n            output_dim=output_dim\n        )\n    def forward(self, x):\n        features = self.encoder(x)\n        projections = self.projection_head(features)\n        # Note: For embedding extraction, we only need 'features'\n        return features, projections # Original return for compatibility if needed elsewhere\n</code></pre>"},{"location":"api/core/extract_embeddings/#prismh.core.extract_embeddings.PathBasedDataset","title":"<code>PathBasedDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/prismh/core/extract_embeddings.py</code> <pre><code>class PathBasedDataset(Dataset): # Simplified dataset for extraction\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n    def __len__(self):\n        return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, img_path # Return image and its path\n        except Exception as e:\n            print(f\"Warning: Error loading image {img_path}: {e}\")\n            # Return None or a placeholder if an image fails to load\n            # For simplicity, we'll return None and handle it in the loop\n            return None, img_path\n</code></pre>"},{"location":"api/core/extract_embeddings/#prismh.core.extract_embeddings.extract_embeddings_main","title":"<code>extract_embeddings_main()</code>","text":"Source code in <code>src/prismh/core/extract_embeddings.py</code> <pre><code>def extract_embeddings_main():\n    # --- Configuration ---\n    output_dir = Path(\"simclr_finetuned\")\n    checkpoint_dir = output_dir / \"checkpoints\"\n    splits_file = output_dir / \"data_splits.pkl\"\n    batch_size = 64 # Can be larger for inference\n    num_workers = 0 # Set to 0 for macOS compatibility if needed\n    # --- End Configuration ---\n\n    # Device configuration\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n    elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        device = torch.device('mps')\n        print(\"Using Apple Silicon GPU (MPS)\")\n    else:\n        device = torch.device('cpu')\n        print(\"Using CPU\")\n\n    # Load data splits\n    try:\n        with splits_file.open('rb') as f:\n            splits = pickle.load(f)\n        train_paths = splits['train']\n        val_paths = splits['val']\n        test_paths = splits['test']\n        all_image_paths = train_paths + val_paths + test_paths\n        print(f\"Loaded data splits from {splits_file}\")\n    except FileNotFoundError:\n        print(f\"Error: Data splits file not found at {splits_file}\")\n        print(\"Please ensure 'simclr.py' has been run successfully to generate the splits file.\")\n        return\n    except Exception as e:\n        print(f\"Error loading splits file {splits_file}: {e}\")\n        return\n\n    # Initialize model architecture\n    # Set pretrained=False as we are loading specific weights\n    model = SimCLRModel(base_model='resnet50', pretrained=False, output_dim=128)\n\n    # Find the best checkpoint to load\n    best_model_path = checkpoint_dir / 'best_model.pt'\n    latest_epoch_checkpoint = None\n\n    # Check if checkpoint directory exists\n    if not checkpoint_dir.is_dir():\n        print(f\"Error: Checkpoint directory not found at {checkpoint_dir}\")\n        print(\"Please ensure 'simclr.py' has been run and checkpoints are saved.\")\n        return\n\n    checkpoints = list(checkpoint_dir.glob('checkpoint_epoch_*.pt'))\n\n    if checkpoints:\n        # Sort by epoch number extracted from filename (Path.stem extracts filename without suffix)\n        try:\n            checkpoints.sort(key=lambda p: int(p.stem.split('_')[-1]), reverse=True)\n            latest_epoch_checkpoint = checkpoints[0]\n        except (ValueError, IndexError):\n            print(\"Warning: Could not parse epoch number from checkpoint filenames.\")\n            latest_epoch_checkpoint = None # Reset if parsing fails\n\n    checkpoint_to_load = None\n    if best_model_path.exists():\n        checkpoint_to_load = best_model_path\n        print(f\"Found best model checkpoint: {best_model_path}\")\n    elif latest_epoch_checkpoint:\n        checkpoint_to_load = latest_epoch_checkpoint\n        print(f\"Using latest epoch checkpoint: {latest_epoch_checkpoint}\")\n    else:\n        print(f\"Error: No suitable checkpoint (.pt file starting with 'checkpoint_epoch_' or 'best_model.pt') found in {checkpoint_dir}\")\n        print(\"Please ensure 'simclr.py' has run and saved checkpoints.\")\n        return\n\n    # Load the checkpoint\n    try:\n        checkpoint = torch.load(checkpoint_to_load, map_location=device)\n        # Ensure the checkpoint contains the model state dict\n        if 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(f\"Successfully loaded weights from {checkpoint_to_load}\")\n        else:\n            print(f\"Error: Checkpoint {checkpoint_to_load} does not contain 'model_state_dict'.\")\n            return\n    except FileNotFoundError:\n        print(f\"Error: Checkpoint file not found at {checkpoint_to_load}\")\n        return\n    except Exception as e:\n        print(f\"Error loading checkpoint {checkpoint_to_load}: {e}\")\n        return\n\n    model = model.to(device)\n    model.eval() # Set to evaluation mode\n\n    # Define evaluation transform (consistent preprocessing)\n    eval_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Function to extract embeddings for a given set of paths\n    def run_extraction(paths, output_filename):\n        dataset = PathBasedDataset(paths, transform=eval_transform)\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True if device != torch.device('cpu') else False, # pin_memory only works with CUDA\n            drop_last=False\n        )\n\n        embeddings_list = []\n        filepaths_list = []\n\n        with torch.no_grad():\n            for images, loaded_paths in tqdm(dataloader, desc=f\"Extracting for {output_filename}\"):\n                # Handle potential loading errors from dataset\n                # Filter out None images and their corresponding paths\n                valid_indices = [i for i, img in enumerate(images) if img is not None]\n                if not valid_indices: # Skip batch if all images failed to load\n                    # loaded_paths is a tuple here, need to access elements\n                    failed_paths = [p for i, p in enumerate(loaded_paths) if i not in valid_indices]\n                    if failed_paths: # Only print if there were actually paths that failed\n                        print(f\"Warning: Skipping batch, failed to load images: {failed_paths}\")\n                    continue\n\n                images_tensor = torch.stack([images[i] for i in valid_indices]).to(device)\n                valid_paths = [loaded_paths[i] for i in valid_indices]\n\n                # Get features from the encoder\n                features, _ = model(images_tensor)\n\n                embeddings_list.append(features.cpu().numpy())\n                filepaths_list.extend(valid_paths)\n\n        if embeddings_list:\n            embeddings_np = np.vstack(embeddings_list)\n            # Ensure output directory exists\n            output_dir.mkdir(parents=True, exist_ok=True)\n            output_path = output_dir / output_filename\n            # Save file paths as UTF-8 strings\n            np.savez_compressed(output_path, embeddings=embeddings_np, file_paths=np.array(filepaths_list, dtype='str'))\n            print(f\"Saved {len(embeddings_np)} embeddings to {output_path}\")\n        else:\n            print(f\"No embeddings extracted for {output_filename}. This might happen if all images in the split failed to load.\")\n\n    # Run extraction for all required splits\n    print(\"Starting embedding extraction...\")\n    run_extraction(train_paths, 'train_embeddings.npz')\n    run_extraction(val_paths, 'val_embeddings.npz')\n    run_extraction(test_paths, 'test_embeddings.npz')\n    run_extraction(all_image_paths, 'all_embeddings.npz')\n\n    print(\"Embedding extraction complete.\")\n</code></pre>"},{"location":"api/core/extract_embeddings/#configuration","title":"Configuration","text":""},{"location":"api/core/extract_embeddings/#model-configuration","title":"Model Configuration","text":"Parameter Default Description <code>model_path</code> Auto-detect Path to SimCLR checkpoint <code>base_model</code> <code>resnet50</code> Backbone architecture <code>output_dim</code> 128 Projection head output dimension"},{"location":"api/core/extract_embeddings/#processing-configuration","title":"Processing Configuration","text":"Parameter Default Description <code>batch_size</code> 64 Batch size for inference <code>num_workers</code> 0 DataLoader worker processes <code>device</code> Auto-detect Device (cpu/cuda/mps)"},{"location":"api/core/extract_embeddings/#data-configuration","title":"Data Configuration","text":"Parameter Default Description <code>input_dir</code> <code>preprocess_results/clean</code> Clean images directory <code>output_dir</code> <code>simclr_finetuned</code> Output directory <code>image_size</code> 224 Input image size"},{"location":"api/core/extract_embeddings/#output-format","title":"Output Format","text":""},{"location":"api/core/extract_embeddings/#embeddings-file","title":"Embeddings File","text":"<p>The extraction process generates <code>all_embeddings.npz</code> containing:</p> <pre><code># Load embeddings\ndata = np.load('all_embeddings.npz', allow_pickle=True)\n\nembeddings = data['embeddings']      # Shape: (N, feature_dim)\nfile_paths = data['file_paths']      # Shape: (N,) - corresponding file paths\n</code></pre>"},{"location":"api/core/extract_embeddings/#file-structure","title":"File Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 all_embeddings.npz              # Main embeddings file\n\u251c\u2500\u2500 train_embeddings.npz            # Training set embeddings\n\u251c\u2500\u2500 val_embeddings.npz              # Validation set embeddings\n\u251c\u2500\u2500 test_embeddings.npz             # Test set embeddings\n\u2514\u2500\u2500 extraction_metadata.json        # Extraction configuration\n</code></pre>"},{"location":"api/core/extract_embeddings/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/extract_embeddings/#basic-extraction","title":"Basic Extraction","text":"<pre><code>from prismh.core.extract_embeddings import extract_embeddings_main\nfrom pathlib import Path\nimport numpy as np\n\ndef basic_extraction():\n    # Run extraction with default settings\n    extract_embeddings_main()\n\n    # Load and examine results\n    embeddings_file = Path(\"simclr_finetuned/all_embeddings.npz\")\n    if embeddings_file.exists():\n        data = np.load(embeddings_file, allow_pickle=True)\n        print(f\"Extracted {len(data['embeddings'])} embeddings\")\n        print(f\"Feature dimension: {data['embeddings'].shape[1]}\")\n    else:\n        print(\"No embeddings found. Check configuration.\")\n\nbasic_extraction()\n</code></pre>"},{"location":"api/core/extract_embeddings/#custom-model-path","title":"Custom Model Path","text":"<pre><code>from prismh.core.extract_embeddings import extract_embeddings_main\nimport os\n\ndef extract_with_custom_model():\n    # Set custom model path\n    os.environ['SIMCLR_MODEL_PATH'] = 'models/custom_simclr.pt'\n\n    # Extract embeddings\n    extract_embeddings_main()\n\n    print(\"Extraction completed with custom model\")\n\nextract_with_custom_model()\n</code></pre>"},{"location":"api/core/extract_embeddings/#batch-processing-with-custom-configuration","title":"Batch Processing with Custom Configuration","text":"<pre><code>from prismh.core.extract_embeddings import SimCLRModel, PathBasedDataset\nfrom torch.utils.data import DataLoader\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\ndef custom_extraction(image_dir, model_path, output_file, batch_size=32):\n    \"\"\"Custom feature extraction with full control\"\"\"\n\n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load model\n    model = SimCLRModel(base_model='resnet50', pretrained=False)\n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    model.eval()\n\n    # Prepare data\n    image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n    dataset = PathBasedDataset(image_paths, transform=get_eval_transform())\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    # Extract embeddings\n    all_embeddings = []\n    all_paths = []\n\n    with torch.no_grad():\n        for batch_images, batch_paths in dataloader:\n            if batch_images is not None:\n                batch_images = batch_images.to(device)\n                features, _ = model(batch_images)\n\n                all_embeddings.append(features.cpu().numpy())\n                all_paths.extend(batch_paths)\n\n    # Save results\n    embeddings = np.vstack(all_embeddings)\n    np.savez_compressed(\n        output_file,\n        embeddings=embeddings,\n        file_paths=np.array(all_paths)\n    )\n\n    print(f\"Saved {len(embeddings)} embeddings to {output_file}\")\n\n# Usage\ncustom_extraction(\n    image_dir=\"data/clean_images\",\n    model_path=\"models/simclr_best.pt\",\n    output_file=\"custom_embeddings.npz\"\n)\n</code></pre>"},{"location":"api/core/extract_embeddings/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/core/extract_embeddings/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>import torch\n\n# Optimize for GPU\nif torch.cuda.is_available():\n    # Enable memory efficiency\n    torch.backends.cudnn.benchmark = True\n\n    # Use larger batch sizes\n    batch_size = 128\n\n    # Enable pin memory\n    pin_memory = True\nelse:\n    batch_size = 32\n    pin_memory = False\n</code></pre>"},{"location":"api/core/extract_embeddings/#memory-management","title":"Memory Management","text":"<pre><code>import gc\nimport torch\n\ndef memory_efficient_extraction():\n    \"\"\"Memory-efficient feature extraction\"\"\"\n\n    # Clear GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # Process in smaller batches\n    batch_size = 32\n\n    # Clear variables after use\n    del model, embeddings\n    gc.collect()\n</code></pre>"},{"location":"api/core/extract_embeddings/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nimport numpy as np\n\ndef parallel_extraction(image_dirs, output_dir):\n    \"\"\"Extract embeddings from multiple directories in parallel\"\"\"\n\n    def extract_single_dir(image_dir):\n        dir_name = Path(image_dir).name\n        output_file = Path(output_dir) / f\"{dir_name}_embeddings.npz\"\n\n        # Run extraction for this directory\n        custom_extraction(image_dir, \"models/simclr.pt\", output_file)\n        return output_file\n\n    # Process directories in parallel\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(extract_single_dir, dir_path) \n                  for dir_path in image_dirs]\n\n        results = [future.result() for future in futures]\n\n    print(f\"Completed parallel extraction: {results}\")\n</code></pre>"},{"location":"api/core/extract_embeddings/#integration-with-pipeline","title":"Integration with Pipeline","text":""},{"location":"api/core/extract_embeddings/#after-preprocessing","title":"After Preprocessing","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\nfrom prismh.core.extract_embeddings import extract_embeddings_main\n\ndef preprocess_and_extract():\n    \"\"\"Complete preprocessing and feature extraction\"\"\"\n\n    # Step 1: Preprocess images\n    preprocessor = ImagePreprocessor(\n        input_dir=\"raw_images\",\n        output_dir=\"processed\"\n    )\n    preprocessor.run_preprocessing()\n\n    # Step 2: Extract features from clean images\n    # Update configuration to use clean images\n    import os\n    os.environ['CLEAN_IMAGES_DIR'] = 'processed/clean'\n\n    extract_embeddings_main()\n\n    print(\"Preprocessing and feature extraction completed\")\n\npreprocess_and_extract()\n</code></pre>"},{"location":"api/core/extract_embeddings/#before-clustering","title":"Before Clustering","text":"<pre><code>from prismh.core.extract_embeddings import extract_embeddings_main\nfrom prismh.core.cluster_embeddings import cluster_main\n\ndef extract_and_cluster():\n    \"\"\"Feature extraction followed by clustering\"\"\"\n\n    # Extract embeddings\n    extract_embeddings_main()\n\n    # Run clustering on embeddings\n    cluster_main()\n\n    print(\"Feature extraction and clustering completed\")\n\nextract_and_cluster()\n</code></pre>"},{"location":"api/core/extract_embeddings/#model-compatibility","title":"Model Compatibility","text":""},{"location":"api/core/extract_embeddings/#supported-architectures","title":"Supported Architectures","text":"Model Backbone Feature Dim Use Case SimCLR-ResNet18 ResNet-18 512 Fast inference SimCLR-ResNet50 ResNet-50 2048 Best performance Custom SimCLR Various Configurable Domain-specific"},{"location":"api/core/extract_embeddings/#loading-different-models","title":"Loading Different Models","text":"<pre><code># Load ImageNet pretrained\nmodel = SimCLRModel(base_model='resnet50', pretrained=True)\n\n# Load custom checkpoint\ncheckpoint = torch.load('custom_model.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Load fine-tuned model\nmodel = SimCLRModel(base_model='resnet50', pretrained=False)\nmodel.load_state_dict(torch.load('finetuned_simclr.pt'))\n</code></pre>"},{"location":"api/core/extract_embeddings/#quality-assessment","title":"Quality Assessment","text":""},{"location":"api/core/extract_embeddings/#embedding-quality-metrics","title":"Embedding Quality Metrics","text":"<pre><code>from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef assess_embedding_quality(embeddings_file):\n    \"\"\"Assess the quality of extracted embeddings\"\"\"\n\n    data = np.load(embeddings_file)\n    embeddings = data['embeddings']\n\n    # Clustering-based quality assessment\n    kmeans = KMeans(n_clusters=5, random_state=42)\n    cluster_labels = kmeans.fit_predict(embeddings)\n\n    # Silhouette score (higher is better)\n    silhouette = silhouette_score(embeddings, cluster_labels)\n\n    # Embedding statistics\n    mean_norm = np.mean(np.linalg.norm(embeddings, axis=1))\n    std_norm = np.std(np.linalg.norm(embeddings, axis=1))\n\n    metrics = {\n        'silhouette_score': silhouette,\n        'mean_embedding_norm': mean_norm,\n        'std_embedding_norm': std_norm,\n        'num_embeddings': len(embeddings),\n        'embedding_dim': embeddings.shape[1]\n    }\n\n    return metrics\n\n# Assess quality\nquality = assess_embedding_quality('all_embeddings.npz')\nprint(f\"Embedding quality metrics: {quality}\")\n</code></pre>"},{"location":"api/core/extract_embeddings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/extract_embeddings/#common-issues","title":"Common Issues","text":"<p>Model not found: <pre><code># Check model path and ensure it exists\nmodel_path = Path(\"models/simclr_model.pt\")\nif not model_path.exists():\n    print(f\"Model not found at {model_path}\")\n    print(\"Train a SimCLR model first or download a pretrained one\")\n</code></pre></p> <p>Out of memory: <pre><code># Reduce batch size\nbatch_size = 16  # Instead of 64\n\n# Clear GPU cache\ntorch.cuda.empty_cache()\n\n# Use CPU if necessary\ndevice = torch.device('cpu')\n</code></pre></p> <p>Inconsistent image sizes: <pre><code># Ensure all images are properly resized\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n</code></pre></p>"},{"location":"api/core/extract_embeddings/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/core/extract_embeddings/#custom-feature-extractors","title":"Custom Feature Extractors","text":"<pre><code>class CustomFeatureExtractor:\n    def __init__(self, model_path, device='auto'):\n        self.device = self._setup_device(device)\n        self.model = self._load_model(model_path)\n        self.transform = self._get_transform()\n\n    def extract_features(self, image_paths, batch_size=32):\n        \"\"\"Extract features from a list of image paths\"\"\"\n        dataset = PathBasedDataset(image_paths, self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n\n        features = []\n        with torch.no_grad():\n            for batch in dataloader:\n                batch_features = self.model.encoder(batch.to(self.device))\n                features.append(batch_features.cpu().numpy())\n\n        return np.vstack(features)\n\n# Usage\nextractor = CustomFeatureExtractor('models/custom_simclr.pt')\nfeatures = extractor.extract_features(image_paths)\n</code></pre>"},{"location":"api/core/extract_embeddings/#related-documentation","title":"Related Documentation","text":"<ul> <li>SimCLR Training - Train custom feature extractors</li> <li>Clustering Analysis - Use embeddings for clustering</li> <li>Classification - Downstream classification tasks</li> <li>Preprocessing - Prepare images for feature extraction </li> </ul>"},{"location":"api/core/preprocess/","title":"Image Preprocessing API","text":"<p>The preprocessing module provides comprehensive image quality assessment and data cleaning capabilities for mosquito breeding spot images.</p>"},{"location":"api/core/preprocess/#overview","title":"Overview","text":"<p>The <code>ImagePreprocessor</code> class uses the fastdup library to automatically identify and categorize problematic images, including:</p> <ul> <li>Invalid images: Corrupted or unreadable files</li> <li>Duplicates: Nearly identical or repeated images</li> <li>Outliers: Images significantly different from the dataset</li> <li>Dark images: Poorly lit or underexposed images</li> <li>Blurry images: Out-of-focus or motion-blurred images</li> </ul>"},{"location":"api/core/preprocess/#quick-start","title":"Quick Start","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\n\n# Initialize the preprocessor\npreprocessor = ImagePreprocessor(\n    input_dir=\"path/to/raw/images\",\n    output_dir=\"path/to/results\",\n    ccthreshold=0.9,\n    outlier_distance=0.68\n)\n\n# Run the complete preprocessing pipeline\npreprocessor.run_preprocessing()\n</code></pre>"},{"location":"api/core/preprocess/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Basic usage\npython -m prismh.core.preprocess --data_dir images/ --output_dir results/\n\n# With custom parameters\npython -m prismh.core.preprocess \\\n    --data_dir /path/to/images \\\n    --output_dir /path/to/results \\\n    --ccthreshold 0.85 \\\n    --outlier_distance 0.70\n</code></pre>"},{"location":"api/core/preprocess/#api-reference","title":"API Reference","text":""},{"location":"api/core/preprocess/#prismh.core.preprocess.ImagePreprocessor","title":"<code>ImagePreprocessor</code>","text":"<p>A class that uses the fastdup library to preprocess images by identifying invalid files, duplicates, outliers, dark, and blurry images, and segregating them into 'clean' and 'problematic' sets.</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>class ImagePreprocessor:\n    \"\"\"\n    A class that uses the fastdup library to preprocess images by identifying\n    invalid files, duplicates, outliers, dark, and blurry images, and segregating\n    them into 'clean' and 'problematic' sets.\n    \"\"\"\n    def __init__(self, \n                 input_dir: str,\n                 output_dir: str,\n                 ccthreshold: float = 0.9,\n                 outlier_distance: float = 0.68):\n        \"\"\"\n        :param input_dir: Path to the folder that contains all your images.\n        :param output_dir: Path where the cleaned and problematic folders will be created.\n        :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9).\n        :param outlier_distance: Distance threshold for outlier detection (default 0.68).\n        \"\"\"\n        # Convert to absolute paths using pathlib\n        self.input_dir = Path(input_dir).resolve()\n        self.output_dir = Path(output_dir).resolve()\n        self.ccthreshold = ccthreshold\n        self.outlier_distance = outlier_distance\n\n        # Folders for final categorized images using pathlib\n        self.clean_folder = self.output_dir / \"clean\"\n        self.problematic_folder = self.output_dir / \"problematic\"\n        self.invalid_folder = self.problematic_folder / \"invalid\"\n        self.duplicates_folder = self.problematic_folder / \"duplicates\"\n        self.outliers_folder = self.problematic_folder / \"outliers\"\n        self.dark_folder = self.problematic_folder / \"dark\"\n        self.blurry_folder = self.problematic_folder / \"blurry\"\n\n        # Create output directories\n        self._create_directories()\n\n    def _create_directories(self):\n        \"\"\"Create the output directory structure using pathlib.\"\"\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.clean_folder.mkdir(parents=True, exist_ok=True)\n        self.problematic_folder.mkdir(parents=True, exist_ok=True)\n        self.invalid_folder.mkdir(parents=True, exist_ok=True)\n        self.duplicates_folder.mkdir(parents=True, exist_ok=True)\n        self.outliers_folder.mkdir(parents=True, exist_ok=True)\n        self.dark_folder.mkdir(parents=True, exist_ok=True)\n        self.blurry_folder.mkdir(parents=True, exist_ok=True)\n\n    def _extract_filename(self, path):\n        \"\"\"Extract just the filename from a path (relative or absolute) using pathlib\"\"\"\n        # Assuming path is a string from fastdup, convert to Path first\n        return Path(path).name\n\n    def run_preprocessing(self):\n        \"\"\"\n        Run the entire fastdup-based preprocessing pipeline:\n          1) Detect invalid images\n          2) Compute similarity and connected components (duplicates)\n          3) Identify outliers, dark images, and blurry images\n          4) Copy images to respective categories\n        \"\"\"\n        # 1) Create a FastDup object and run the analysis\n        fd = fastdup.create(input_dir=self.input_dir)\n        fd.run(ccthreshold=self.ccthreshold)\n\n        # 2) Identify invalid images\n        broken_images_df = fd.invalid_instances()\n        broken_filenames = [self._extract_filename(path) for path in broken_images_df['filename'].tolist()]\n        print(f\"Found {len(broken_filenames)} invalid images.\")\n\n        # 3) Find duplicates via connected components\n        connected_components_df, _ = fd.connected_components()\n        clusters_df = self._get_clusters(\n            connected_components_df, \n            sort_by='count', \n            min_count=2, \n            ascending=False\n        )\n\n        keep_filenames = []\n        duplicate_filenames = []\n\n        for cluster_file_list in clusters_df.filename:\n            if not cluster_file_list:  # Skip empty lists\n                continue\n\n            # We'll keep the first one and mark the rest as duplicates\n            keep = self._extract_filename(cluster_file_list[0])\n            discard = [self._extract_filename(path) for path in cluster_file_list[1:]]\n\n            keep_filenames.append(keep)\n            duplicate_filenames.extend(discard)\n\n        print(f\"Found {len(set(duplicate_filenames))} duplicates.\")\n\n        # 4) Find outliers (distance &lt; outlier_distance)\n        outlier_df = fd.outliers()\n        outlier_filenames = [\n            self._extract_filename(path) \n            for path in outlier_df[outlier_df.distance &lt; self.outlier_distance].filename_outlier.tolist()\n        ]\n        print(f\"Found {len(outlier_filenames)} outliers with distance &lt; {self.outlier_distance}.\")\n\n        # 5) Dark and blurry images from stats\n        stats_df = fd.img_stats()\n\n        dark_images = stats_df[stats_df['mean'] &lt; 13]    # threshold for darkness\n        dark_filenames = [self._extract_filename(path) for path in dark_images['filename'].tolist()]\n        print(f\"Found {len(dark_filenames)} dark images (mean &lt; 13).\")\n\n        blurry_images = stats_df[stats_df['blur'] &lt; 50]  # threshold for blur\n        blurry_filenames = [self._extract_filename(path) for path in blurry_images['filename'].tolist()]\n        print(f\"Found {len(blurry_filenames)} blurry images (blur &lt; 50).\")\n\n        # 6) Collect all problematic filenames\n        broken_set = set(broken_filenames)\n        duplicates_set = set(duplicate_filenames)\n        outlier_set = set(outlier_filenames)\n        dark_set = set(dark_filenames)\n        blurry_set = set(blurry_filenames)\n        keep_set = set(keep_filenames)\n\n        # 7) Build sets for processing\n        all_problematic = broken_set.union(duplicates_set, outlier_set, dark_set, blurry_set)\n        print(f\"Total problematic images: {len(all_problematic)}\")\n        print(f\"Images to keep from clusters: {len(keep_set)}\")\n\n        # 8) Process all files in the input directory\n        problematic_count = {\n            \"invalid\": 0,\n            \"duplicates\": 0,\n            \"outliers\": 0,\n            \"dark\": 0,\n            \"blurry\": 0\n        }\n\n        clean_count = 0\n        kept_duplicates = 0\n\n        # Get a list of all files using pathlib (assuming flat structure or recursion handled by fastdup already, adjust if needed)\n        # Using rglob to find all files recursively. Filter for actual files.\n        all_paths = [p for p in self.input_dir.rglob('*') if p.is_file()]\n        all_files = [(p, p.name) for p in all_paths]\n\n        print(f\"Found {len(all_files)} total files in input directory: {self.input_dir}\")\n\n        # Process each file\n        for full_path, filename in all_files:\n            # Copy to the problematic folders if needed\n            if filename in broken_set:\n                self._copy_to_folder(full_path, self.invalid_folder)\n                problematic_count[\"invalid\"] += 1\n\n            if filename in duplicates_set:\n                self._copy_to_folder(full_path, self.duplicates_folder)\n                problematic_count[\"duplicates\"] += 1\n\n            if filename in outlier_set:\n                self._copy_to_folder(full_path, self.outliers_folder)\n                problematic_count[\"outliers\"] += 1\n\n            if filename in dark_set:\n                self._copy_to_folder(full_path, self.dark_folder)\n                problematic_count[\"dark\"] += 1\n\n            if filename in blurry_set:\n                self._copy_to_folder(full_path, self.blurry_folder)\n                problematic_count[\"blurry\"] += 1\n\n            # Copy to clean folder if not problematic or if it's a keeper\n            if filename not in all_problematic or filename in keep_set:\n                self._copy_to_folder(full_path, self.clean_folder)\n                clean_count += 1\n                if filename in keep_set:\n                    kept_duplicates += 1\n\n        # Print summary\n        print(\"Copying results:\")\n        print(f\"- Invalid: {problematic_count['invalid']}/{len(broken_set)}\")\n        print(f\"- Duplicates: {problematic_count['duplicates']}/{len(duplicates_set)}\")\n        print(f\"- Outliers: {problematic_count['outliers']}/{len(outlier_set)}\")\n        print(f\"- Dark: {problematic_count['dark']}/{len(dark_set)}\")\n        print(f\"- Blurry: {problematic_count['blurry']}/{len(blurry_set)}\")\n        print(f\"- Clean: {clean_count} (including {kept_duplicates} kept duplicates)\")\n\n    def _copy_to_folder(self, src_path, dest_folder):\n        \"\"\"Copy a file to the destination folder using pathlib\"\"\"\n        # Ensure src_path is a Path object if it comes from all_files list\n        src_path_obj = Path(src_path) \n        filename = src_path_obj.name \n        # Ensure dest_folder is a Path object\n        dest_folder_obj = Path(dest_folder)\n        dest_path = os.path.join(dest_folder, filename)\n        dest_path_obj = dest_folder_obj / filename # Use pathlib join\n        try: \n            shutil.copy2(src_path, dest_path)\n            return True\n        except Exception as e:\n            print(f\"Error copying {src_path} to {dest_folder}: {e}\")\n            return False\n\n    def _get_clusters(self, df, sort_by='count', min_count=2, ascending=False):\n        \"\"\"\n        Given a connected_components DataFrame from fastdup, group into clusters\n        with the specified sorting options.\n        \"\"\"\n        agg_dict = {'filename': list, 'mean_distance': 'max', 'count': 'count'}\n        if 'label' in df.columns:\n            agg_dict['label'] = list\n\n        # only consider rows where 'count' &gt;= min_count\n        df = df[df['count'] &gt;= min_count]\n\n        grouped_df = df.groupby('component_id').agg(agg_dict)\n        grouped_df = grouped_df.sort_values(by=[sort_by], ascending=ascending)\n        return grouped_df\n</code></pre>"},{"location":"api/core/preprocess/#prismh.core.preprocess.ImagePreprocessor.__init__","title":"<code>__init__(input_dir: str, output_dir: str, ccthreshold: float = 0.9, outlier_distance: float = 0.68)</code>","text":"<p>:param input_dir: Path to the folder that contains all your images. :param output_dir: Path where the cleaned and problematic folders will be created. :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9). :param outlier_distance: Distance threshold for outlier detection (default 0.68).</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>def __init__(self, \n             input_dir: str,\n             output_dir: str,\n             ccthreshold: float = 0.9,\n             outlier_distance: float = 0.68):\n    \"\"\"\n    :param input_dir: Path to the folder that contains all your images.\n    :param output_dir: Path where the cleaned and problematic folders will be created.\n    :param ccthreshold: Threshold for similarity detection in fastdup (default 0.9).\n    :param outlier_distance: Distance threshold for outlier detection (default 0.68).\n    \"\"\"\n    # Convert to absolute paths using pathlib\n    self.input_dir = Path(input_dir).resolve()\n    self.output_dir = Path(output_dir).resolve()\n    self.ccthreshold = ccthreshold\n    self.outlier_distance = outlier_distance\n\n    # Folders for final categorized images using pathlib\n    self.clean_folder = self.output_dir / \"clean\"\n    self.problematic_folder = self.output_dir / \"problematic\"\n    self.invalid_folder = self.problematic_folder / \"invalid\"\n    self.duplicates_folder = self.problematic_folder / \"duplicates\"\n    self.outliers_folder = self.problematic_folder / \"outliers\"\n    self.dark_folder = self.problematic_folder / \"dark\"\n    self.blurry_folder = self.problematic_folder / \"blurry\"\n\n    # Create output directories\n    self._create_directories()\n</code></pre>"},{"location":"api/core/preprocess/#prismh.core.preprocess.ImagePreprocessor.run_preprocessing","title":"<code>run_preprocessing()</code>","text":"Run the entire fastdup-based preprocessing pipeline <p>1) Detect invalid images 2) Compute similarity and connected components (duplicates) 3) Identify outliers, dark images, and blurry images 4) Copy images to respective categories</p> Source code in <code>src/prismh/core/preprocess.py</code> <pre><code>def run_preprocessing(self):\n    \"\"\"\n    Run the entire fastdup-based preprocessing pipeline:\n      1) Detect invalid images\n      2) Compute similarity and connected components (duplicates)\n      3) Identify outliers, dark images, and blurry images\n      4) Copy images to respective categories\n    \"\"\"\n    # 1) Create a FastDup object and run the analysis\n    fd = fastdup.create(input_dir=self.input_dir)\n    fd.run(ccthreshold=self.ccthreshold)\n\n    # 2) Identify invalid images\n    broken_images_df = fd.invalid_instances()\n    broken_filenames = [self._extract_filename(path) for path in broken_images_df['filename'].tolist()]\n    print(f\"Found {len(broken_filenames)} invalid images.\")\n\n    # 3) Find duplicates via connected components\n    connected_components_df, _ = fd.connected_components()\n    clusters_df = self._get_clusters(\n        connected_components_df, \n        sort_by='count', \n        min_count=2, \n        ascending=False\n    )\n\n    keep_filenames = []\n    duplicate_filenames = []\n\n    for cluster_file_list in clusters_df.filename:\n        if not cluster_file_list:  # Skip empty lists\n            continue\n\n        # We'll keep the first one and mark the rest as duplicates\n        keep = self._extract_filename(cluster_file_list[0])\n        discard = [self._extract_filename(path) for path in cluster_file_list[1:]]\n\n        keep_filenames.append(keep)\n        duplicate_filenames.extend(discard)\n\n    print(f\"Found {len(set(duplicate_filenames))} duplicates.\")\n\n    # 4) Find outliers (distance &lt; outlier_distance)\n    outlier_df = fd.outliers()\n    outlier_filenames = [\n        self._extract_filename(path) \n        for path in outlier_df[outlier_df.distance &lt; self.outlier_distance].filename_outlier.tolist()\n    ]\n    print(f\"Found {len(outlier_filenames)} outliers with distance &lt; {self.outlier_distance}.\")\n\n    # 5) Dark and blurry images from stats\n    stats_df = fd.img_stats()\n\n    dark_images = stats_df[stats_df['mean'] &lt; 13]    # threshold for darkness\n    dark_filenames = [self._extract_filename(path) for path in dark_images['filename'].tolist()]\n    print(f\"Found {len(dark_filenames)} dark images (mean &lt; 13).\")\n\n    blurry_images = stats_df[stats_df['blur'] &lt; 50]  # threshold for blur\n    blurry_filenames = [self._extract_filename(path) for path in blurry_images['filename'].tolist()]\n    print(f\"Found {len(blurry_filenames)} blurry images (blur &lt; 50).\")\n\n    # 6) Collect all problematic filenames\n    broken_set = set(broken_filenames)\n    duplicates_set = set(duplicate_filenames)\n    outlier_set = set(outlier_filenames)\n    dark_set = set(dark_filenames)\n    blurry_set = set(blurry_filenames)\n    keep_set = set(keep_filenames)\n\n    # 7) Build sets for processing\n    all_problematic = broken_set.union(duplicates_set, outlier_set, dark_set, blurry_set)\n    print(f\"Total problematic images: {len(all_problematic)}\")\n    print(f\"Images to keep from clusters: {len(keep_set)}\")\n\n    # 8) Process all files in the input directory\n    problematic_count = {\n        \"invalid\": 0,\n        \"duplicates\": 0,\n        \"outliers\": 0,\n        \"dark\": 0,\n        \"blurry\": 0\n    }\n\n    clean_count = 0\n    kept_duplicates = 0\n\n    # Get a list of all files using pathlib (assuming flat structure or recursion handled by fastdup already, adjust if needed)\n    # Using rglob to find all files recursively. Filter for actual files.\n    all_paths = [p for p in self.input_dir.rglob('*') if p.is_file()]\n    all_files = [(p, p.name) for p in all_paths]\n\n    print(f\"Found {len(all_files)} total files in input directory: {self.input_dir}\")\n\n    # Process each file\n    for full_path, filename in all_files:\n        # Copy to the problematic folders if needed\n        if filename in broken_set:\n            self._copy_to_folder(full_path, self.invalid_folder)\n            problematic_count[\"invalid\"] += 1\n\n        if filename in duplicates_set:\n            self._copy_to_folder(full_path, self.duplicates_folder)\n            problematic_count[\"duplicates\"] += 1\n\n        if filename in outlier_set:\n            self._copy_to_folder(full_path, self.outliers_folder)\n            problematic_count[\"outliers\"] += 1\n\n        if filename in dark_set:\n            self._copy_to_folder(full_path, self.dark_folder)\n            problematic_count[\"dark\"] += 1\n\n        if filename in blurry_set:\n            self._copy_to_folder(full_path, self.blurry_folder)\n            problematic_count[\"blurry\"] += 1\n\n        # Copy to clean folder if not problematic or if it's a keeper\n        if filename not in all_problematic or filename in keep_set:\n            self._copy_to_folder(full_path, self.clean_folder)\n            clean_count += 1\n            if filename in keep_set:\n                kept_duplicates += 1\n\n    # Print summary\n    print(\"Copying results:\")\n    print(f\"- Invalid: {problematic_count['invalid']}/{len(broken_set)}\")\n    print(f\"- Duplicates: {problematic_count['duplicates']}/{len(duplicates_set)}\")\n    print(f\"- Outliers: {problematic_count['outliers']}/{len(outlier_set)}\")\n    print(f\"- Dark: {problematic_count['dark']}/{len(dark_set)}\")\n    print(f\"- Blurry: {problematic_count['blurry']}/{len(blurry_set)}\")\n    print(f\"- Clean: {clean_count} (including {kept_duplicates} kept duplicates)\")\n</code></pre>"},{"location":"api/core/preprocess/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"api/core/preprocess/#quality-thresholds","title":"Quality Thresholds","text":"Parameter Description Default Range <code>ccthreshold</code> Similarity threshold for duplicate detection 0.9 0.0-1.0 <code>outlier_distance</code> Distance threshold for outlier detection 0.68 0.0-1.0"},{"location":"api/core/preprocess/#image-quality-metrics","title":"Image Quality Metrics","text":"Metric Threshold Purpose Mean brightness &lt; 13 Detect dark images Blur variance &lt; 50 Detect blurry images File validity - Detect corrupted files"},{"location":"api/core/preprocess/#output-structure","title":"Output Structure","text":"<p>The preprocessing pipeline creates the following directory structure:</p> <pre><code>output_dir/\n\u251c\u2500\u2500 clean/                    # High-quality images\n\u2514\u2500\u2500 problematic/             # Filtered images\n    \u251c\u2500\u2500 invalid/             # Corrupted files\n    \u251c\u2500\u2500 duplicates/          # Duplicate images\n    \u251c\u2500\u2500 outliers/            # Unusual images\n    \u251c\u2500\u2500 dark/                # Dark/underexposed images\n    \u2514\u2500\u2500 blurry/              # Blurry images\n</code></pre>"},{"location":"api/core/preprocess/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/preprocess/#basic-preprocessing","title":"Basic Preprocessing","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\n\n# Simple preprocessing\npreprocessor = ImagePreprocessor(\n    input_dir=\"raw_images/\",\n    output_dir=\"processed/\"\n)\npreprocessor.run_preprocessing()\n</code></pre>"},{"location":"api/core/preprocess/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Advanced configuration\npreprocessor = ImagePreprocessor(\n    input_dir=\"raw_images/\",\n    output_dir=\"processed/\",\n    ccthreshold=0.85,        # More strict duplicate detection\n    outlier_distance=0.75    # More lenient outlier detection\n)\npreprocessor.run_preprocessing()\n</code></pre>"},{"location":"api/core/preprocess/#with-metadata-integration","title":"With Metadata Integration","text":"<pre><code>import json\nfrom pathlib import Path\n\n# Load metadata\nmetadata_path = Path(\"metadata/annotations.json\")\nwith open(metadata_path) as f:\n    metadata = json.load(f)\n\n# Process with metadata context\npreprocessor = ImagePreprocessor(\n    input_dir=\"raw_images/\",\n    output_dir=\"processed/\"\n)\npreprocessor.run_preprocessing()\n\n# Analyze results with metadata\nresults_summary = {\n    \"total_processed\": len(list(Path(\"processed/clean\").glob(\"*\"))),\n    \"problematic_count\": len(list(Path(\"processed/problematic\").rglob(\"*\"))),\n    \"metadata_entries\": len(metadata)\n}\n</code></pre>"},{"location":"api/core/preprocess/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/core/preprocess/#memory-management","title":"Memory Management","text":"<pre><code># For large datasets, process in batches\nimport os\nos.environ['FASTDUP_BATCH_SIZE'] = '1000'\n\npreprocessor = ImagePreprocessor(\n    input_dir=\"large_dataset/\",\n    output_dir=\"results/\"\n)\n</code></pre>"},{"location":"api/core/preprocess/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Utilize multiple CPU cores\nimport multiprocessing\nos.environ['FASTDUP_NUM_WORKERS'] = str(multiprocessing.cpu_count())\n</code></pre>"},{"location":"api/core/preprocess/#quality-metrics","title":"Quality Metrics","text":"<p>The preprocessing pipeline provides detailed quality metrics:</p> <pre><code># Access quality statistics\nstats = preprocessor.get_quality_stats()\nprint(f\"Quality score: {stats['quality_score']:.2f}\")\nprint(f\"Duplicate rate: {stats['duplicate_rate']:.1%}\")\nprint(f\"Outlier rate: {stats['outlier_rate']:.1%}\")\n</code></pre>"},{"location":"api/core/preprocess/#integration-with-other-modules","title":"Integration with Other Modules","text":""},{"location":"api/core/preprocess/#with-feature-extraction","title":"With Feature Extraction","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\nfrom prismh.core.extract_embeddings import extract_embeddings_main\n\n# Step 1: Preprocess\npreprocessor = ImagePreprocessor(\"raw/\", \"processed/\")\npreprocessor.run_preprocessing()\n\n# Step 2: Extract features from clean images\n# (Configure paths in extract_embeddings.py)\nextract_embeddings_main()\n</code></pre>"},{"location":"api/core/preprocess/#with-clustering","title":"With Clustering","text":"<pre><code>from prismh.core.cluster_embeddings import cluster_main\n\n# After preprocessing and feature extraction\ncluster_main()\n</code></pre>"},{"location":"api/core/preprocess/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/preprocess/#common-issues","title":"Common Issues","text":"<p>Memory errors with large datasets: <pre><code># Reduce batch size or process in chunks\nos.environ['FASTDUP_BATCH_SIZE'] = '500'\n</code></pre></p> <p>Path-related errors: <pre><code># Use absolute paths\nfrom pathlib import Path\ninput_path = Path(\"images\").resolve()\noutput_path = Path(\"results\").resolve()\n</code></pre></p> <p>Permission errors: <pre><code># Ensure write permissions\nchmod -R 755 output_directory/\n</code></pre></p>"},{"location":"api/core/preprocess/#advanced-features","title":"Advanced Features","text":""},{"location":"api/core/preprocess/#custom-quality-filters","title":"Custom Quality Filters","text":"<pre><code>class CustomImagePreprocessor(ImagePreprocessor):\n    def custom_quality_check(self, image_path):\n        \"\"\"Add custom quality assessment logic\"\"\"\n        # Implement custom quality checks\n        pass\n\n    def run_preprocessing(self):\n        # Run standard preprocessing\n        super().run_preprocessing()\n        # Add custom processing steps\n        self.apply_custom_filters()\n</code></pre>"},{"location":"api/core/preprocess/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_multiple_datasets(datasets):\n    \"\"\"Process multiple image datasets\"\"\"\n    for dataset_info in datasets:\n        preprocessor = ImagePreprocessor(\n            input_dir=dataset_info['input'],\n            output_dir=dataset_info['output']\n        )\n        preprocessor.run_preprocessing()\n        print(f\"Completed: {dataset_info['name']}\")\n</code></pre>"},{"location":"api/core/preprocess/#related-documentation","title":"Related Documentation","text":"<ul> <li>Feature Extraction - Next step in the pipeline</li> <li>Clustering Analysis - Pattern discovery</li> <li>Configuration Guide - Detailed parameter tuning</li> <li>Examples - Practical usage scenarios </li> </ul>"},{"location":"api/models/simclr/","title":"SimCLR Self-Supervised Learning API","text":"<p>The SimCLR module implements self-supervised contrastive learning for extracting meaningful visual representations from mosquito breeding spot images without requiring extensive labeled data.</p>"},{"location":"api/models/simclr/#overview","title":"Overview","text":"<p>SimCLR (Simple Contrastive Learning of Visual Representations) learns visual features by maximizing agreement between differently augmented views of the same image. This approach is particularly valuable for our use case where labeled data is limited or unreliable.</p>"},{"location":"api/models/simclr/#key-components","title":"Key Components","text":"<ul> <li>SimCLRModel: Main model architecture with ResNet backbone</li> <li>SimCLRDataset: Dataset class for contrastive learning</li> <li>NTXentLoss: Normalized Temperature-scaled Cross Entropy loss</li> <li>Training Pipeline: Complete training workflow with checkpointing</li> </ul>"},{"location":"api/models/simclr/#quick-start","title":"Quick Start","text":"<pre><code>from prismh.models.simclr import SimCLRModel, train_simclr\nimport torch\n\n# Initialize model\nmodel = SimCLRModel(base_model='resnet50', pretrained=True)\n\n# Train on your data\ntrain_simclr(\n    model=model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    epochs=100\n)\n</code></pre>"},{"location":"api/models/simclr/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Train SimCLR model\npython -m prismh.models.simclr \\\n    --data_dir /path/to/clean/images \\\n    --output_dir simclr_results \\\n    --epochs 100 \\\n    --batch_size 32\n\n# Resume training from checkpoint\npython -m prismh.models.simclr \\\n    --data_dir /path/to/clean/images \\\n    --output_dir simclr_results \\\n    --resume_from simclr_results/checkpoints/checkpoint_epoch_50.pt\n</code></pre>"},{"location":"api/models/simclr/#api-reference","title":"API Reference","text":""},{"location":"api/models/simclr/#prismh.models.simclr.SimCLRModel","title":"<code>SimCLRModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>SimCLR model with encoder and projection head</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRModel(nn.Module):\n    \"\"\"SimCLR model with encoder and projection head\"\"\"\n\n    def __init__(self, base_model='resnet50', pretrained=True, output_dim=128):\n        super(SimCLRModel, self).__init__()\n\n        # Load the base encoder model (e.g., ResNet-50)\n        if base_model == 'resnet50':\n            self.encoder = models.resnet50(pretrained=pretrained)\n            self.encoder_dim = 2048\n        elif base_model == 'resnet18':\n            self.encoder = models.resnet18(pretrained=pretrained)\n            self.encoder_dim = 512\n        else:\n            raise ValueError(f\"Unsupported base model: {base_model}\")\n\n        # Replace the final fully connected layer\n        self.encoder.fc = nn.Identity()\n\n        # Add projection head\n        self.projection_head = SimCLRProjectionHead(\n            input_dim=self.encoder_dim,\n            output_dim=output_dim\n        )\n\n    def forward(self, x):\n        features = self.encoder(x)\n        projections = self.projection_head(features)\n        return features, F.normalize(projections, dim=1)\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.SimCLRDataset","title":"<code>SimCLRDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimCLR that returns two augmented views of each image</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRDataset(Dataset):\n    \"\"\"Dataset for SimCLR that returns two augmented views of each image\"\"\"\n\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (string or Path): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.image_paths = []\n\n        # Collect all image paths\n        for entry in self.root_dir.iterdir():\n            if entry.is_file() and entry.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n                self.image_paths.append(entry)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            # Apply the same transform twice to get two different augmented views\n            view1 = self.transform(image)\n            view2 = self.transform(image)\n            return view1, view2\n\n        return image, image\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.SimCLRDataset.__init__","title":"<code>__init__(root_dir, transform=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>string or Path</code> <p>Directory with all the images.</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied on a sample.</p> <code>None</code> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def __init__(self, root_dir, transform=None):\n    \"\"\"\n    Args:\n        root_dir (string or Path): Directory with all the images.\n        transform (callable, optional): Optional transform to be applied on a sample.\n    \"\"\"\n    self.root_dir = Path(root_dir)\n    self.transform = transform\n    self.image_paths = []\n\n    # Collect all image paths\n    for entry in self.root_dir.iterdir():\n        if entry.is_file() and entry.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n            self.image_paths.append(entry)\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.SimCLRProjectionHead","title":"<code>SimCLRProjectionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Projection head for SimCLR</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class SimCLRProjectionHead(nn.Module):\n    \"\"\"Projection head for SimCLR\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=2048, output_dim=128):\n        super(SimCLRProjectionHead, self).__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.projection(x)\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.NTXentLoss","title":"<code>NTXentLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Normalized Temperature-scaled Cross Entropy Loss from SimCLR paper</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>class NTXentLoss(nn.Module):\n    \"\"\"\n    Normalized Temperature-scaled Cross Entropy Loss from SimCLR paper\n    \"\"\"\n    def __init__(self, temperature=0.5, batch_size=32):\n        super(NTXentLoss, self).__init__()\n        self.temperature = temperature\n        self.batch_size = batch_size\n        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n        self.similarity_f = nn.CosineSimilarity(dim=2)\n        # Mask to remove positive examples from the denominator of the loss function\n        mask = torch.ones((2 * batch_size, 2 * batch_size), dtype=bool)\n        mask.fill_diagonal_(0)\n\n        for i in range(batch_size):\n            mask[i, batch_size + i] = 0\n            mask[batch_size + i, i] = 0\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        Calculate NT-Xent loss\n        Args:\n            z_i, z_j: Normalized projection vectors from the two augmented views\n        \"\"\"\n        # Calculate cosine similarity\n        representations = torch.cat([z_i, z_j], dim=0)\n        similarity_matrix = self.similarity_f(representations.unsqueeze(1), representations.unsqueeze(0)) / self.temperature\n\n        # Mask out the positives\n        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n        positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n        # Mask out the diagnonal (self-similarity)\n        negatives = similarity_matrix[self.mask].reshape(2 * self.batch_size, -1)\n\n        # Create labels - positives are the \"correct\" predictions\n        labels = torch.zeros(2 * self.batch_size).long().to(positives.device)\n\n        # Calculate loss\n        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n        loss = self.criterion(logits, labels)\n        loss = loss / (2 * self.batch_size)\n\n        return loss\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.NTXentLoss.forward","title":"<code>forward(z_i, z_j)</code>","text":"<p>Calculate NT-Xent loss Args:     z_i, z_j: Normalized projection vectors from the two augmented views</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def forward(self, z_i, z_j):\n    \"\"\"\n    Calculate NT-Xent loss\n    Args:\n        z_i, z_j: Normalized projection vectors from the two augmented views\n    \"\"\"\n    # Calculate cosine similarity\n    representations = torch.cat([z_i, z_j], dim=0)\n    similarity_matrix = self.similarity_f(representations.unsqueeze(1), representations.unsqueeze(0)) / self.temperature\n\n    # Mask out the positives\n    sim_ij = torch.diag(similarity_matrix, self.batch_size)\n    sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n    positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n    # Mask out the diagnonal (self-similarity)\n    negatives = similarity_matrix[self.mask].reshape(2 * self.batch_size, -1)\n\n    # Create labels - positives are the \"correct\" predictions\n    labels = torch.zeros(2 * self.batch_size).long().to(positives.device)\n\n    # Calculate loss\n    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n    loss = self.criterion(logits, labels)\n    loss = loss / (2 * self.batch_size)\n\n    return loss\n</code></pre>"},{"location":"api/models/simclr/#prismh.models.simclr.train_simclr","title":"<code>train_simclr(model, train_loader, optimizer, criterion, device, epochs=100, checkpoint_dir='checkpoints', resume_from=None, early_stopping_patience=10, validation_loader=None, writer=None)</code>","text":"<p>Train the SimCLR model with proper analytics and early stopping</p> Source code in <code>src/prismh/models/simclr.py</code> <pre><code>def train_simclr(model, train_loader, optimizer, criterion, device, epochs=100, checkpoint_dir=\"checkpoints\", \n                 resume_from=None, early_stopping_patience=10, validation_loader=None, writer=None):\n    \"\"\"Train the SimCLR model with proper analytics and early stopping\"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize trackers\n    best_loss = float('inf')\n    best_model_path = checkpoint_dir / 'best_model.pt'\n    patience_counter = 0\n    start_epoch = 0\n    train_losses = []\n    val_losses = []\n    lr_history = []\n    global_step = 0\n\n    # Resume from checkpoint if available\n    if resume_from:\n        resume_path = Path(resume_from)\n        if resume_path.exists():\n            print(f\"Resuming training from checkpoint: {resume_path}\")\n            checkpoint = torch.load(resume_path, map_location=device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            start_epoch = checkpoint['epoch'] + 1\n            if 'best_loss' in checkpoint:\n                best_loss = checkpoint['best_loss']\n            if 'train_losses' in checkpoint:\n                train_losses = checkpoint['train_losses']\n            if 'val_losses' in checkpoint:\n                val_losses = checkpoint['val_losses']\n            if 'lr_history' in checkpoint:\n                lr_history = checkpoint['lr_history']\n            if 'global_step' in checkpoint:\n                global_step = checkpoint['global_step']\n            print(f\"Resuming from epoch {start_epoch}, best loss: {best_loss:.4f}, global step: {global_step}\")\n\n    # Create a learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n\n    # Log model graph to TensorBoard if available\n    if writer is not None:\n        # Get a batch of data for graph visualization\n        sample_images, _ = next(iter(train_loader))\n        sample_images = sample_images.to(device)\n        writer.add_graph(model, sample_images)\n\n        # Log some example augmented pairs\n        fig = plt.figure(figsize=(12, 6))\n        for i in range(min(4, len(sample_images))):\n            ax1 = fig.add_subplot(2, 4, i+1)\n            img = sample_images[i].cpu().permute(1, 2, 0).numpy()\n            # Denormalize image\n            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img = np.clip(img, 0, 1)\n            ax1.imshow(img)\n            ax1.set_title(f\"View 1 - img {i}\")\n            ax1.axis('off')\n\n            sample2, _ = next(iter(train_loader))\n            ax2 = fig.add_subplot(2, 4, i+5)\n            img2 = sample2[i].cpu().permute(1, 2, 0).numpy()\n            img2 = img2 * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img2 = np.clip(img2, 0, 1)\n            ax2.imshow(img2)\n            ax2.set_title(f\"View 2 - img {i}\")\n            ax2.axis('off')\n\n        plt.tight_layout()\n        writer.add_figure('Example Augmented Pairs', fig, global_step=0)\n\n    # Function to evaluate on validation set\n    def evaluate():\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for images1, images2 in validation_loader:\n                images1, images2 = images1.to(device), images2.to(device)\n                _, z1 = model(images1)\n                _, z2 = model(images2)\n                loss = criterion(z1, z2)\n                total_val_loss += loss.item()\n        return total_val_loss / len(validation_loader)\n\n    # Function to log embeddings\n    def log_embeddings(step):\n        if writer is None:\n            return\n\n        # Extract embeddings for visualization\n        model.eval()\n        embeddings = []\n        imgs = []\n        with torch.no_grad():\n            for i, (images1, _) in enumerate(validation_loader):\n                if i &gt;= 2:  # Limit to a few batches for visualization\n                    break\n                images1 = images1.to(device)\n                features, _ = model(images1)\n                embeddings.append(features.cpu().numpy())\n                imgs.append(images1.cpu())\n\n        if not embeddings:\n            return\n\n        embeddings = np.vstack(embeddings)\n        imgs = torch.cat(imgs, dim=0)\n\n        # Use t-SNE for dimensionality reduction\n        if len(embeddings) &gt; 10:  # Need enough samples for meaningful t-SNE\n            tsne = TSNE(n_components=2, random_state=42)\n            embeddings_2d = tsne.fit_transform(embeddings)\n\n            # Plot t-SNE visualization\n            fig = plt.figure(figsize=(10, 10))\n            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n            plt.title('t-SNE of Feature Embeddings')\n            writer.add_figure('Embeddings/t-SNE', fig, global_step=step)\n\n        # Log embeddings with images\n        writer.add_embedding(\n            mat=torch.from_numpy(embeddings),\n            label_img=imgs,\n            global_step=step,\n            tag='features' # Explicit tag for embeddings\n        )\n\n    print(f\"Starting training from epoch {start_epoch+1}/{epochs}\")\n    for epoch in range(start_epoch, epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for i, (images1, images2) in enumerate(progress_bar):\n            # Move images to device\n            images1 = images1.to(device)\n            images2 = images2.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass for both augmented views\n            _, z1 = model(images1)\n            _, z2 = model(images2)\n\n            # Calculate loss\n            loss = criterion(z1, z2)\n\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Update statistics\n            running_loss += loss.item()\n            progress_bar.set_postfix({'loss': loss.item()})\n\n            # Log to TensorBoard (every 10 batches)\n            if writer is not None and i % 10 == 0:\n                writer.add_scalar('Batch/train_loss', loss.item(), global_step)\n                global_step += 1\n\n        # Record average loss for the epoch\n        epoch_loss = running_loss / len(train_loader)\n        train_losses.append(epoch_loss)\n\n        # Track current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        lr_history.append(current_lr)\n\n        # Validation phase (if validation loader provided)\n        val_loss = None\n        if validation_loader:\n            val_loss = evaluate()\n            val_losses.append(val_loss)\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n\n            # Log metrics to TensorBoard\n            if writer is not None:\n                writer.add_scalar('Epoch/train_loss', epoch_loss, epoch)\n                writer.add_scalar('Epoch/val_loss', val_loss, epoch)\n                writer.add_scalar('Epoch/learning_rate', current_lr, epoch)\n\n                # Log embeddings periodically\n                if epoch % 5 == 0 or epoch == epochs - 1:\n                    log_embeddings(epoch)\n\n            # Update scheduler based on validation loss\n            scheduler.step(val_loss)\n\n            # Early stopping check\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                # Save best model\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': val_loss,\n                    'best_loss': best_loss,\n                    'train_losses': train_losses,\n                    'val_losses': val_losses,\n                    'lr_history': lr_history,\n                    'global_step': global_step,\n                }, best_model_path)\n                patience_counter = 0\n                print(f\"New best model saved with validation loss: {best_loss:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"Validation loss did not improve. Patience: {patience_counter}/{early_stopping_patience}\")\n\n                if patience_counter &gt;= early_stopping_patience:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    # Load the best model before returning\n                    checkpoint = torch.load(best_model_path, map_location=device)\n                    model.load_state_dict(checkpoint['model_state_dict'])\n                    break\n        else:\n            # If no validation set, use training loss\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}\")\n\n            # Log to TensorBoard\n            if writer is not None:\n                writer.add_scalar('Epoch/train_loss', epoch_loss, epoch)\n                writer.add_scalar('Epoch/learning_rate', current_lr, epoch)\n\n                # Log embeddings periodically\n                if epoch % 5 == 0 or epoch == epochs - 1:\n                    log_embeddings(epoch)\n\n            # Save if better than best so far\n            if epoch_loss &lt; best_loss:\n                best_loss = epoch_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': epoch_loss,\n                    'best_loss': best_loss,\n                    'train_losses': train_losses,\n                    'val_losses': val_losses,\n                    'lr_history': lr_history,\n                    'global_step': global_step,\n                }, best_model_path)\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter &gt;= early_stopping_patience:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    # Load the best model before returning\n                    checkpoint = torch.load(best_model_path, map_location=device)\n                    model.load_state_dict(checkpoint['model_state_dict'])\n                    break\n\n            # Update scheduler based on training loss\n            scheduler.step(epoch_loss)\n\n        # Regular checkpoint (every 5 epochs)\n        if (epoch + 1) % 5 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': val_loss if validation_loader else epoch_loss,\n                'best_loss': best_loss,\n                'train_losses': train_losses,\n                'val_losses': val_losses,\n                'lr_history': lr_history,\n                'global_step': global_step,\n            }, checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pt')\n\n    # Plot training curves\n    plt.figure(figsize=(15, 5))\n\n    # Plot loss curves\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    if val_losses:\n        plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('SimCLR Training Loss')\n    plt.legend()\n\n    # Plot learning rate\n    plt.subplot(1, 2, 2)\n    plt.plot(lr_history)\n    plt.xlabel('Epoch')\n    plt.ylabel('Learning Rate')\n    plt.title('Learning Rate Schedule')\n    plt.yscale('log')\n\n    plt.tight_layout()\n    plt.savefig(checkpoint_dir / 'training_curves.png')\n\n    # Save training history as CSV for further analysis\n    history = pd.DataFrame({\n        'epoch': list(range(1, len(train_losses) + 1)),\n        'train_loss': train_losses,\n        'val_loss': val_losses if val_losses else [None] * len(train_losses),\n        'learning_rate': lr_history\n    })\n    history.to_csv(checkpoint_dir / 'training_history.csv', index=False)\n\n    print(f\"Training completed. Best loss: {best_loss:.4f}\")\n    print(f\"Training analytics saved to {checkpoint_dir}/\")\n\n    return model, {'train_losses': train_losses, 'val_losses': val_losses, 'lr_history': lr_history}\n</code></pre>"},{"location":"api/models/simclr/#architecture-details","title":"Architecture Details","text":""},{"location":"api/models/simclr/#model-architecture","title":"Model Architecture","text":"<pre><code>graph TD\n    A[Input Images] --&gt; B[Data Augmentation]\n    B --&gt; C[Two Augmented Views]\n    C --&gt; D[ResNet Encoder]\n    D --&gt; E[Feature Representations]\n    E --&gt; F[Projection Head]\n    F --&gt; G[Normalized Projections]\n    G --&gt; H[Contrastive Loss]\n</code></pre>"},{"location":"api/models/simclr/#backbone-options","title":"Backbone Options","text":"Backbone Output Dim Parameters Use Case ResNet-18 512 11.7M Fast experimentation ResNet-50 2048 25.6M Recommended for production"},{"location":"api/models/simclr/#data-augmentation-pipeline","title":"Data Augmentation Pipeline","text":"<pre><code>transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n</code></pre>"},{"location":"api/models/simclr/#training-configuration","title":"Training Configuration","text":""},{"location":"api/models/simclr/#hyperparameters","title":"Hyperparameters","text":"Parameter Default Description <code>batch_size</code> 32 Batch size for training <code>learning_rate</code> 0.001 Initial learning rate <code>temperature</code> 0.5 Temperature for contrastive loss <code>epochs</code> 100 Number of training epochs <code>weight_decay</code> 1e-4 L2 regularization"},{"location":"api/models/simclr/#training-features","title":"Training Features","text":"<ul> <li>Automatic Mixed Precision: Faster training with reduced memory</li> <li>Early Stopping: Prevents overfitting</li> <li>Learning Rate Scheduling: Adaptive learning rate adjustment</li> <li>Checkpointing: Resume training from interruptions</li> <li>TensorBoard Logging: Training metrics visualization</li> </ul>"},{"location":"api/models/simclr/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/simclr/#basic-training","title":"Basic Training","text":"<pre><code>from prismh.models.simclr import SimCLRModel, SimCLRDataset, train_simclr\nfrom torch.utils.data import DataLoader\nimport torch\n\n# Setup device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create dataset and dataloader\ndataset = SimCLRDataset(\n    root_dir='data/clean_images',\n    transform=get_simclr_transforms()\n)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Initialize model\nmodel = SimCLRModel(base_model='resnet50').to(device)\n\n# Setup training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = NTXentLoss(temperature=0.5, batch_size=32)\n\n# Train model\ntrain_simclr(\n    model=model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    epochs=100\n)\n</code></pre>"},{"location":"api/models/simclr/#advanced-training-with-validation","title":"Advanced Training with Validation","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n\n# Setup validation\nval_dataset = SimCLRDataset('data/val_images', transform=get_simclr_transforms())\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Setup logging\nwriter = SummaryWriter('runs/simclr_experiment')\n\n# Train with validation\ntrain_simclr(\n    model=model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    epochs=100,\n    validation_loader=val_loader,\n    writer=writer,\n    early_stopping_patience=10\n)\n</code></pre>"},{"location":"api/models/simclr/#fine-tuning-pretrained-model","title":"Fine-tuning Pretrained Model","text":"<pre><code># Load pretrained SimCLR model\nmodel = SimCLRModel(base_model='resnet50', pretrained=True)\n\n# Load domain-specific checkpoint\ncheckpoint = torch.load('pretrained_simclr.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Fine-tune on your data\ntrain_simclr(\n    model=model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    epochs=50,  # Fewer epochs for fine-tuning\n    checkpoint_dir='finetuned_checkpoints'\n)\n</code></pre>"},{"location":"api/models/simclr/#feature-extraction","title":"Feature Extraction","text":"<p>After training, use the encoder for downstream tasks:</p> <pre><code># Load trained model\nmodel = SimCLRModel(base_model='resnet50')\ncheckpoint = torch.load('best_model.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Extract features\nmodel.eval()\nwith torch.no_grad():\n    features, _ = model(images)\n    # Use features for classification, clustering, etc.\n</code></pre>"},{"location":"api/models/simclr/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/models/simclr/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>if torch.cuda.device_count() &gt; 1:\n    model = torch.nn.DataParallel(model)\n</code></pre>"},{"location":"api/models/simclr/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Gradient accumulation for larger effective batch size\naccumulation_steps = 4\nfor i, (images1, images2) in enumerate(train_loader):\n    # Forward pass\n    loss = compute_loss(images1, images2) / accumulation_steps\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"api/models/simclr/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor images1, images2 in train_loader:\n    with autocast():\n        loss = compute_loss(images1, images2)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"api/models/simclr/#monitoring-and-visualization","title":"Monitoring and Visualization","text":""},{"location":"api/models/simclr/#tensorboard-integration","title":"TensorBoard Integration","text":"<pre><code># Log training metrics\nwriter.add_scalar('Loss/Train', loss.item(), epoch)\nwriter.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n\n# Log embeddings\nwriter.add_embedding(features, metadata=labels, tag='SimCLR_Features')\n</code></pre>"},{"location":"api/models/simclr/#t-sne-visualization","title":"t-SNE Visualization","text":"<pre><code>from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Extract features\nfeatures = extract_features(model, dataloader)\n\n# Reduce dimensionality\ntsne = TSNE(n_components=2, random_state=42)\nfeatures_2d = tsne.fit_transform(features)\n\n# Plot\nplt.scatter(features_2d[:, 0], features_2d[:, 1])\nplt.title('SimCLR Features t-SNE')\nplt.show()\n</code></pre>"},{"location":"api/models/simclr/#model-evaluation","title":"Model Evaluation","text":""},{"location":"api/models/simclr/#downstream-task-performance","title":"Downstream Task Performance","text":"<pre><code># Evaluate on classification task\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Extract features\nfeatures = extract_features(model, dataloader)\n\n# Train classifier\nclassifier = LogisticRegression()\nclassifier.fit(features, labels)\n\n# Evaluate\npredictions = classifier.predict(test_features)\naccuracy = accuracy_score(test_labels, predictions)\nprint(f'Classification accuracy: {accuracy:.3f}')\n</code></pre>"},{"location":"api/models/simclr/#clustering-quality","title":"Clustering Quality","text":"<pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Cluster features\nkmeans = KMeans(n_clusters=5)\ncluster_labels = kmeans.fit_predict(features)\n\n# Evaluate clustering\nsilhouette = silhouette_score(features, cluster_labels)\nprint(f'Silhouette score: {silhouette:.3f}')\n</code></pre>"},{"location":"api/models/simclr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/simclr/#common-issues","title":"Common Issues","text":"<p>Out of memory errors: <pre><code># Reduce batch size\nbatch_size = 16  # Instead of 32\n\n# Use gradient accumulation\naccumulation_steps = 2\n</code></pre></p> <p>Slow convergence: <pre><code># Increase learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n\n# Adjust temperature\ncriterion = NTXentLoss(temperature=0.3)\n</code></pre></p> <p>Poor feature quality: <pre><code># Increase augmentation strength\ntransforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8)\n\n# Train for more epochs\nepochs = 200\n</code></pre></p>"},{"location":"api/models/simclr/#integration-with-pipeline","title":"Integration with Pipeline","text":""},{"location":"api/models/simclr/#with-preprocessing","title":"With Preprocessing","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\n\n# Preprocess first\npreprocessor = ImagePreprocessor(input_dir='raw/', output_dir='processed/')\npreprocessor.run_preprocessing()\n\n# Then train SimCLR on clean images\ndataset = SimCLRDataset('processed/clean')\n# ... training code\n</code></pre>"},{"location":"api/models/simclr/#with-classification","title":"With Classification","text":"<pre><code># After SimCLR training, use for classification\nfrom prismh.models.classify import train_classifier\n\n# Use SimCLR encoder as backbone\nencoder = model.encoder\ntrain_classifier(encoder, labeled_data)\n</code></pre>"},{"location":"api/models/simclr/#related-documentation","title":"Related Documentation","text":"<ul> <li>Feature Extraction - Extract embeddings using trained SimCLR</li> <li>Classification - Downstream classification tasks</li> <li>Clustering - Unsupervised analysis</li> <li>Configuration - Training parameter tuning </li> </ul>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":"<p>This page provides practical examples of using Prism-H for mosquito breeding spot detection and analysis.</p>"},{"location":"examples/basic_usage/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete example that demonstrates the entire pipeline:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Complete Prism-H workflow example\"\"\"\n\nfrom pathlib import Path\nfrom prismh.core.preprocess import ImagePreprocessor\n\ndef main():\n    # Configuration\n    config = {\n        'data_dir': '/Users/kirubeso.r/Documents/ArtPark/all_them_images',\n        'metadata_file': '/Users/kirubeso.r/Documents/ArtPark/all_jsons/first100k.json',\n        'output_dir': 'results_complete_workflow',\n        'sample_size': 5000\n    }\n\n    print(\"\ud83d\udd0d Starting Prism-H Complete Workflow\")\n\n    # Step 1: Data Preprocessing\n    print(\"\\n\ud83d\udccb Step 1: Data Preprocessing\")\n    preprocessor = ImagePreprocessor(\n        input_dir=config['data_dir'],\n        output_dir=config['output_dir'],\n        ccthreshold=0.9,\n        outlier_distance=0.68\n    )\n    preprocessor.run_preprocessing()\n\n    print(\"\u2705 Workflow completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/basic_usage/#command-line-examples","title":"Command Line Examples","text":""},{"location":"examples/basic_usage/#basic-usage","title":"Basic Usage","text":"<pre><code># Preprocess images\npython -m prismh.core.preprocess \\\n    --data_dir /path/to/images \\\n    --output_dir results \\\n    --sample_size 5000\n\n# Extract features\npython -m prismh.core.extract_embeddings \\\n    --input_dir results/clean \\\n    --output_dir results/embeddings\n\n# Run clustering\npython -m prismh.core.cluster_embeddings \\\n    --embeddings results/embeddings/all_embeddings.npz \\\n    --output_dir results/clustering\n</code></pre>"},{"location":"examples/basic_usage/#step-by-step-examples","title":"Step-by-Step Examples","text":""},{"location":"examples/basic_usage/#1-basic-preprocessing","title":"1. Basic Preprocessing","text":"<pre><code>from prismh.core.preprocess import ImagePreprocessor\n\ndef basic_preprocessing():\n    preprocessor = ImagePreprocessor(\n        input_dir=\"data/raw_images\",\n        output_dir=\"results/preprocessing\"\n    )\n\n    # Run preprocessing\n    preprocessor.run_preprocessing()\n\n    # Check results\n    clean_dir = Path(\"results/preprocessing/clean\")\n    print(f\"Clean images: {len(list(clean_dir.glob('*')))}\")\n\nbasic_preprocessing()\n</code></pre>"},{"location":"examples/basic_usage/#2-feature-extraction","title":"2. Feature Extraction","text":"<pre><code>from prismh.core.extract_embeddings import extract_embeddings_main\n\ndef extract_features():\n    # Extract embeddings from clean images\n    extract_embeddings_main()\n\n    print(\"\u2705 Feature extraction completed\")\n\nextract_features()\n</code></pre>"},{"location":"examples/basic_usage/#3-clustering-analysis","title":"3. Clustering Analysis","text":"<pre><code>from prismh.core.cluster_embeddings import cluster_main\n\ndef run_clustering():\n    # Run clustering on extracted embeddings\n    cluster_main()\n\n    print(\"\u2705 Clustering analysis completed\")\n\nrun_clustering()\n</code></pre>"},{"location":"examples/basic_usage/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef performance_monitor(operation_name):\n    \"\"\"Monitor operation performance\"\"\"\n    start_time = time.time()\n    print(f\"\ud83d\ude80 Starting {operation_name}\")\n\n    try:\n        yield\n    finally:\n        duration = time.time() - start_time\n        print(f\"\u2705 {operation_name} completed in {duration:.2f} seconds\")\n\n# Usage example\nwith performance_monitor(\"Image Preprocessing\"):\n    preprocessor = ImagePreprocessor(\"data/images\", \"results\")\n    preprocessor.run_preprocessing()\n</code></pre>"},{"location":"examples/basic_usage/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/basic_usage/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_multiple_datasets(datasets):\n    \"\"\"Process multiple image datasets\"\"\"\n    for dataset_info in datasets:\n        print(f\"Processing {dataset_info['name']}...\")\n\n        preprocessor = ImagePreprocessor(\n            input_dir=dataset_info['input'],\n            output_dir=dataset_info['output']\n        )\n        preprocessor.run_preprocessing()\n\n        print(f\"Completed: {dataset_info['name']}\")\n\n# Example usage\ndatasets = [\n    {'name': 'Dataset_A', 'input': 'data/a', 'output': 'results/a'},\n    {'name': 'Dataset_B', 'input': 'data/b', 'output': 'results/b'}\n]\n\nprocess_multiple_datasets(datasets)\n</code></pre>"},{"location":"examples/basic_usage/#common-use-cases","title":"Common Use Cases","text":""},{"location":"examples/basic_usage/#processing-sample-data","title":"Processing Sample Data","text":"<pre><code># Process a small sample for testing\ndef process_sample():\n    preprocessor = ImagePreprocessor(\n        input_dir=\"data/sample\",\n        output_dir=\"results/sample\",\n        ccthreshold=0.9,\n        outlier_distance=0.68\n    )\n\n    preprocessor.run_preprocessing()\n\n    # Print summary\n    clean_count = len(list(Path(\"results/sample/clean\").glob(\"*\")))\n    print(f\"Processed sample: {clean_count} clean images\")\n\nprocess_sample()\n</code></pre>"},{"location":"examples/basic_usage/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Advanced preprocessing with custom settings\ndef advanced_preprocessing():\n    preprocessor = ImagePreprocessor(\n        input_dir=\"data/large_dataset\",\n        output_dir=\"results/advanced\",\n        ccthreshold=0.85,        # More strict duplicate detection\n        outlier_distance=0.75    # More lenient outlier detection\n    )\n\n    preprocessor.run_preprocessing()\n\nadvanced_preprocessing()\n</code></pre>"},{"location":"examples/basic_usage/#next-steps","title":"Next Steps","text":"<p>After completing these examples:</p> <ul> <li>\ud83d\udcd6 Configuration Guide - Customize parameters</li> <li>\ud83d\udd27 API Reference - Detailed function documentation</li> <li>\ud83d\udcca Advanced Examples - Complex workflows </li> </ul>"},{"location":"guide/configuration/","title":"Configuration Guide","text":"<p>This guide explains how to configure Prism-H for different use cases and environments. The system provides multiple configuration methods to suit different workflows.</p>"},{"location":"guide/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"guide/configuration/#1-command-line-arguments","title":"1. Command Line Arguments","text":"<p>The most common way to configure the system:</p> <pre><code># Preprocessing configuration\npython -m prismh.core.preprocess \\\n    --data_dir /path/to/images \\\n    --output_dir results \\\n    --ccthreshold 0.9 \\\n    --outlier_distance 0.68 \\\n    --sample_size 5000\n\n# Feature extraction configuration\npython -m prismh.core.extract_embeddings \\\n    --input_dir results/clean \\\n    --output_dir results/embeddings \\\n    --batch_size 64 \\\n    --device cuda\n</code></pre>"},{"location":"guide/configuration/#2-environment-variables","title":"2. Environment Variables","text":"<p>Set system-wide defaults:</p> <pre><code># Data paths\nexport PRISMH_DATA_DIR=\"/path/to/default/data\"\nexport PRISMH_OUTPUT_DIR=\"/path/to/default/results\"\n\n# Model configuration\nexport PRISMH_MODEL_PATH=\"/path/to/simclr/model.pt\"\nexport PRISMH_DEVICE=\"cuda\"\n\n# Processing parameters\nexport PRISMH_BATCH_SIZE=\"64\"\nexport PRISMH_NUM_WORKERS=\"4\"\n</code></pre>"},{"location":"guide/configuration/#3-configuration-files","title":"3. Configuration Files","text":"<p>Create YAML configuration files for complex setups:</p> <pre><code># config/preprocessing.yaml\npreprocessing:\n  ccthreshold: 0.9\n  outlier_distance: 0.68\n  sample_size: 10000\n  quality_thresholds:\n    dark_threshold: 13\n    blur_threshold: 50\n\n# config/simclr.yaml\nsimclr:\n  model:\n    base_model: \"resnet50\"\n    output_dim: 128\n    pretrained: true\n  training:\n    batch_size: 32\n    learning_rate: 0.001\n    epochs: 100\n    temperature: 0.5\n\n# config/extraction.yaml\nextraction:\n  batch_size: 64\n  num_workers: 4\n  device: \"auto\"\n  image_size: 224\n</code></pre>"},{"location":"guide/configuration/#4-python-configuration","title":"4. Python Configuration","text":"<p>Direct configuration in Python code:</p> <pre><code>from prismh.core.preprocess import ImagePreprocessor\nfrom prismh.config import Config\n\n# Using configuration objects\nconfig = Config({\n    'preprocessing': {\n        'ccthreshold': 0.85,\n        'outlier_distance': 0.70\n    },\n    'extraction': {\n        'batch_size': 32,\n        'device': 'cuda'\n    }\n})\n\n# Initialize with configuration\npreprocessor = ImagePreprocessor(\n    input_dir=\"data/images\",\n    output_dir=\"results\",\n    **config.preprocessing\n)\n</code></pre>"},{"location":"guide/configuration/#module-specific-configuration","title":"Module-Specific Configuration","text":""},{"location":"guide/configuration/#preprocessing-configuration","title":"Preprocessing Configuration","text":""},{"location":"guide/configuration/#core-parameters","title":"Core Parameters","text":"Parameter Type Default Range Description <code>ccthreshold</code> float 0.9 0.0-1.0 Similarity threshold for duplicate detection <code>outlier_distance</code> float 0.68 0.0-1.0 Distance threshold for outlier detection <code>sample_size</code> int None &gt;0 Number of images to process (None for all)"},{"location":"guide/configuration/#quality-thresholds","title":"Quality Thresholds","text":"Parameter Type Default Description <code>dark_threshold</code> int 13 Mean brightness threshold for dark images <code>blur_threshold</code> int 50 Variance threshold for blur detection <code>min_file_size</code> int 1024 Minimum file size in bytes"},{"location":"guide/configuration/#example-configuration","title":"Example Configuration","text":"<pre><code># Conservative settings (higher quality)\nconservative_config = {\n    'ccthreshold': 0.95,        # Very strict duplicate detection\n    'outlier_distance': 0.60,   # More aggressive outlier removal\n    'dark_threshold': 20,       # Higher brightness requirement\n    'blur_threshold': 100       # Stricter blur detection\n}\n\n# Permissive settings (keep more images)\npermissive_config = {\n    'ccthreshold': 0.80,        # More lenient duplicate detection\n    'outlier_distance': 0.80,   # Keep more outliers\n    'dark_threshold': 8,        # Accept darker images\n    'blur_threshold': 30        # Accept more blur\n}\n</code></pre>"},{"location":"guide/configuration/#simclr-training-configuration","title":"SimCLR Training Configuration","text":""},{"location":"guide/configuration/#model-architecture","title":"Model Architecture","text":"Parameter Type Default Options Description <code>base_model</code> str \"resnet50\" resnet18, resnet50 Backbone architecture <code>output_dim</code> int 128 64, 128, 256 Projection head output dimension <code>pretrained</code> bool true true, false Use ImageNet pretrained weights"},{"location":"guide/configuration/#training-parameters","title":"Training Parameters","text":"Parameter Type Default Range Description <code>batch_size</code> int 32 8-512 Training batch size <code>learning_rate</code> float 0.001 1e-5 to 1e-2 Initial learning rate <code>epochs</code> int 100 1-1000 Number of training epochs <code>temperature</code> float 0.5 0.1-1.0 Contrastive loss temperature <code>weight_decay</code> float 1e-4 0-1e-2 L2 regularization strength"},{"location":"guide/configuration/#data-augmentation","title":"Data Augmentation","text":"<pre><code># Default augmentation configuration\naugmentation_config = {\n    'resize': 256,\n    'crop_size': 224,\n    'horizontal_flip_prob': 0.5,\n    'color_jitter': {\n        'brightness': 0.4,\n        'contrast': 0.4,\n        'saturation': 0.4,\n        'hue': 0.1,\n        'prob': 0.8\n    },\n    'grayscale_prob': 0.2,\n    'gaussian_blur': {\n        'kernel_size': 23,\n        'sigma': [0.1, 2.0],\n        'prob': 0.5\n    }\n}\n\n# Strong augmentation for challenging datasets\nstrong_augmentation = {\n    'color_jitter': {\n        'brightness': 0.8,\n        'contrast': 0.8,\n        'saturation': 0.8,\n        'hue': 0.2,\n        'prob': 0.8\n    },\n    'gaussian_blur': {\n        'prob': 0.8\n    }\n}\n</code></pre>"},{"location":"guide/configuration/#feature-extraction-configuration","title":"Feature Extraction Configuration","text":""},{"location":"guide/configuration/#processing-parameters","title":"Processing Parameters","text":"Parameter Type Default Description <code>batch_size</code> int 64 Inference batch size <code>num_workers</code> int 0 DataLoader worker processes <code>device</code> str \"auto\" Device (cpu/cuda/mps/auto) <code>pin_memory</code> bool true Enable memory pinning for GPU"},{"location":"guide/configuration/#model-configuration","title":"Model Configuration","text":"Parameter Type Default Description <code>model_path</code> str auto Path to trained SimCLR model <code>checkpoint_key</code> str \"model_state_dict\" Key for model weights in checkpoint <code>strict_loading</code> bool true Strict state dict loading"},{"location":"guide/configuration/#clustering-configuration","title":"Clustering Configuration","text":""},{"location":"guide/configuration/#fastdup-parameters","title":"Fastdup Parameters","text":"Parameter Type Default Description <code>threshold</code> float 0.9 Similarity threshold for clustering <code>min_cluster_size</code> int 2 Minimum images per cluster <code>ccthreshold</code> float 0.96 Connected components threshold"},{"location":"guide/configuration/#visualization-parameters","title":"Visualization Parameters","text":"Parameter Type Default Description <code>max_images_per_cluster</code> int 50 Maximum images shown per cluster <code>image_size</code> tuple (224, 224) Display image size <code>gallery_format</code> str \"html\" Output format (html/json)"},{"location":"guide/configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"guide/configuration/#development-environment","title":"Development Environment","text":"<pre><code># config/dev.yaml\ndevelopment:\n  preprocessing:\n    sample_size: 1000      # Small sample for fast iteration\n    ccthreshold: 0.85      # Moderate quality filtering\n\n  simclr:\n    training:\n      epochs: 10           # Quick training\n      batch_size: 16       # Small batch for limited GPU memory\n\n  extraction:\n    batch_size: 32         # Conservative batch size\n    device: \"cpu\"          # Fallback to CPU if needed\n</code></pre>"},{"location":"guide/configuration/#production-environment","title":"Production Environment","text":"<pre><code># config/prod.yaml\nproduction:\n  preprocessing:\n    sample_size: null      # Process all images\n    ccthreshold: 0.92      # High quality filtering\n\n  simclr:\n    training:\n      epochs: 200          # Thorough training\n      batch_size: 64       # Utilize full GPU capacity\n\n  extraction:\n    batch_size: 128        # Large batch for efficiency\n    device: \"cuda\"         # GPU acceleration\n    num_workers: 8         # Parallel data loading\n</code></pre>"},{"location":"guide/configuration/#cloudhpc-environment","title":"Cloud/HPC Environment","text":"<pre><code># config/cloud.yaml\ncloud:\n  preprocessing:\n    sample_size: 100000    # Large-scale processing\n\n  simclr:\n    training:\n      batch_size: 256      # Large batch for distributed training\n      num_gpus: 4          # Multi-GPU setup\n\n  extraction:\n    batch_size: 512        # High-throughput processing\n    distributed: true      # Distributed processing\n</code></pre>"},{"location":"guide/configuration/#hardware-specific-optimization","title":"Hardware-Specific Optimization","text":""},{"location":"guide/configuration/#gpu-configuration","title":"GPU Configuration","text":"<pre><code># NVIDIA GPU optimization\ngpu_config = {\n    'device': 'cuda',\n    'batch_size': 128,\n    'num_workers': 8,\n    'pin_memory': True,\n    'mixed_precision': True,\n    'compile_model': True  # PyTorch 2.0+\n}\n\n# Multi-GPU configuration\nmulti_gpu_config = {\n    'device': 'cuda',\n    'data_parallel': True,\n    'devices': [0, 1, 2, 3],\n    'batch_size': 256,  # Total batch size across GPUs\n    'sync_batchnorm': True\n}\n</code></pre>"},{"location":"guide/configuration/#cpu-configuration","title":"CPU Configuration","text":"<pre><code># CPU optimization\ncpu_config = {\n    'device': 'cpu',\n    'batch_size': 32,\n    'num_workers': 4,  # Number of CPU cores\n    'pin_memory': False,\n    'mixed_precision': False\n}\n</code></pre>"},{"location":"guide/configuration/#apple-silicon-m1m2-configuration","title":"Apple Silicon (M1/M2) Configuration","text":"<pre><code># Apple Silicon optimization\nmps_config = {\n    'device': 'mps',\n    'batch_size': 64,\n    'num_workers': 0,  # MPS works best with num_workers=0\n    'pin_memory': False\n}\n</code></pre>"},{"location":"guide/configuration/#dataset-specific-configuration","title":"Dataset-Specific Configuration","text":""},{"location":"guide/configuration/#large-dataset-100k-images","title":"Large Dataset (&gt;100k images)","text":"<pre><code>large_dataset:\n  preprocessing:\n    sample_size: null\n    ccthreshold: 0.90\n    outlier_distance: 0.65\n\n  extraction:\n    batch_size: 128\n    streaming: true        # Stream data to reduce memory usage\n    checkpoint_frequency: 1000\n\n  clustering:\n    max_samples: 50000     # Subsample for clustering if needed\n    threshold: 0.92\n</code></pre>"},{"location":"guide/configuration/#small-dataset-10k-images","title":"Small Dataset (&lt;10k images)","text":"<pre><code>small_dataset:\n  preprocessing:\n    ccthreshold: 0.85      # More permissive to retain data\n    outlier_distance: 0.75\n\n  simclr:\n    training:\n      epochs: 300          # More epochs for small datasets\n      batch_size: 16       # Smaller batches\n      augmentation_strength: 'strong'\n\n  clustering:\n    min_cluster_size: 1    # Allow singleton clusters\n</code></pre>"},{"location":"guide/configuration/#noisy-dataset","title":"Noisy Dataset","text":"<pre><code>noisy_dataset:\n  preprocessing:\n    ccthreshold: 0.95      # Strict duplicate removal\n    outlier_distance: 0.60 # Aggressive outlier removal\n    dark_threshold: 25     # Higher quality requirements\n    blur_threshold: 80\n\n  simclr:\n    training:\n      temperature: 0.3     # Lower temperature for noisy data\n      weight_decay: 1e-3   # More regularization\n</code></pre>"},{"location":"guide/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guide/configuration/#custom-configuration-classes","title":"Custom Configuration Classes","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass PreprocessingConfig:\n    ccthreshold: float = 0.9\n    outlier_distance: float = 0.68\n    sample_size: Optional[int] = None\n    dark_threshold: int = 13\n    blur_threshold: int = 50\n\n    def validate(self):\n        assert 0.0 &lt;= self.ccthreshold &lt;= 1.0\n        assert 0.0 &lt;= self.outlier_distance &lt;= 1.0\n        if self.sample_size is not None:\n            assert self.sample_size &gt; 0\n\n@dataclass\nclass SimCLRConfig:\n    base_model: str = \"resnet50\"\n    output_dim: int = 128\n    batch_size: int = 32\n    learning_rate: float = 0.001\n    temperature: float = 0.5\n    epochs: int = 100\n\n    def __post_init__(self):\n        assert self.base_model in [\"resnet18\", \"resnet50\"]\n        assert self.output_dim in [64, 128, 256]\n\n# Usage\nconfig = PreprocessingConfig(ccthreshold=0.85, sample_size=5000)\nconfig.validate()\n</code></pre>"},{"location":"guide/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code>class BaseConfig:\n    def __init__(self):\n        self.load_defaults()\n\n    def load_defaults(self):\n        self.preprocessing = PreprocessingConfig()\n        self.simclr = SimCLRConfig()\n\n    def update_from_file(self, config_file):\n        import yaml\n        with open(config_file) as f:\n            config_data = yaml.safe_load(f)\n        self._update_from_dict(config_data)\n\n    def update_from_env(self):\n        import os\n        if 'PRISMH_CCTHRESHOLD' in os.environ:\n            self.preprocessing.ccthreshold = float(os.environ['PRISMH_CCTHRESHOLD'])\n        # ... more environment variables\n\nclass DevelopmentConfig(BaseConfig):\n    def load_defaults(self):\n        super().load_defaults()\n        self.preprocessing.sample_size = 1000\n        self.simclr.epochs = 10\n\nclass ProductionConfig(BaseConfig):\n    def load_defaults(self):\n        super().load_defaults()\n        self.preprocessing.ccthreshold = 0.92\n        self.simclr.epochs = 200\n</code></pre>"},{"location":"guide/configuration/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>def get_config_for_dataset(dataset_path):\n    \"\"\"Automatically configure based on dataset characteristics\"\"\"\n    import os\n\n    # Count images\n    image_count = len([f for f in os.listdir(dataset_path) \n                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n\n    if image_count &lt; 5000:\n        return SmallDatasetConfig()\n    elif image_count &gt; 100000:\n        return LargeDatasetConfig()\n    else:\n        return StandardConfig()\n\ndef auto_configure_hardware():\n    \"\"\"Automatically configure based on available hardware\"\"\"\n    import torch\n\n    config = {}\n\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n        if gpu_memory &gt; 16e9:  # 16GB+\n            config['batch_size'] = 128\n        elif gpu_memory &gt; 8e9:  # 8GB+\n            config['batch_size'] = 64\n        else:\n            config['batch_size'] = 32\n        config['device'] = 'cuda'\n    else:\n        config['batch_size'] = 16\n        config['device'] = 'cpu'\n\n    return config\n</code></pre>"},{"location":"guide/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"guide/configuration/#parameter-validation","title":"Parameter Validation","text":"<pre><code>def validate_preprocessing_config(config):\n    \"\"\"Validate preprocessing configuration\"\"\"\n    errors = []\n\n    if not 0.0 &lt;= config.ccthreshold &lt;= 1.0:\n        errors.append(\"ccthreshold must be between 0.0 and 1.0\")\n\n    if not 0.0 &lt;= config.outlier_distance &lt;= 1.0:\n        errors.append(\"outlier_distance must be between 0.0 and 1.0\")\n\n    if config.sample_size is not None and config.sample_size &lt;= 0:\n        errors.append(\"sample_size must be positive\")\n\n    if errors:\n        raise ValueError(\"Configuration errors: \" + \"; \".join(errors))\n\ndef validate_hardware_config(config):\n    \"\"\"Validate hardware configuration\"\"\"\n    import torch\n\n    if config.device == 'cuda' and not torch.cuda.is_available():\n        raise ValueError(\"CUDA requested but not available\")\n\n    if config.device == 'mps' and not torch.backends.mps.is_available():\n        raise ValueError(\"MPS requested but not available\")\n\n    if config.batch_size &gt; 512:\n        print(\"Warning: Very large batch size may cause memory issues\")\n</code></pre>"},{"location":"guide/configuration/#configuration-compatibility","title":"Configuration Compatibility","text":"<pre><code>def check_config_compatibility(preprocess_config, simclr_config):\n    \"\"\"Check compatibility between different module configurations\"\"\"\n    warnings = []\n\n    # Check if sample size is appropriate for training epochs\n    if (preprocess_config.sample_size and \n        preprocess_config.sample_size &lt; 1000 and \n        simclr_config.epochs &gt; 50):\n        warnings.append(\"Small sample size with many epochs may cause overfitting\")\n\n    # Check batch size vs dataset size\n    if (preprocess_config.sample_size and \n        simclr_config.batch_size &gt; preprocess_config.sample_size // 10):\n        warnings.append(\"Batch size may be too large for dataset size\")\n\n    for warning in warnings:\n        print(f\"Warning: {warning}\")\n</code></pre>"},{"location":"guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"guide/configuration/#configuration-management","title":"Configuration Management","text":"<ol> <li>Use version control for configuration files</li> <li>Separate configs by environment (dev/staging/prod)</li> <li>Document configuration changes and their impact</li> <li>Validate configurations before running experiments</li> <li>Use meaningful parameter names and comments</li> </ol>"},{"location":"guide/configuration/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Profile different configurations to find optimal settings</li> <li>Monitor resource usage (GPU memory, CPU, disk I/O)</li> <li>Adjust batch sizes based on available hardware</li> <li>Use mixed precision for compatible hardware</li> <li>Enable compilation with PyTorch 2.0+</li> </ol>"},{"location":"guide/configuration/#reproducibility","title":"Reproducibility","text":"<pre><code># Ensure reproducible results\nreproducibility_config = {\n    'random_seed': 42,\n    'deterministic': True,\n    'benchmark': False,  # Disable cudnn benchmark for reproducibility\n    'num_workers': 0     # Avoid multiprocessing for deterministic results\n}\n\n# Set seeds\nimport torch\nimport numpy as np\nimport random\n\ndef set_reproducible_config(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"guide/configuration/#configuration-examples-by-use-case","title":"Configuration Examples by Use Case","text":""},{"location":"guide/configuration/#researchexperimentation","title":"Research/Experimentation","text":"<pre><code>research:\n  preprocessing:\n    sample_size: 5000\n    ccthreshold: 0.85\n  simclr:\n    epochs: 50\n    batch_size: 32\n    save_frequency: 10\n  logging:\n    level: DEBUG\n    tensorboard: true\n    save_embeddings: true\n</code></pre>"},{"location":"guide/configuration/#production-deployment","title":"Production Deployment","text":"<pre><code>production:\n  preprocessing:\n    ccthreshold: 0.92\n    quality_checks: strict\n  extraction:\n    batch_size: 128\n    optimization: maximum\n  monitoring:\n    metrics: true\n    alerts: true\n    performance_tracking: true\n</code></pre>"},{"location":"guide/configuration/#edgemobile-deployment","title":"Edge/Mobile Deployment","text":"<pre><code>edge:\n  model:\n    quantization: int8\n    pruning: 0.3\n  processing:\n    batch_size: 1\n    memory_limit: 512MB\n  optimization:\n    model_compression: true\n    inference_only: true\n</code></pre> <p>This configuration system provides the flexibility to adapt Prism-H to various environments, datasets, and use cases while maintaining reproducibility and performance.</p>"},{"location":"guide/getting-started/","title":"Getting Started","text":"<p>How to get started with Prism-H.</p>"},{"location":"guide/installation/","title":"Installation Guide","text":"<p>This guide will help you set up the Prism-H mosquito breeding spot detection system on your local machine.</p>"},{"location":"guide/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Prism-H, ensure you have the following prerequisites:</p> <ul> <li>Python 3.11 or higher</li> <li>Git (for cloning the repository)</li> <li>Poetry (recommended) or pip for dependency management</li> <li>CUDA-compatible GPU (optional, for faster training)</li> </ul>"},{"location":"guide/installation/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended Python 3.11+ 3.11+ RAM 8GB 16GB+ Storage 10GB free 50GB+ GPU None (CPU works) CUDA 11.7+"},{"location":"guide/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"guide/installation/#method-1-using-poetry-recommended","title":"Method 1: Using Poetry (Recommended)","text":"<p>Poetry provides better dependency management and environment isolation.</p>"},{"location":"guide/installation/#1-install-poetry","title":"1. Install Poetry","text":"Linux/macOSWindows <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre>"},{"location":"guide/installation/#2-clone-and-setup","title":"2. Clone and Setup","text":"<pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd prism-h\n\n# Install dependencies\npoetry install\n\n# Activate the environment\npoetry shell\n</code></pre>"},{"location":"guide/installation/#method-2-using-pip-and-virtualenv","title":"Method 2: Using pip and virtualenv","text":"<p>If you prefer using pip and virtual environments:</p> <pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd prism-h\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate virtual environment\n# On Linux/macOS:\nsource .venv/bin/activate\n# On Windows:\n.venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"guide/installation/#verification","title":"Verification","text":"<p>Verify your installation by running:</p> <pre><code># Check Python version\npython --version\n\n# Test GPU availability (if applicable)\npython check_gpu.py\n\n# Run a quick test\npython -c \"import torch; print(f'PyTorch version: {torch.__version__}')\"\n</code></pre>"},{"location":"guide/installation/#additional-dependencies","title":"Additional Dependencies","text":""},{"location":"guide/installation/#for-gpu-support","title":"For GPU Support","text":"<p>If you have a CUDA-compatible GPU, install the appropriate PyTorch version:</p> <pre><code># For CUDA 11.7\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n\n# For CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"guide/installation/#for-apple-silicon-m1m2-macs","title":"For Apple Silicon (M1/M2) Macs","text":"<pre><code># Install with MPS (Metal Performance Shaders) support\npip install torch torchvision torchaudio\n</code></pre>"},{"location":"guide/installation/#for-development","title":"For Development","text":"<p>If you plan to contribute to the project:</p> <pre><code># Install development dependencies\npoetry install --group dev\n\n# Or with pip:\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"guide/installation/#configuration-files","title":"Configuration Files","text":"<p>After installation, you'll need to set up configuration files:</p>"},{"location":"guide/installation/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Copy the example environment file\ncp .env.example .env\n\n# Edit with your specific settings\nnano .env\n</code></pre> <p>Example <code>.env</code> content: <pre><code># Data paths\nDATA_DIR=/path/to/your/data\nRESULTS_DIR=/path/to/results\n\n# HuggingFace token (if needed)\nHF_TOKEN=your_huggingface_token_here\n\n# GPU settings\nCUDA_VISIBLE_DEVICES=0\n</code></pre></p>"},{"location":"guide/installation/#model-configurations","title":"Model Configurations","text":"<p>The project uses several configuration files located in the <code>configs/</code> directory:</p> <ul> <li><code>preprocessing.yaml</code> - Preprocessing parameters</li> <li><code>simclr.yaml</code> - Self-supervised learning settings</li> <li><code>classification.yaml</code> - Classification model parameters</li> </ul>"},{"location":"guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/installation/#common-issues","title":"Common Issues","text":""},{"location":"guide/installation/#poetry-installation-problems","title":"Poetry Installation Problems","text":"<pre><code># If poetry command not found, add to PATH\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Or reinstall poetry\ncurl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | python3 -\n</code></pre>"},{"location":"guide/installation/#cudagpu-issues","title":"CUDA/GPU Issues","text":"<pre><code># Check CUDA version\nnvidia-smi\n\n# Install specific PyTorch version\npip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>"},{"location":"guide/installation/#memory-issues","title":"Memory Issues","text":"<p>If you encounter out-of-memory errors:</p> <pre><code># Reduce batch size in configuration files\n# Or set environment variable\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n</code></pre>"},{"location":"guide/installation/#permission-errors","title":"Permission Errors","text":"<pre><code># On Linux/macOS, you might need to adjust permissions\nchmod +x scripts/*.py\n</code></pre>"},{"location":"guide/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":"LinuxmacOSWindows <pre><code># Install system dependencies (Ubuntu/Debian)\nsudo apt-get update\nsudo apt-get install python3-dev build-essential\n</code></pre> <pre><code># Install Xcode command line tools\nxcode-select --install\n\n# Install Homebrew (if not already installed)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <pre><code># Install Microsoft C++ Build Tools\n# Download from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n</code></pre>"},{"location":"guide/installation/#data-setup","title":"Data Setup","text":"<p>After installation, you'll need to prepare your data:</p> <ol> <li>Image Data: Place your images in the designated data directory</li> <li>Metadata: Ensure JSON metadata files are properly formatted</li> <li>Directory Structure: Follow the expected directory structure</li> </ol> <pre><code>data/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2514\u2500\u2500 processed/\n\u251c\u2500\u2500 metadata/\n\u2502   \u2514\u2500\u2500 annotations.json\n\u2514\u2500\u2500 results/\n    \u251c\u2500\u2500 preprocessing/\n    \u251c\u2500\u2500 embeddings/\n    \u2514\u2500\u2500 models/\n</code></pre>"},{"location":"guide/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete:</p> <ol> <li>\ud83d\udcd6 Quick Start - Run your first analysis</li> <li>\u2699\ufe0f Configuration - Customize settings</li> <li>\ud83d\udccb User Guide - Learn about all features</li> </ol>"},{"location":"guide/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during installation:</p> <ol> <li>Check the troubleshooting section above</li> <li>Review the project's GitHub issues</li> <li>Ensure all system requirements are met</li> <li>Try the alternative installation method</li> </ol> <p>Important</p> <p>Make sure to activate your virtual environment (<code>poetry shell</code> or <code>source .venv/bin/activate</code>) before running any project commands.</p>"},{"location":"guide/overview/","title":"Project Overview","text":"<p>Prism-H is an AI-driven system designed to revolutionize mosquito breeding spot detection and analysis. This comprehensive guide explains the technical approach, methodology, and components that make up the system.</p>"},{"location":"guide/overview/#problem-context","title":"Problem Context","text":""},{"location":"guide/overview/#the-challenge","title":"The Challenge","text":"<p>Mosquito-borne diseases pose a significant public health threat, particularly in regions where stagnant water creates ideal breeding conditions. ASHA (Accredited Social Health Activist) workers are responsible for identifying and documenting these breeding sites, but face several critical challenges:</p>"},{"location":"guide/overview/#data-quality-issues","title":"Data Quality Issues","text":"<ul> <li>Mislabeling: Up to 40% of images have incorrect or missing container type labels</li> <li>Duplicates: Multiple images of the same location clog the dataset</li> <li>Poor Quality: Blurry, dark, or poorly framed images make analysis difficult</li> <li>Inconsistent Protocols: Lack of standardized data collection procedures</li> </ul>"},{"location":"guide/overview/#resource-constraints","title":"Resource Constraints","text":"<ul> <li>Limited Training: Workers often lack proper guidance on effective image capture</li> <li>No Feedback Loop: No real-time quality assessment of submitted images</li> <li>Inefficient Workflow: Manual sorting and analysis is time-consuming and error-prone</li> </ul>"},{"location":"guide/overview/#technical-approach","title":"Technical Approach","text":""},{"location":"guide/overview/#multi-stage-ai-pipeline","title":"Multi-Stage AI Pipeline","text":"<p>Our solution employs a sophisticated multi-stage pipeline that combines self-supervised learning, computer vision, and geospatial analysis:</p> <pre><code>graph TD\n    A[Raw Images + Metadata] --&gt; B[Quality Assessment]\n    B --&gt; C[Data Cleaning]\n    C --&gt; D[Self-Supervised Learning]\n    D --&gt; E[Feature Extraction]\n    E --&gt; F[Clustering Analysis]\n    E --&gt; G[Classification]\n    F --&gt; H[Pattern Discovery]\n    G --&gt; I[Container Detection]\n    A --&gt; J[Metadata Analysis]\n    J --&gt; K[Spatial-Temporal Insights]\n    H --&gt; L[Comprehensive Reports]\n    I --&gt; L\n    K --&gt; L\n</code></pre>"},{"location":"guide/overview/#core-methodology","title":"Core Methodology","text":""},{"location":"guide/overview/#1-self-supervised-learning-simclr","title":"1. Self-Supervised Learning (SimCLR)","text":"<ul> <li>Approach: Learns visual representations without requiring extensive labeled data</li> <li>Architecture: ResNet-50 backbone with contrastive learning</li> <li>Benefits: Reduces dependency on manual labeling while capturing domain-specific features</li> </ul>"},{"location":"guide/overview/#2-quality-based-filtering","title":"2. Quality-Based Filtering","text":"<ul> <li>Fastdup Integration: Automated detection of problematic images</li> <li>Multi-Criteria Assessment: Evaluates blur, lighting, duplicates, and validity</li> <li>Efficiency Gains: Reduces manual review time by 70%</li> </ul>"},{"location":"guide/overview/#3-intelligent-classification","title":"3. Intelligent Classification","text":"<ul> <li>Multi-Class Recognition: Identifies container types (drums, sumps, plant pots)</li> <li>Object Detection: Grounding DINO for precise localization</li> <li>Confidence Scoring: Provides reliability metrics for each prediction</li> </ul>"},{"location":"guide/overview/#4-geospatial-intelligence","title":"4. Geospatial Intelligence","text":"<ul> <li>Location-Based Analysis: GPS coordinate integration for spatial insights</li> <li>Temporal Patterns: Time-series analysis of breeding site development</li> <li>Worker Behavior: Analysis of data collection patterns and quality</li> </ul>"},{"location":"guide/overview/#system-architecture","title":"System Architecture","text":""},{"location":"guide/overview/#core-components","title":"Core Components","text":""},{"location":"guide/overview/#data-processing-layer","title":"Data Processing Layer","text":"<pre><code># Preprocessing Pipeline\nImagePreprocessor\n\u251c\u2500\u2500 Quality Assessment\n\u251c\u2500\u2500 Duplicate Detection  \n\u251c\u2500\u2500 Outlier Identification\n\u2514\u2500\u2500 Data Categorization\n</code></pre>"},{"location":"guide/overview/#machine-learning-layer","title":"Machine Learning Layer","text":"<pre><code># ML Pipeline\nSimCLRModel\n\u251c\u2500\u2500 Feature Extraction\n\u251c\u2500\u2500 Representation Learning\n\u2514\u2500\u2500 Domain Adaptation\n\nClassificationModel\n\u251c\u2500\u2500 Container Type Recognition\n\u251c\u2500\u2500 Quality Assessment\n\u2514\u2500\u2500 Confidence Scoring\n</code></pre>"},{"location":"guide/overview/#analysis-layer","title":"Analysis Layer","text":"<pre><code># Analytics Pipeline\nMetadataIntegrator\n\u251c\u2500\u2500 Spatial Analysis\n\u251c\u2500\u2500 Temporal Patterns\n\u251c\u2500\u2500 Worker Performance\n\u2514\u2500\u2500 Intervention Insights\n</code></pre>"},{"location":"guide/overview/#data-flow","title":"Data Flow","text":"<ol> <li>Input Stage: Raw images and JSON metadata</li> <li>Preprocessing: Quality filtering and data cleaning</li> <li>Feature Learning: Self-supervised embedding extraction</li> <li>Analysis: Clustering, classification, and pattern discovery</li> <li>Integration: Metadata fusion and geospatial analysis</li> <li>Output: Comprehensive reports and visualizations</li> </ol>"},{"location":"guide/overview/#key-features","title":"Key Features","text":""},{"location":"guide/overview/#intelligent-data-cleaning","title":"\ud83d\udd0d Intelligent Data Cleaning","text":"<p>Automated Quality Assessment - Detects blurry images using variance of Laplacian - Identifies dark/overexposed images through histogram analysis - Finds duplicates using perceptual hashing - Flags invalid/corrupted files</p> <p>Efficiency Metrics - 95% accuracy in quality classification - 70% reduction in manual review time - 85% improvement in dataset quality</p>"},{"location":"guide/overview/#advanced-machine-learning","title":"\ud83e\udde0 Advanced Machine Learning","text":"<p>Self-Supervised Learning - SimCLR implementation for domain-specific feature learning - Contrastive learning without extensive labeled data - Transfer learning from ImageNet with fine-tuning</p> <p>Classification Pipeline - Multi-class container type recognition - Hierarchical classification for nested categories - Ensemble methods for improved accuracy</p>"},{"location":"guide/overview/#comprehensive-analytics","title":"\ud83d\udcca Comprehensive Analytics","text":"<p>Clustering Analysis - Unsupervised pattern discovery - Outlier detection and analysis - Visual similarity grouping</p> <p>Metadata Integration - GPS-based spatial analysis - Temporal pattern recognition - Worker behavior analysis</p>"},{"location":"guide/overview/#geospatial-intelligence","title":"\ud83d\uddfa\ufe0f Geospatial Intelligence","text":"<p>Location-Based Insights - Hotspot identification and mapping - Territory optimization for workers - Duplicate location detection</p> <p>Temporal Analysis - Seasonal pattern recognition - Intervention effectiveness tracking - Predictive modeling for breeding cycles</p>"},{"location":"guide/overview/#technical-specifications","title":"Technical Specifications","text":""},{"location":"guide/overview/#performance-metrics","title":"Performance Metrics","text":"Component Metric Value Preprocessing Throughput 1000+ images/min Quality Detection Accuracy 95% Duplicate Detection Precision 92% Classification F1-Score 0.87 Feature Extraction Speed 64 images/batch"},{"location":"guide/overview/#scalability","title":"Scalability","text":"<ul> <li>Batch Processing: Handles 200K+ images efficiently</li> <li>Distributed Computing: Supports multi-GPU training</li> <li>Memory Optimization: Efficient data loading and processing</li> <li>Cloud Integration: AWS/GCP deployment ready</li> </ul>"},{"location":"guide/overview/#workflow-integration","title":"Workflow Integration","text":""},{"location":"guide/overview/#data-collection-phase","title":"Data Collection Phase","text":"<ol> <li>Image Capture: ASHA workers photograph breeding spots</li> <li>Metadata Recording: GPS, timestamp, and worker ID captured</li> <li>Quality Feedback: Real-time assessment and guidance</li> </ol>"},{"location":"guide/overview/#processing-phase","title":"Processing Phase","text":"<ol> <li>Automated Cleaning: Quality filtering and deduplication</li> <li>Feature Extraction: Self-supervised learning pipeline</li> <li>Analysis: Clustering and classification</li> <li>Integration: Metadata fusion and spatial analysis</li> </ol>"},{"location":"guide/overview/#output-phase","title":"Output Phase","text":"<ol> <li>Reports: Comprehensive analysis summaries</li> <li>Visualizations: Interactive maps and galleries</li> <li>Recommendations: Intervention priorities and worker feedback</li> </ol>"},{"location":"guide/overview/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"guide/overview/#phase-1-foundation-completed","title":"Phase 1: Foundation (Completed)","text":"<ul> <li>\u2705 Core preprocessing pipeline</li> <li>\u2705 SimCLR implementation</li> <li>\u2705 Basic classification system</li> <li>\u2705 Metadata integration framework</li> </ul>"},{"location":"guide/overview/#phase-2-enhancement-in-progress","title":"Phase 2: Enhancement (In Progress)","text":"<ul> <li>\ud83d\udd04 Advanced object detection</li> <li>\ud83d\udd04 Real-time feedback system</li> <li>\ud83d\udd04 Mobile application integration</li> <li>\ud83d\udd04 Cloud deployment optimization</li> </ul>"},{"location":"guide/overview/#phase-3-scale-planned","title":"Phase 3: Scale (Planned)","text":"<ul> <li>\ud83d\udccb Multi-region deployment</li> <li>\ud83d\udccb Predictive modeling</li> <li>\ud83d\udccb Automated intervention recommendations</li> <li>\ud83d\udccb Integration with health systems</li> </ul>"},{"location":"guide/overview/#expected-outcomes","title":"Expected Outcomes","text":""},{"location":"guide/overview/#immediate-benefits","title":"Immediate Benefits","text":"<ul> <li>70% reduction in data cleaning time</li> <li>85% improvement in data quality</li> <li>60% decrease in duplicate submissions</li> <li>50% faster intervention response times</li> </ul>"},{"location":"guide/overview/#long-term-impact","title":"Long-term Impact","text":"<ul> <li>Improved Public Health: More effective mosquito control</li> <li>Enhanced Efficiency: Optimized resource allocation</li> <li>Better Training: Data-driven worker development</li> <li>Scalable Solutions: Replicable across regions</li> </ul>"},{"location":"guide/overview/#research-contributions","title":"Research Contributions","text":""},{"location":"guide/overview/#novel-techniques","title":"Novel Techniques","text":"<ul> <li>Domain-Specific SimCLR: Adaptation for medical imaging</li> <li>Multi-Modal Integration: Combining visual and spatial data</li> <li>Quality-Aware Learning: Incorporating data quality into training</li> </ul>"},{"location":"guide/overview/#publications-presentations","title":"Publications &amp; Presentations","text":"<ul> <li>Technical reports on self-supervised learning applications</li> <li>Conference presentations on AI for public health</li> <li>Open-source contributions to the community</li> </ul>"},{"location":"guide/overview/#future-directions","title":"Future Directions","text":""},{"location":"guide/overview/#technical-enhancements","title":"Technical Enhancements","text":"<ul> <li>Transformer Architectures: Exploring Vision Transformers</li> <li>Federated Learning: Privacy-preserving distributed training</li> <li>Edge Computing: On-device processing capabilities</li> </ul>"},{"location":"guide/overview/#application-expansion","title":"Application Expansion","text":"<ul> <li>Multi-Disease Detection: Extending to other vector-borne diseases</li> <li>Environmental Monitoring: Broader ecosystem health assessment</li> <li>Community Engagement: Citizen science integration</li> </ul>"},{"location":"guide/overview/#next-steps","title":"Next Steps","text":"<p>To dive deeper into specific components:</p> <ul> <li>\ud83d\udd27 Data Preprocessing</li> <li>\ud83e\udde0 Feature Extraction</li> <li>\ud83c\udfaf Classification</li> <li>\ud83d\uddfa\ufe0f Metadata Integration</li> <li>\ud83d\udcca Visualization</li> </ul> <p>Research Context</p> <p>This project is part of ongoing research in AI applications for public health, with a focus on supporting underserved communities and improving health outcomes through technology. </p>"},{"location":"guide/quickstart/","title":"Quick Start Guide","text":"<p>This guide will walk you through running your first mosquito breeding spot analysis with Prism-H.</p>"},{"location":"guide/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have completed the installation process and have:</p> <ul> <li>\u2705 Prism-H installed and environment activated</li> <li>\u2705 Sample data available (images + metadata JSON)</li> <li>\u2705 Basic understanding of the project structure</li> </ul>"},{"location":"guide/quickstart/#basic-workflow","title":"Basic Workflow","text":"<p>The typical Prism-H workflow consists of these steps:</p> <pre><code>graph LR\n    A[Raw Data] --&gt; B[Preprocess]\n    B --&gt; C[Extract Features]\n    C --&gt; D[Cluster/Classify]\n    D --&gt; E[Analyze Results]\n</code></pre>"},{"location":"guide/quickstart/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":""},{"location":"guide/quickstart/#data-structure","title":"Data Structure","text":"<p>Organize your data following this structure:</p> <pre><code>data/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 IMG_001.jpg\n\u2502   \u251c\u2500\u2500 IMG_002.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 metadata/\n    \u2514\u2500\u2500 annotations.json\n</code></pre>"},{"location":"guide/quickstart/#sample-data","title":"Sample Data","text":"<p>If you don't have data yet, you can use the sample dataset:</p> <pre><code># Download sample data (if available)\npython scripts/download_sample_data.py --output data/\n</code></pre>"},{"location":"guide/quickstart/#step-2-data-preprocessing","title":"Step 2: Data Preprocessing","text":"<p>Clean and filter your image data:</p> <pre><code># Basic preprocessing\npython -m prismh.core.preprocess \\\n    --data_dir data/images \\\n    --output_dir results/preprocessing \\\n    --sample_size 5000\n\n# Or using the full dataset\npython -m prismh.core.preprocess \\\n    --data_dir /path/to/all_images \\\n    --metadata /path/to/metadata.json \\\n    --output_dir results/preprocessing\n</code></pre>"},{"location":"guide/quickstart/#understanding-results","title":"Understanding Results","text":"<p>The preprocessing step creates:</p> <ul> <li><code>results/preprocessing/clean/</code> - High-quality images</li> <li><code>results/preprocessing/problematic/</code> - Filtered out images</li> <li><code>invalid/</code> - Corrupted images</li> <li><code>duplicates/</code> - Duplicate images</li> <li><code>blurry/</code> - Low-quality images</li> <li><code>dark/</code> - Poorly lit images</li> <li><code>outliers/</code> - Unusual images</li> </ul>"},{"location":"guide/quickstart/#step-3-feature-extraction","title":"Step 3: Feature Extraction","text":"<p>Extract features using the SimCLR model:</p> <pre><code># Extract embeddings from clean images\npython -m prismh.core.extract_embeddings \\\n    --input_dir results/preprocessing/clean \\\n    --output_dir results/embeddings \\\n    --model_path models/simclr_pretrained.pt\n</code></pre> <p>This creates: - <code>results/embeddings/all_embeddings.npz</code> - Feature vectors - <code>results/embeddings/metadata.json</code> - Image metadata</p>"},{"location":"guide/quickstart/#step-4-clustering-analysis","title":"Step 4: Clustering Analysis","text":"<p>Discover patterns in your data:</p> <pre><code># Cluster similar images\npython -m prismh.core.cluster_embeddings \\\n    --embeddings results/embeddings/all_embeddings.npz \\\n    --output_dir results/clustering\n</code></pre> <p>This generates: - <code>results/clustering/cluster_gallery.html</code> - Interactive cluster visualization - <code>results/clustering/outlier_gallery.html</code> - Outlier detection results</p>"},{"location":"guide/quickstart/#step-5-classification","title":"Step 5: Classification","text":"<p>Classify container types:</p> <pre><code># Classify images\npython -m prismh.models.classify \\\n    --embeddings results/embeddings/all_embeddings.npz \\\n    --labels data/metadata/annotations.json \\\n    --output_dir results/classification\n</code></pre>"},{"location":"guide/quickstart/#step-6-metadata-integration","title":"Step 6: Metadata Integration","text":"<p>Integrate spatial and temporal analysis:</p> <pre><code># Analyze metadata patterns\npython -m prismh.data.metadata_integrator \\\n    --json data/metadata/annotations.json \\\n    --images results/preprocessing/clean \\\n    --output_dir results/analysis \\\n    --worker-quality\n</code></pre>"},{"location":"guide/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete example using the command line:</p> <pre><code># Set up paths\nDATA_DIR=\"/Users/kirubeso.r/Documents/ArtPark/all_them_images\"\nMETADATA=\"/Users/kirubeso.r/Documents/ArtPark/all_jsons/first100k.json\"\nOUTPUT_DIR=\"results_sample\"\n\n# Step 1: Preprocess\npython -m prismh.core.preprocess \\\n    --data_dir $DATA_DIR \\\n    --metadata $METADATA \\\n    --output_dir $OUTPUT_DIR \\\n    --sample_size 5000\n\n# Step 2: Extract features\npython -m prismh.core.extract_embeddings \\\n    --input_dir $OUTPUT_DIR/clean \\\n    --output_dir $OUTPUT_DIR/embeddings\n\n# Step 3: Cluster analysis\npython -m prismh.core.cluster_embeddings \\\n    --embeddings $OUTPUT_DIR/embeddings/all_embeddings.npz \\\n    --output_dir $OUTPUT_DIR/clustering\n\n# Step 4: Generate report\npython -m prismh.data.metadata_integrator \\\n    --json $METADATA \\\n    --images $OUTPUT_DIR/clean \\\n    --output_dir $OUTPUT_DIR/analysis \\\n    --worker-quality\n</code></pre>"},{"location":"guide/quickstart/#viewing-results","title":"Viewing Results","text":""},{"location":"guide/quickstart/#interactive-visualizations","title":"Interactive Visualizations","text":"<p>Open the generated HTML files in your browser:</p> <pre><code># View clustering results\nopen results/clustering/cluster_gallery.html\n\n# View outlier analysis\nopen results/clustering/outlier_gallery.html\n\n# View metadata analysis\nopen results/analysis/worker_analysis.html\n</code></pre>"},{"location":"guide/quickstart/#command-line-summaries","title":"Command Line Summaries","text":"<pre><code># Print summary statistics\npython -c \"\nimport json\nwith open('results/analysis/summary.json') as f:\n    data = json.load(f)\n    print(f'Total images processed: {data[\\\"total_images\\\"]}')\n    print(f'Clean images: {data[\\\"clean_images\\\"]}')\n    print(f'Problematic images: {data[\\\"problematic_images\\\"]}')\n\"\n</code></pre>"},{"location":"guide/quickstart/#python-api-usage","title":"Python API Usage","text":"<p>You can also use the Python API directly:</p> <pre><code>from prismh.core.preprocess import ImagePreprocessor\nfrom prismh.core.extract_embeddings import extract_embeddings_main\nfrom prismh.core.cluster_embeddings import cluster_main\n\n# Initialize preprocessor\npreprocessor = ImagePreprocessor(\n    input_dir=\"data/images\",\n    output_dir=\"results/preprocessing\"\n)\n\n# Run preprocessing\npreprocessor.run_preprocessing()\n\n# Extract embeddings\nextract_embeddings_main()\n\n# Cluster analysis\ncluster_main()\n</code></pre>"},{"location":"guide/quickstart/#common-parameters","title":"Common Parameters","text":""},{"location":"guide/quickstart/#preprocessing-parameters","title":"Preprocessing Parameters","text":"Parameter Description Default <code>--ccthreshold</code> Similarity threshold for duplicates 0.9 <code>--outlier_distance</code> Distance threshold for outliers 0.68 <code>--sample_size</code> Number of images to process All"},{"location":"guide/quickstart/#feature-extraction-parameters","title":"Feature Extraction Parameters","text":"Parameter Description Default <code>--batch_size</code> Batch size for processing 64 <code>--model_path</code> Path to pretrained model Auto-detect <code>--device</code> Device to use (cpu/cuda/mps) Auto-detect"},{"location":"guide/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Configuration Guide - Customize parameters</li> <li>\ud83d\udcca User Guide - Learn about advanced features</li> <li>\ud83d\udd27 API Reference - Detailed function documentation</li> <li>\ud83d\udca1 Examples - More usage examples</li> </ul>"},{"location":"guide/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/quickstart/#common-issues","title":"Common Issues","text":"<p>\"No images found\" <pre><code># Check your data directory structure\nls -la data/images/\n</code></pre></p> <p>\"Out of memory\" <pre><code># Reduce batch size\npython -m prismh.core.extract_embeddings --batch_size 32\n</code></pre></p> <p>\"Model not found\" <pre><code># Check if SimCLR model exists\nls -la models/\n# Or train a new model first\npython -m prismh.models.simclr --train\n</code></pre></p> <p>Performance Tips</p> <ul> <li>Use GPU acceleration when available</li> <li>Start with smaller sample sizes for testing</li> <li>Process images in batches to manage memory usage</li> <li>Use SSD storage for faster I/O operations </li> </ul>"}]}