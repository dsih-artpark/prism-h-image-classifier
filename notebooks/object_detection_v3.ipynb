{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Object Detection V3: 3-Model Ensemble (Colab Ready)\n",
                "\n",
                "This notebook implements the **3-Model Ensemble** evaluated on the PRISM-H dataset:\n",
                "1. **Grounding DINO Swin-T**\n",
                "2. **Grounding DINO Swin-B**\n",
                "3. **OWLv2**\n",
                "\n",
                "It fuses predictions using **Non-Maximum Suppression (NMS)**.\n",
                "\n",
                "### Instructions\n",
                "1. Run the **Setup** cell to install dependencies and download weights.\n",
                "2. **IMPORTANT:** If you see an import error, try **Runtime > Restart Runtime** and run the cells again (skip the installation part if already done).\n",
                "3. Run the **Model Loading** cell to initialize the ensemble.\n",
                "4. Run the **Inference** cell to upload an image and see the result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SETUP & INSTALLATION ---\n",
                "# This cell is designed to be robust for Google Colab.\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# 1. Install System Dependencies\n",
                "!apt-get install -y libgl1-mesa-glx\n",
                "\n",
                "# 2. Install Python Packages\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install -q transformers accelerate scipy safetensors\n",
                "!pip install -q opencv-python matplotlib pycocotools\n",
                "\n",
                "# 3. Install Grounding DINO (Robust Method)\n",
                "if not os.path.exists(\"GroundingDINO\"):\n",
                "    !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
                "    %cd GroundingDINO\n",
                "    !pip install -q -e .\n",
                "    %cd ..\n",
                "else:\n",
                "    print(\"GroundingDINO repo already exists. Ensuring it is installed...\")\n",
                "    %cd GroundingDINO\n",
                "    !pip install -q -e .\n",
                "    %cd ..\n",
                "\n",
                "# 4. Download Weights\n",
                "def download_weight(url, filename):\n",
                "    if not os.path.exists(filename):\n",
                "        print(f\"Downloading {filename}...\")\n",
                "        !wget -q {url} -O {filename}\n",
                "\n",
                "download_weight(\"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\", \"groundingdino_swint_ogc.pth\")\n",
                "download_weight(\"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth\", \"groundingdino_swinb_cogcoor.pth\")\n",
                "\n",
                "print(\"Setup Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- IMPORTS & CONFIGURATION ---\n",
                "\n",
                "import torch\n",
                "import cv2\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from torchvision.ops import nms\n",
                "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
                "\n",
                "# Grounding DINO Imports\n",
                "# We remove the try-except block to see the actual error if it fails.\n",
                "import sys\n",
                "if \"GroundingDINO\" not in sys.path:\n",
                "    sys.path.append(\"GroundingDINO\")\n",
                "\n",
                "from groundingdino.util.inference import load_model, predict as groundingdino_predict\n",
                "import groundingdino.datasets.transforms as T\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Target Classes (PRISM-H)\n",
                "TARGET_CLASSES = [\n",
                "    \"Sump\", \"Cement Tank\", \"Plastic Barrel\", \"Metal Drum\", \"Mud Pot\", \n",
                "    \"Plastic Bucket\", \"Stone Cistern\", \"Grinding-stone\", \"Cement Tanks\", \n",
                "    \"Water Puddle\", \"Plant-holder\", \"Tyre\", \"Solid Waste\", \"Other Container\"\n",
                "]\n",
                "PROMPT_LIST = TARGET_CLASSES\n",
                "PROMPT_STRING = \" . \".join(TARGET_CLASSES) + \" .\"\n",
                "\n",
                "BOX_THRESHOLD = 0.25\n",
                "TEXT_THRESHOLD = 0.25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MODEL LOADING ---\n",
                "\n",
                "def init_detector(backend, device):\n",
                "    print(f\"Initializing {backend}...\")\n",
                "    if backend == \"gdino_swint\":\n",
                "        return load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\", device=device)\n",
                "    elif backend == \"gdino_swinb\":\n",
                "        return load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"groundingdino_swinb_cogcoor.pth\", device=device)\n",
                "    elif backend == \"owlv2\":\n",
                "        processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
                "        model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(device)\n",
                "        return (processor, model)\n",
                "    elif backend == \"ensemble_3\":\n",
                "        models = {\n",
                "            \"gdino_swint\": init_detector(\"gdino_swint\", device),\n",
                "            \"gdino_swinb\": init_detector(\"gdino_swinb\", device),\n",
                "            \"owlv2\":       init_detector(\"owlv2\", device),\n",
                "        }\n",
                "        return models\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown backend: {backend}\")\n",
                "\n",
                "print(\"Loading Ensemble Models (this may take a minute)...\")\n",
                "ensemble_models = init_detector(\"ensemble_3\", device)\n",
                "print(\"Models Loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- INFERENCE LOGIC ---\n",
                "\n",
                "def run_single_detector(backend, model, img_rgb, prompt_text, prompt_list, threshold, device):\n",
                "    h, w = img_rgb.shape[:2]\n",
                "\n",
                "    if backend.startswith(\"gdino\"):\n",
                "        transform = T.Compose([\n",
                "            T.RandomResize([800], max_size=1333),\n",
                "            T.ToTensor(),\n",
                "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "        img_pil = Image.fromarray(img_rgb)\n",
                "        img_tensor, _ = transform(img_pil, None)\n",
                "        \n",
                "        boxes, logits, phrases = groundingdino_predict(\n",
                "            model=model,\n",
                "            image=img_tensor,\n",
                "            caption=prompt_text,\n",
                "            box_threshold=threshold,\n",
                "            text_threshold=TEXT_THRESHOLD,\n",
                "            device=device\n",
                "        )\n",
                "        if len(boxes) > 0:\n",
                "            # Convert cxcywh -> xyxy for NMS later\n",
                "            boxes_xyxy = boxes * torch.Tensor([w, h, w, h])\n",
                "            boxes_xyxy[:, :2] -= boxes_xyxy[:, 2:] / 2  # cx,cy -> x1,y1\n",
                "            boxes_xyxy[:, 2:] += boxes_xyxy[:, :2]      # w,h -> x2,y2\n",
                "            return boxes_xyxy.cpu().numpy(), logits.cpu().numpy(), phrases\n",
                "        else:\n",
                "            return np.array([]), np.array([]), []\n",
                "\n",
                "    elif backend == \"owlv2\":\n",
                "        processor, owl_model = model\n",
                "        texts = [[f\"a photo of a {t}\" for t in prompt_list]]\n",
                "        inputs = processor(text=texts, images=Image.fromarray(img_rgb), return_tensors=\"pt\").to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = owl_model(**inputs)\n",
                "            \n",
                "        target_sizes = torch.tensor([[h, w]]).to(device)\n",
                "        results = processor.post_process_object_detection(outputs, threshold=threshold, target_sizes=target_sizes)[0]\n",
                "        \n",
                "        boxes = results[\"boxes\"]\n",
                "        scores = results[\"scores\"]\n",
                "        labels = [prompt_list[i] for i in results[\"labels\"].cpu().numpy()]\n",
                "        return boxes.cpu().numpy(), scores.cpu().numpy(), labels\n",
                "\n",
                "    return np.array([]), np.array([]), []\n",
                "\n",
                "def run_ensemble(models, img_rgb, prompt_text, prompt_list, threshold, device):\n",
                "    # Run all three\n",
                "    b_T, s_T, l_T = run_single_detector(\"gdino_swint\", models[\"gdino_swint\"], img_rgb, prompt_text, prompt_list, threshold, device)\n",
                "    b_B, s_B, l_B = run_single_detector(\"gdino_swinb\", models[\"gdino_swinb\"], img_rgb, prompt_text, prompt_list, threshold, device)\n",
                "    b_O, s_O, l_O = run_single_detector(\"owlv2\",       models[\"owlv2\"],       img_rgb, prompt_text, prompt_list, threshold, device)\n",
                "\n",
                "    # Collect\n",
                "    all_boxes, all_scores, all_labels = [], [], []\n",
                "    if len(b_T) > 0: all_boxes.append(b_T); all_scores.append(s_T); all_labels.extend(l_T)\n",
                "    if len(b_B) > 0: all_boxes.append(b_B); all_scores.append(s_B); all_labels.extend(l_B)\n",
                "    if len(b_O) > 0: all_boxes.append(b_O); all_scores.append(s_O); all_labels.extend(l_O)\n",
                "\n",
                "    if not all_boxes:\n",
                "        return [], [], []\n",
                "\n",
                "    boxes_np = np.concatenate(all_boxes, axis=0)\n",
                "    scores_np = np.concatenate(all_scores, axis=0)\n",
                "    \n",
                "    # NMS\n",
                "    boxes_tensor = torch.tensor(boxes_np, dtype=torch.float32)\n",
                "    scores_tensor = torch.tensor(scores_np, dtype=torch.float32)\n",
                "    \n",
                "    keep = nms(boxes_tensor, scores_tensor, iou_threshold=0.5)\n",
                "    \n",
                "    return boxes_tensor[keep].numpy(), scores_tensor[keep].numpy(), [all_labels[i] for i in keep]\n",
                "\n",
                "def visualize(img_rgb, boxes, scores, labels):\n",
                "    plt.figure(figsize=(12, 12))\n",
                "    plt.imshow(img_rgb)\n",
                "    ax = plt.gca()\n",
                "    ax.set_axis_off()\n",
                "\n",
                "    for box, score, label in zip(boxes, scores, labels):\n",
                "        x1, y1, x2, y2 = box\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        rect = plt.Rectangle((x1, y1), w, h, fill=False, edgecolor='red', linewidth=2)\n",
                "        ax.add_patch(rect)\n",
                "        ax.text(x1, y1-5, f\"{label}: {score:.2f}\", color='white', fontsize=10, bbox=dict(facecolor='red', alpha=0.5))\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- RUN ON IMAGE ---\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    uploaded = files.upload()\n",
                "    filename = list(uploaded.keys())[0]\n",
                "    \n",
                "    img = cv2.imread(filename)\n",
                "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    print(\"Running Ensemble Detection...\")\n",
                "    boxes, scores, labels = run_ensemble(ensemble_models, img_rgb, PROMPT_STRING, PROMPT_LIST, BOX_THRESHOLD, device)\n",
                "    \n",
                "    print(f\"Found {len(boxes)} objects.\")\n",
                "    visualize(img_rgb, boxes, scores, labels)\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"Not running in Colab or no file uploaded.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
